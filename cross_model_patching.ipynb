{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014efce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/nikhil_prakash/miniconda/envs/anima/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.nn import CosineSimilarity\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer\n",
    "from functools import partial\n",
    "from baukit import TraceDict\n",
    "from einops import rearrange, einsum\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly_utils import imshow, scatter\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import PeftModel\n",
    "\n",
    "import pysvelte\n",
    "import analysis_utils\n",
    "from counterfactual_datasets.entity_tracking import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(10)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "068509e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loading...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76874841d24f483bba40fe6fd17ba3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df28c7f8b7a47a1844cf3d184c784c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Model Loading...\")\n",
    "# path = \"AlekseyKorshuk/vicuna-7b\"\n",
    "path = \"/data/nikhil_prakash/llama_weights/7B\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(path).to(device)\n",
    "\n",
    "base_model = \"decapoda-research/llama-7b-hf\"\n",
    "lora_weights = \"tiedong/goat-lora-7b\"\n",
    "\n",
    "goat_model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "goat_model = PeftModel.from_pretrained(\n",
    "    goat_model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={'': 0},\n",
    ")\n",
    "\n",
    "llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767b0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./box_datasets/no_instructions/alternative/Random/7/train.jsonl\"\n",
    "object_file = \"./box_datasets/objects_with_bnc_frequency.csv\"\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bcc0eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 600\n"
     ]
    }
   ],
   "source": [
    "raw_data = entity_tracking_example_sampler(\n",
    "    tokenizer=llama_tokenizer,\n",
    "    num_samples=600,\n",
    "    data_file=data_file,\n",
    "    # object_file=object_file,\n",
    "    few_shot=False,\n",
    "    alt_examples=True,\n",
    "    # num_ents_or_ops=3,\n",
    "    architecture=\"LLaMAForCausalLM\",\n",
    ")\n",
    "\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_data[0],\n",
    "        \"last_token_indices\": raw_data[1],\n",
    "        \"labels\": raw_data[2],\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "\n",
    "print(f\"Length of dataset: {len(dataset)}\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a177adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E, the bill is in Box M, the cross is in Box K, the map is in Box D. Box X contains the\n",
      "Answer:  document\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(\n",
    "    f\"Prompt: {llama_tokenizer.decode(dataset[idx]['input_ids'][:dataset[idx]['last_token_indices']+1])}\"\n",
    ")\n",
    "print(f\"Answer: {llama_tokenizer.decode(dataset[idx]['labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ba8f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [00:30,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 82.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "correct_count = 0\n",
    "goat_model.eval()\n",
    "with torch.no_grad():\n",
    "    for _, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(goat_model.device)\n",
    "\n",
    "        outputs = goat_model(input_ids=inputs[\"input_ids\"])\n",
    "\n",
    "        for bi in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][bi]\n",
    "            pred = torch.argmax(outputs.logits[bi][inputs[\"last_token_indices\"][bi]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            # else:\n",
    "            #     print(f\"Label: {tokenizer.decode(label)}, Prediction: {tokenizer.decode(pred)}\")\n",
    "            total_count += 1\n",
    "\n",
    "del outputs\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "current_acc = round(correct_count / total_count * 100, 2)\n",
    "print(f\"Task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "719dc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"./new_pp_exps/reverse/7_boxes\"\n",
    "path = root_path + \"/direct_logit_heads.pt\"\n",
    "direct_logit_heads = analysis_utils.compute_topk_components(\n",
    "    torch.load(path), k=52, largest=False\n",
    ")\n",
    "\n",
    "path = root_path + \"/heads_affect_direct_logit.pt\"\n",
    "heads_affecting_direct_logit_heads = analysis_utils.compute_topk_components(\n",
    "    torch.load(path), k=15, largest=False\n",
    ")\n",
    "\n",
    "path = root_path + \"/heads_at_query_box_pos.pt\"\n",
    "head_at_query_box_token = analysis_utils.compute_topk_components(\n",
    "    torch.load(path), k=30, largest=False\n",
    ")\n",
    "\n",
    "path = root_path + \"/heads_at_prev_query_box_pos.pt\"\n",
    "heads_at_prev_box_pos = analysis_utils.compute_topk_components(\n",
    "    torch.load(path), k=5, largest=False\n",
    ")\n",
    "\n",
    "intersection = []\n",
    "for head in direct_logit_heads:\n",
    "    if head in heads_affecting_direct_logit_heads:\n",
    "        intersection.append(head)\n",
    "\n",
    "for head in intersection:\n",
    "    direct_logit_heads.remove(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "885dc16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 15 30 5\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "        len(direct_logit_heads),\n",
    "        len(heads_affecting_direct_logit_heads),\n",
    "        len(head_at_query_box_token),\n",
    "        len(heads_at_prev_box_pos),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "247e0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_modules = [[f\"model.layers.{layer}.self_attn.k_proj\", \n",
    "                  f\"model.layers.{layer}.self_attn.q_proj\",\n",
    "                  f\"model.layers.{layer}.self_attn.v_proj\",\n",
    "                 f\"model.layers.{layer}.self_attn.o_proj\"] \n",
    "                 for layer in range(32)]\n",
    "goat_modules = [[f\"base_model.model.model.layers.{layer}.self_attn.k_proj\", \n",
    "                 f\"base_model.model.model.layers.{layer}.self_attn.q_proj\",\n",
    "                 f\"base_model.model.model.layers.{layer}.self_attn.v_proj\",\n",
    "                f\"base_model.model.model.layers.{layer}.self_attn.o_proj\"] \n",
    "                for layer in range(32)]\n",
    "\n",
    "llama_modules = [item for sublist in llama_modules for item in sublist]\n",
    "goat_modules = [item for sublist in goat_modules for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "95e4dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [00:44,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "goat_cache = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(goat_model.device)\n",
    "\n",
    "        with TraceDict(goat_model, goat_modules, retain_input=True) as cache:\n",
    "            _ = goat_model(inputs[\"input_ids\"])\n",
    "        \n",
    "        for llama_layer, goat_layer in zip(llama_modules, goat_modules):\n",
    "            if \"o_proj\" in llama_layer:\n",
    "                if bi in goat_cache:\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].input.cpu()\n",
    "                else:\n",
    "                    goat_cache[bi] = {}\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].input.cpu()\n",
    "            else:\n",
    "                if bi in goat_cache:\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].output.cpu()\n",
    "                else:\n",
    "                    goat_cache[bi] = {}\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007922e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "44a686ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_model_patching(inputs, outputs, layer, bi, relative_pos, input_tokens):\n",
    "    if isinstance(inputs, tuple):\n",
    "        inputs = inputs[0]\n",
    "\n",
    "    if isinstance(outputs, tuple):\n",
    "        outputs = outputs[0]\n",
    "\n",
    "    cache = rearrange(\n",
    "                goat_cache[bi][layer],\n",
    "                \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "    \n",
    "    if \"o_proj\" in layer:\n",
    "        pass\n",
    "#         inputs = rearrange(\n",
    "#                 inputs,\n",
    "#                 \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "#                 n_heads=llama_model.config.num_attention_heads,\n",
    "#             )\n",
    "\n",
    "#         for rel_pos, heads in relative_pos.items():\n",
    "#             curr_layer_heads = [h for l, h in heads if l == int(layer.split(\".\")[2])]            \n",
    "            \n",
    "#             if rel_pos == -1:\n",
    "#                 for batch in range(inputs.size(0)):\n",
    "#                     prev_query_box_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "#                         input_tokens[\"input_ids\"][batch],\n",
    "#                         input_tokens[\"last_token_indices\"][batch]\n",
    "#                     )\n",
    "#                     for head in curr_layer_heads:\n",
    "#                         inputs[batch, prev_query_box_pos, head] = cache[batch, prev_query_box_pos, head]\n",
    "\n",
    "#             else:\n",
    "#                 pos = inputs.size(1) - rel_pos - 1\n",
    "#                 for head in curr_layer_heads:\n",
    "#                     inputs[:, pos, head] = cache[:, pos, head]\n",
    "\n",
    "#         inputs = rearrange(\n",
    "#                 inputs,\n",
    "#                 \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\",\n",
    "#                 n_heads=llama_model.config.num_attention_heads,\n",
    "#             )\n",
    "#         w_o = llama_model.state_dict()[f\"{layer}.weight\"]\n",
    "#         outputs = einsum(\n",
    "#             inputs, w_o, \"batch seq_len hidden_size, d_model hidden_size -> batch seq_len d_model\"\n",
    "#         )\n",
    "\n",
    "    else:\n",
    "        outputs = rearrange(\n",
    "                outputs,\n",
    "                \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "\n",
    "        for rel_pos, heads in relative_pos.items():\n",
    "            curr_layer_heads = [h for l, h in heads if l == int(layer.split(\".\")[2])]   \n",
    "            \n",
    "            if rel_pos == -1:\n",
    "                for batch in range(inputs.size(0)):\n",
    "                    prev_query_box_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "                        input_tokens[\"input_ids\"][batch],\n",
    "                        input_tokens[\"last_token_indices\"][batch]\n",
    "                    )\n",
    "#                 if \"v_proj\" in layer:\n",
    "#                     for head in curr_layer_heads:\n",
    "#                         outputs[:, prev_query_box_pos, head] = cache[:, prev_query_box_pos, head]\n",
    "\n",
    "#                 if \"k_proj\" in layer:\n",
    "#                     for head in curr_layer_heads:\n",
    "#                         outputs[:, prev_query_box_pos, head] = cache[:, prev_query_box_pos, head]\n",
    "\n",
    "#                 if \"q_proj\" in layer:\n",
    "#                     for head in curr_layer_heads:\n",
    "#                         outputs[:, prev_query_box_pos, head] = cache[:, prev_query_box_pos, head]\n",
    "            else:\n",
    "#                 pos = outputs.size(1) - rel_pos - 1\n",
    "#                 for batch in range(inputs.size(0)):\n",
    "#                     prev_query_box_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "#                         input_tokens[\"input_ids\"][batch],\n",
    "#                         input_tokens[\"last_token_indices\"][batch]\n",
    "#                     )\n",
    "#                     correct_obj_pos = prev_query_box_pos - 4\n",
    "                if \"v_proj\" in layer:\n",
    "                    for head in curr_layer_heads:\n",
    "                        outputs[:, :, head] = cache[:, :, head]\n",
    "\n",
    "                if \"k_proj\" in layer:\n",
    "                    for head in curr_layer_heads:\n",
    "                        outputs[:, :, head] = cache[:, :, head]\n",
    "\n",
    "#                 if \"q_proj\" in layer:\n",
    "#                     for head in curr_layer_heads:\n",
    "#                         outputs[:, -1, head] = cache[:, -1, head]\n",
    "\n",
    "        outputs = rearrange(\n",
    "                    outputs,\n",
    "                    \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\",\n",
    "                    n_heads=llama_model.config.num_attention_heads,\n",
    "                )\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ee702e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_pos = {}\n",
    "# relative_pos[0] = direct_logit_heads\n",
    "relative_pos[0] = heads_affecting_direct_logit_heads\n",
    "# relative_pos[2] = head_at_query_box_token\n",
    "# relative_pos[-1] = heads_at_prev_box_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c95c5393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [00:50,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 73.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_count, total_count = 0, 0\n",
    "correct_obj_attn_score_sum, incorrect_obj_attn_score_sum, first_token_attn_score_sum = 0, 0, 0\n",
    "box_attn_score_sum = 0\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        with TraceDict(llama_model, llama_modules, retain_input=True, edit_output=partial(\n",
    "                                                                        cross_model_patching,\n",
    "                                                                        bi = bi,\n",
    "                                                                        relative_pos = relative_pos,\n",
    "                                                                        input_tokens = inputs)) as trace:\n",
    "                outputs = llama_model(inputs[\"input_ids\"], output_attentions=True)\n",
    "\n",
    "#         scaled_attn = {}\n",
    "#         for layer_idx in range(32):\n",
    "#             attn_score = outputs.attentions[layer_idx]\n",
    "#             value_vector = trace[f\"model.layers.{layer_idx}.self_attn.v_proj\"].output\n",
    "#             value_vector = rearrange(value_vector,\n",
    "#                                     \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "#                                     n_heads=llama_model.config.num_attention_heads,\n",
    "#                                 )\n",
    "#             value_vector_norm = torch.norm(value_vector, dim=-1)\n",
    "#             scaled_attn[layer_idx] = einsum(\n",
    "#                 value_vector_norm,\n",
    "#                 attn_score,\n",
    "#                 \"batch k_seq_len n_heads, batch n_heads q_seq_len k_seq_len -> batch n_heads q_seq_len k_seq_len\",\n",
    "#             )\n",
    "\n",
    "#         box_attn_tmp = 0\n",
    "        for batch in range(inputs[\"labels\"].size(0)):\n",
    "#             for l, h in heads_affecting_direct_logit_heads:\n",
    "#                 box_attn_tmp += scaled_attn[l][batch, h, -1, -3]\n",
    "#             box_attn_tmp /= len(heads_affecting_direct_logit_heads)\n",
    "            \n",
    "            label = inputs[\"labels\"][batch]\n",
    "            pred = torch.argmax(outputs.logits[batch][inputs[\"last_token_indices\"][batch]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            # else:\n",
    "            #     print(f\"Label: {tokenizer.decode(label)}, Prediction: {tokenizer.decode(pred)}\")\n",
    "            total_count += 1\n",
    "\n",
    "#         box_attn_score_sum += box_attn_tmp / inputs[\"labels\"].size(0)\n",
    "\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "current_acc = round(correct_count / total_count * 100, 2)\n",
    "box_attn_score_sum = box_attn_score_sum / len(dataloader)\n",
    "print(f\"Task accuracy: {current_acc}\")\n",
    "# print(f\"Task accuracy: {current_acc}\\nCorrect object attn score: {correct_obj_attn_score_sum}\\nIncorrect object attn score: {incorrect_obj_attn_score_sum}\\nStart token attn score: {first_token_attn_score_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531f146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94840d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Task accuracy: 66.5, Attn Score: 0.01679055020213127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Task accuracy: 78.33, Attn Score: 0.017840217798948288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102763b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Task accuracy: 66.5, Attn Score: 0.12626506388187408"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Task accuracy: 78.33, Attn Score: 0.1326800286769867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d67c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f64963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Logit Heads\n",
    "# Original - 66.5\n",
    "# Query - 75.17\n",
    "# All keys (no Query) - 66.67\n",
    "# All values - 69.0\n",
    "# All Keys with final query -75.33\n",
    "# All Keys with final query and correct object value vector - 76.0\n",
    "# All Keys with final query and all value vectors - 76.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65fdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heads affecting Direct Logit Heads\n",
    "# Original - 66.5\n",
    "# Query (final pos) - 69.67\n",
    "# All Keys (no Query) - 61.67\n",
    "# All values - 73.67\n",
    "# All Keys with final query - 71.33\n",
    "# All Keys with final query and box label pos value vector - 74.67\n",
    "# All Keys with final query and box all value vectors - 78.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048a21f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856d92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task accuracy: 66.5\n",
    "# Correct object attn score: 0.14423631131649017\n",
    "# Incorrect object attn score: 0.0002922056009992957\n",
    "# Start token attn score: 0.41476279497146606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "721bc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task accuracy: 75.17\n",
    "# Correct object attn score: 0.18178720772266388\n",
    "# Incorrect object attn score: 0.00031543115619570017\n",
    "# Start token attn score: 0.28832370042800903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1f92b7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 75.17, 'key': 66.33, 'value': 67.0}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_patching_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b00ae9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_patching_scores['value'] = 67.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "466c2082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'direct_logit_heads': 75.83,\n",
       " 'heads_affect_direct_logit': 78.33,\n",
       " 'head_at_query_box_token': 66.17,\n",
       " 'heads_at_prev_box_pos': 67.33,\n",
       " 'direct_logit_heads + heads_affect_direct_logit': 79.67,\n",
       " 'all_head_groups': 79.5}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patching_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8942951f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             prev_query_box_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "#                 inputs[\"input_ids\"][batch],\n",
    "#                 inputs[\"last_token_indices\"][batch]\n",
    "#             )\n",
    "#             correct_obj_pos = prev_query_box_pos - 4\n",
    "#             object_pos = [i for i in range(2, 50, 7)]\n",
    "#             object_pos.remove(correct_obj_pos)\n",
    "\n",
    "#             correct_obj_attn_temp, incorrect_obj_attn_temp, first_token_attn_temp = 0, 0, 0\n",
    "#             for l, h in direct_logit_heads:\n",
    "#                 correct_obj_attn_temp += outputs.attentions[l][batch, h, -1, correct_obj_pos]\n",
    "#                 first_token_attn_temp += outputs.attentions[l][batch, h, -1, 0]\n",
    "                \n",
    "#                 for pos in object_pos:\n",
    "#                     incorrect_obj_attn_temp += outputs.attentions[l][batch, h, -1, pos]\n",
    "#                 incorrect_obj_attn_temp = incorrect_obj_attn_temp/len(object_pos)\n",
    "                \n",
    "#             correct_obj_attn_score_sum += correct_obj_attn_temp/len(direct_logit_heads)\n",
    "#             first_token_attn_score_sum += first_token_attn_temp/len(direct_logit_heads)\n",
    "#             incorrect_obj_attn_score_sum += incorrect_obj_attn_temp/len(direct_logit_heads)\n",
    "\n",
    "\n",
    "#         correct_obj_attn_score_sum = correct_obj_attn_score_sum / inputs[\"labels\"].size(0)\n",
    "#         first_token_attn_score_sum = first_token_attn_score_sum / inputs[\"labels\"].size(0)\n",
    "#         incorrect_obj_attn_score_sum = incorrect_obj_attn_score_sum / inputs[\"labels\"].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "11926ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_pos = [i for i in range(2, 50, 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3cc8b097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The cross is in Box Z, the machine is in Box K, the drink is in Box M, the magnet is in Box E, the paper is in Box A, the ball is in Box Y, the milk is in Box L. Box A contains the'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.decode(inputs[\"input_ids\"][batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f8834172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' cross', ' machine', ' drink', ' magnet', ' paper', ' ball', ' milk']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[llama_tokenizer.decode(token_idx) for token_idx in inputs[\"input_ids\"][batch][object_pos]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
