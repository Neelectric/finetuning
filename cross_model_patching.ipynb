{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014efce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/nikhil_prakash/miniconda/envs/anima/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.nn import CosineSimilarity\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer\n",
    "from functools import partial\n",
    "from baukit import TraceDict\n",
    "from einops import rearrange, einsum\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly_utils import imshow, scatter\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import PeftModel\n",
    "\n",
    "import pysvelte\n",
    "import analysis_utils\n",
    "from counterfactual_datasets.entity_tracking import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(10)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "068509e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loading...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769893a7c47c412c9acbf1fd36a8e3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0574a2488da64c7782e53e84c179af16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Model Loading...\")\n",
    "# path = \"AlekseyKorshuk/vicuna-7b\"\n",
    "path = \"/data/nikhil_prakash/llama_weights/7B\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(path).to(device)\n",
    "\n",
    "base_model = \"decapoda-research/llama-7b-hf\"\n",
    "lora_weights = \"tiedong/goat-lora-7b\"\n",
    "\n",
    "goat_model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "goat_model = PeftModel.from_pretrained(\n",
    "    goat_model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={'': 0},\n",
    ")\n",
    "\n",
    "llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767b0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./box_datasets/no_instructions/alternative/Random/7/train.jsonl\"\n",
    "object_file = \"./box_datasets/objects_with_bnc_frequency.csv\"\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bcc0eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 500\n"
     ]
    }
   ],
   "source": [
    "raw_data = entity_tracking_example_sampler(\n",
    "    tokenizer=llama_tokenizer,\n",
    "    num_samples=500,\n",
    "    data_file=data_file,\n",
    "    # object_file=object_file,\n",
    "    few_shot=False,\n",
    "    alt_examples=True,\n",
    "    # num_ents_or_ops=3,\n",
    "    architecture=\"LLaMAForCausalLM\",\n",
    ")\n",
    "\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_data[0],\n",
    "        \"last_token_indices\": raw_data[1],\n",
    "        \"labels\": raw_data[2],\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "\n",
    "print(f\"Length of dataset: {len(dataset)}\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a177adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E, the bill is in Box M, the cross is in Box K, the map is in Box D. Box X contains the\n",
      "Answer:  document\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(\n",
    "    f\"Prompt: {llama_tokenizer.decode(dataset[idx]['input_ids'][:dataset[idx]['last_token_indices']+1])}\"\n",
    ")\n",
    "print(f\"Answer: {llama_tokenizer.decode(dataset[idx]['labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ba8f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:24,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "correct_count = 0\n",
    "goat_model.eval()\n",
    "with torch.no_grad():\n",
    "    for _, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        outputs = llama_model(input_ids=inputs[\"input_ids\"])\n",
    "\n",
    "        for bi in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][bi]\n",
    "            pred = torch.argmax(outputs.logits[bi][inputs[\"last_token_indices\"][bi]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            # else:\n",
    "            #     print(f\"Label: {tokenizer.decode(label)}, Prediction: {tokenizer.decode(pred)}\")\n",
    "            total_count += 1\n",
    "\n",
    "del outputs\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "current_acc = round(correct_count / total_count, 2)\n",
    "print(f\"Task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d9ffc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"circuit_heads.json\", \"r\") as f:\n",
    "    circuit_heads = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "247e0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_modules = [[f\"model.layers.{layer}.self_attn.k_proj\", \n",
    "                  f\"model.layers.{layer}.self_attn.q_proj\",\n",
    "                  f\"model.layers.{layer}.self_attn.v_proj\",\n",
    "                 f\"model.layers.{layer}.self_attn.o_proj\"] \n",
    "                 for layer in range(32)]\n",
    "goat_modules = [[f\"base_model.model.model.layers.{layer}.self_attn.k_proj\", \n",
    "                 f\"base_model.model.model.layers.{layer}.self_attn.q_proj\",\n",
    "                 f\"base_model.model.model.layers.{layer}.self_attn.v_proj\",\n",
    "                f\"base_model.model.model.layers.{layer}.self_attn.o_proj\"] \n",
    "                for layer in range(32)]\n",
    "\n",
    "llama_modules = [item for sublist in llama_modules for item in sublist]\n",
    "goat_modules = [item for sublist in goat_modules for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95e4dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:49,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "goat_cache = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(goat_model.device)\n",
    "\n",
    "        with TraceDict(goat_model, goat_modules, retain_input=True) as cache:\n",
    "            _ = goat_model(inputs[\"input_ids\"])\n",
    "        \n",
    "        for llama_layer, goat_layer in zip(llama_modules, goat_modules):\n",
    "            if \"o_proj\" in llama_layer:\n",
    "                if bi in goat_cache:\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].input.cpu()\n",
    "                else:\n",
    "                    goat_cache[bi] = {}\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].input.cpu()\n",
    "            else:\n",
    "                if bi in goat_cache:\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].output.cpu()\n",
    "                else:\n",
    "                    goat_cache[bi] = {}\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44a686ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_model_patching(inputs, outputs, layer, bi, relative_pos, input_tokens):\n",
    "    if isinstance(inputs, tuple):\n",
    "        inputs = inputs[0]\n",
    "\n",
    "    if isinstance(outputs, tuple):\n",
    "        outputs = outputs[0]\n",
    "\n",
    "    cache = rearrange(\n",
    "                goat_cache[bi][layer],\n",
    "                \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "    \n",
    "    if \"o_proj\" in layer:\n",
    "        pass\n",
    "#         inputs = rearrange(\n",
    "#                 inputs,\n",
    "#                 \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "#                 n_heads=llama_model.config.num_attention_heads,\n",
    "#             )\n",
    "\n",
    "#         for rel_pos, heads in relative_pos.items():\n",
    "#             curr_layer_heads = [h for l, h in heads if l == int(layer.split(\".\")[2])]            \n",
    "            \n",
    "#             if rel_pos == -1:\n",
    "#                 for batch in range(inputs.size(0)):\n",
    "#                     prev_query_box_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "#                         input_tokens[\"input_ids\"][batch],\n",
    "#                         input_tokens[\"last_token_indices\"][batch]\n",
    "#                     )\n",
    "#                     for head in curr_layer_heads:\n",
    "#                         inputs[batch, prev_query_box_pos, head] = cache[batch, prev_query_box_pos, head]\n",
    "\n",
    "#             else:\n",
    "#                 pos = inputs.size(1) - rel_pos - 1\n",
    "#                 for head in curr_layer_heads:\n",
    "#                     inputs[:, pos, head] = cache[:, pos, head]\n",
    "\n",
    "#         inputs = rearrange(\n",
    "#                 inputs,\n",
    "#                 \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\",\n",
    "#                 n_heads=llama_model.config.num_attention_heads,\n",
    "#             )\n",
    "#         w_o = llama_model.state_dict()[f\"{layer}.weight\"]\n",
    "#         outputs = einsum(\n",
    "#             inputs, w_o, \"batch seq_len hidden_size, d_model hidden_size -> batch seq_len d_model\"\n",
    "#         )\n",
    "\n",
    "    else:\n",
    "        outputs = rearrange(\n",
    "                outputs,\n",
    "                \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "\n",
    "        for rel_pos, heads in relative_pos.items():\n",
    "            curr_layer_heads = [h for l, h in heads if l == int(layer.split(\".\")[2])]   \n",
    "            \n",
    "            if rel_pos == -1:\n",
    "                for batch in range(inputs.size(0)):\n",
    "                    prev_query_box_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "                        input_tokens[\"input_ids\"][batch],\n",
    "                        input_tokens[\"last_token_indices\"][batch]\n",
    "                    )\n",
    "#                 if \"v_proj\" in layer:\n",
    "#                     for head in curr_layer_heads:\n",
    "#                         outputs[:, prev_query_box_pos, head] = cache[:, prev_query_box_pos, head]\n",
    "\n",
    "#                 if \"k_proj\" in layer:\n",
    "#                     for head in curr_layer_heads:\n",
    "#                         outputs[:, prev_query_box_pos, head] = cache[:, prev_query_box_pos, head]\n",
    "\n",
    "#                 if \"q_proj\" in layer:\n",
    "#                     for head in curr_layer_heads:\n",
    "#                         outputs[:, prev_query_box_pos, head] = cache[:, prev_query_box_pos, head]\n",
    "            else:\n",
    "#                 pos = outputs.size(1) - rel_pos - 1\n",
    "#                 for batch in range(inputs.size(0)):\n",
    "#                     prev_query_box_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "#                         input_tokens[\"input_ids\"][batch],\n",
    "#                         input_tokens[\"last_token_indices\"][batch]\n",
    "#                     )\n",
    "#                     correct_obj_pos = prev_query_box_pos - 4\n",
    "                if \"v_proj\" in layer:\n",
    "                    for head in curr_layer_heads:\n",
    "                        outputs[:, :, head] = cache[:, :, head]\n",
    "\n",
    "                if \"k_proj\" in layer:\n",
    "                    for head in curr_layer_heads:\n",
    "                        outputs[:, :, head] = cache[:, :, head]\n",
    "\n",
    "                if \"q_proj\" in layer:\n",
    "                    for head in curr_layer_heads:\n",
    "                        outputs[:, -1, head] = cache[:, -1, head]\n",
    "\n",
    "        outputs = rearrange(\n",
    "                    outputs,\n",
    "                    \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\",\n",
    "                    n_heads=llama_model.config.num_attention_heads,\n",
    "                )\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "089e404e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Fetcher Heads: 39\n",
      "Heads affecting direct logit heads: 7\n",
      "Heads at query box token: 17\n",
      "Heads at prev query box token: 4\n"
     ]
    }
   ],
   "source": [
    "with open(\"./new_masks/llama-7b/direct_logit_heads/object_value/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    value_fetcher_heads = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "with open(\"./new_masks/llama-7b/heads_affect_direct_logit/positional/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    positional_info_fetcher_heads = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "with open(\"./new_masks/llama-7b/heads_at_query_box_pos/positional/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    duplicate_token_heads = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "print(f\"Value Fetcher Heads: {len(value_fetcher_heads)}\")\n",
    "print(f\"Heads affecting direct logit heads: {len(positional_info_fetcher_heads)}\")\n",
    "print(f\"Heads at query box token: {len(duplicate_token_heads)}\")\n",
    "print(f\"Heads at prev query box token: {len(circuit_heads['heads_at_prev_box_pos'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee702e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_pos = {}\n",
    "relative_pos[0] = value_fetcher_heads\n",
    "# relative_pos[0] = positional_info_fetcher_heads\n",
    "# relative_pos[2] = duplicate_token_heads\n",
    "# relative_pos[-1] = circuit_heads['heads_at_prev_box_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c95c5393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [01:19,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_count, total_count = 0, 0\n",
    "correct_obj_attn_score_mean, incorrect_obj_attn_score_sum, first_token_attn_score_sum = 0, 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        with TraceDict(llama_model, llama_modules, retain_input=True, edit_output=partial(\n",
    "                                                                        cross_model_patching,\n",
    "                                                                        bi = bi,\n",
    "                                                                        relative_pos = relative_pos,\n",
    "                                                                        input_tokens = inputs)) as trace:\n",
    "                outputs = llama_model(inputs[\"input_ids\"], output_attentions=True)\n",
    "\n",
    "        for batch in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][batch]\n",
    "            pred = torch.argmax(outputs.logits[batch][inputs[\"last_token_indices\"][batch]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            total_count += 1\n",
    "\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "current_acc = round(correct_count / total_count, 2)\n",
    "correct_obj_attn_score_mean = correct_obj_attn_score_mean / len(dataloader)\n",
    "print(f\"Task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c5625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
