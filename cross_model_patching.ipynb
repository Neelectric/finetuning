{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014efce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.nn import CosineSimilarity\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer\n",
    "from functools import partial\n",
    "from baukit import TraceDict\n",
    "from einops import rearrange, einsum\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "# from plotly_utils import imshow, scatter\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import PeftModel\n",
    "\n",
    "import pysvelte\n",
    "import analysis_utils\n",
    "from counterfactual_datasets.entity_tracking import *\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")/\n",
    "device = \"cuda\"\n",
    "torch.manual_seed(10)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c69f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"AlekseyKorshuk/vicuna-7b\"\n",
    "# path = \"/data/nikhil_prakash/llama_weights/7B\"\n",
    "# vicuna_tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "# vicuna_model = AutoModelForCausalLM.from_pretrained(path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "068509e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loading...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9d4a8fe3ca46b9a90bdcfe0b0dc131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26aa7432de254c3280bdf6ca13ae605e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Model Loading...\")\n",
    "# path = \"AlekseyKorshuk/vicuna-7b\"\n",
    "path = \"/data/nikhil_prakash/llama_weights/7B\"\n",
    "# llama_tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained('hf-internal-testing/llama-tokenizer', padding_side='right')\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(path).to(device)\n",
    "\n",
    "fine_tuned_path = \"/data/nikhil_prakash/goat-finetuning/vibrant-glitter-10/\"\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(fine_tuned_path).to(device)\n",
    "\n",
    "# base_model = \"decapoda-research/llama-7b-hf\"\n",
    "# lora_weights = \"tiedong/goat-lora-7b\"\n",
    "# fine_tuned_model = LlamaForCausalLM.from_pretrained(\n",
    "#     base_model,\n",
    "#     load_in_8bit=False,\n",
    "#     torch_dtype=torch.float32,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# fine_tuned_model = PeftModel.from_pretrained(\n",
    "#     fine_tuned_model,\n",
    "#     lora_weights,\n",
    "#     torch_dtype=torch.float32,\n",
    "#     device_map={'': 0},\n",
    "# )\n",
    "\n",
    "llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767b0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./box_datasets/no_instructions/alternative/Random/7/train.jsonl\"\n",
    "object_file = \"./box_datasets/objects_with_bnc_frequency.csv\"\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bcc0eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 500\n"
     ]
    }
   ],
   "source": [
    "raw_data = entity_tracking_example_sampler(\n",
    "    tokenizer=llama_tokenizer,\n",
    "    num_samples=500,\n",
    "    data_file=data_file,\n",
    "    # object_file=object_file,\n",
    "    few_shot=False,\n",
    "    alt_examples=True,\n",
    "    # num_ents_or_ops=3,\n",
    "    architecture=\"LLaMAForCausalLM\",\n",
    ")\n",
    "\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_data[0],\n",
    "        \"last_token_indices\": raw_data[1],\n",
    "        \"labels\": raw_data[2],\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "\n",
    "print(f\"Length of dataset: {len(dataset)}\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a177adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <s>The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E, the bill is in Box M, the cross is in Box K, the map is in Box D. Box X contains the\n",
      "Answer: document\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(\n",
    "    f\"Prompt: {llama_tokenizer.decode(dataset[idx]['input_ids'][:dataset[idx]['last_token_indices']+1])}\"\n",
    ")\n",
    "print(f\"Answer: {llama_tokenizer.decode(dataset[idx]['labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc32c00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:23,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "correct_count = 0\n",
    "llama_model.eval()\n",
    "with torch.no_grad():\n",
    "    for _, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        outputs = llama_model(input_ids=inputs[\"input_ids\"])\n",
    "\n",
    "        for bi in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][bi]\n",
    "            pred = torch.argmax(outputs.logits[bi][inputs[\"last_token_indices\"][bi]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "#             else:\n",
    "#                 print(f\"Label: {llama_tokenizer.decode(label)}, Prediction: {llama_tokenizer.decode(pred)}\")\n",
    "            total_count += 1\n",
    "\n",
    "del outputs\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "current_acc = round(correct_count / total_count, 2)\n",
    "print(f\"Task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8ba8f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:23,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "correct_count = 0\n",
    "fine_tuned_model.eval()\n",
    "with torch.no_grad():\n",
    "    for _, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(fine_tuned_model.device)\n",
    "\n",
    "        outputs = fine_tuned_model(input_ids=inputs[\"input_ids\"])\n",
    "\n",
    "        for bi in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][bi]\n",
    "            pred = torch.argmax(outputs.logits[bi][inputs[\"last_token_indices\"][bi]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "#             else:\n",
    "#                 print(f\"Label: {llama_tokenizer.decode(label)}, Prediction: {llama_tokenizer.decode(pred)}\")\n",
    "            total_count += 1\n",
    "\n",
    "del outputs\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "current_acc = round(correct_count / total_count, 2)\n",
    "print(f\"Task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d9ffc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"circuit_heads.json\", \"r\") as f:\n",
    "    circuit_heads = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "247e0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_modules = [[f\"model.layers.{layer}.self_attn.k_proj\", \n",
    "                  f\"model.layers.{layer}.self_attn.q_proj\",\n",
    "                  f\"model.layers.{layer}.self_attn.v_proj\",\n",
    "                 f\"model.layers.{layer}.self_attn.o_proj\"] \n",
    "                 for layer in range(32)]\n",
    "goat_modules = [[f\"base_model.model.model.layers.{layer}.self_attn.k_proj\", \n",
    "                 f\"base_model.model.model.layers.{layer}.self_attn.q_proj\",\n",
    "                 f\"base_model.model.model.layers.{layer}.self_attn.v_proj\",\n",
    "                f\"base_model.model.model.layers.{layer}.self_attn.o_proj\"] \n",
    "                for layer in range(32)]\n",
    "\n",
    "llama_modules = [item for sublist in llama_modules for item in sublist]\n",
    "goat_modules = [item for sublist in goat_modules for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e4dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:45,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_cache = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(fine_tuned_model.device)\n",
    "\n",
    "        with TraceDict(fine_tuned_model, llama_modules, retain_input=True) as cache:\n",
    "            _ = fine_tuned_model(inputs[\"input_ids\"])\n",
    "        \n",
    "        for goat_layer, llama_layer in zip(goat_modules, llama_modules):\n",
    "            if \"o_proj\" in llama_layer:\n",
    "                if bi in fine_tuned_cache:\n",
    "                    fine_tuned_cache[bi][llama_layer] = cache[llama_layer].input.cpu()\n",
    "                else:\n",
    "                    fine_tuned_cache[bi] = {}\n",
    "                    fine_tuned_cache[bi][llama_layer] = cache[llama_layer].input.cpu()\n",
    "            else:\n",
    "                if bi in fine_tuned_cache:\n",
    "                    fine_tuned_cache[bi][llama_layer] = cache[llama_layer].output.cpu()\n",
    "                else:\n",
    "                    fine_tuned_cache[bi] = {}\n",
    "                    fine_tuned_cache[bi][llama_layer] = cache[llama_layer].output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b229d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:43,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "llama_cache = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        with TraceDict(llama_model, llama_modules, retain_input=True) as cache:\n",
    "            _ = llama_model(inputs[\"input_ids\"])\n",
    "        \n",
    "        for llama_layer in llama_modules:\n",
    "            if \"o_proj\" in llama_layer:\n",
    "                if bi in llama_cache:\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].input.cpu()\n",
    "                else:\n",
    "                    llama_cache[bi] = {}\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].input.cpu()\n",
    "            else:\n",
    "                if bi in llama_cache:\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].output.cpu()\n",
    "                else:\n",
    "                    llama_cache[bi] = {}\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1265b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44a686ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_model_patching(inputs, outputs, layer, bi, relative_pos, input_tokens):\n",
    "    if isinstance(inputs, tuple):\n",
    "        inputs = inputs[0]\n",
    "\n",
    "    if isinstance(outputs, tuple):\n",
    "        outputs = outputs[0]\n",
    "\n",
    "    g_cache = rearrange(\n",
    "                fine_tuned_cache[bi][layer],\n",
    "                \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "\n",
    "    l_cache = rearrange(\n",
    "                llama_cache[bi][layer],\n",
    "                \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "    \n",
    "    if \"o_proj\" in layer:\n",
    "        inputs = rearrange(\n",
    "                inputs,\n",
    "                \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "\n",
    "        for rel_pos, heads in relative_pos.items():\n",
    "            curr_layer_heads = [h for l, h in heads if l == int(layer.split(\".\")[2])]            \n",
    "            \n",
    "            if rel_pos == -1:\n",
    "                for batch in range(inputs.size(0)):\n",
    "                    prev_query_box_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "                        input_tokens[\"input_ids\"][batch],\n",
    "                        input_tokens[\"last_token_indices\"][batch]\n",
    "                    )\n",
    "                    for head in curr_layer_heads:\n",
    "                        inputs[batch, prev_query_box_pos, head] = g_cache[batch, prev_query_box_pos, head]\n",
    "\n",
    "            else:\n",
    "                for head in curr_layer_heads:\n",
    "                    inputs[:, -1, head] = g_cache[:, -1, head]\n",
    "        \n",
    "#         inputs[:, :-1, :] = l_cache[:, :-1, :]\n",
    "\n",
    "        inputs = rearrange(\n",
    "                inputs,\n",
    "                \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "        w_o = llama_model.state_dict()[f\"{layer}.weight\"]\n",
    "        outputs = einsum(\n",
    "            inputs, w_o, \"batch seq_len hidden_size, d_model hidden_size -> batch seq_len d_model\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "#         outputs = rearrange(\n",
    "#                 outputs,\n",
    "#                 \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "#                 n_heads=llama_model.config.num_attention_heads,\n",
    "#             )\n",
    "\n",
    "#         for rel_pos, heads in relative_pos.items():\n",
    "#             curr_layer_heads = [h for l, h in heads if l == int(layer.split(\".\")[2])]   \n",
    "\n",
    "#             if rel_pos == -1:\n",
    "#                 for batch in range(inputs.size(0)):\n",
    "#                     prev_query_box_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "#                         input_tokens[\"input_ids\"][batch],\n",
    "#                         input_tokens[\"last_token_indices\"][batch]\n",
    "#                     )\n",
    "# #                 if \"v_proj\" in layer:\n",
    "# #                     for head in curr_layer_heads:\n",
    "# #                         outputs[:, prev_query_box_pos, head] = cache[:, prev_query_box_pos, head]\n",
    "\n",
    "# #                 if \"k_proj\" in layer:\n",
    "# #                     for head in curr_layer_heads:\n",
    "# #                         outputs[:, prev_query_box_pos, head] = cache[:, prev_query_box_pos, head]\n",
    "\n",
    "# #                 if \"q_proj\" in layer:\n",
    "# #                     for head in curr_layer_heads:\n",
    "# #                         outputs[:, prev_query_box_pos, head] = cache[:, prev_query_box_pos, head]\n",
    "#             else:\n",
    "# #                 pos = outputs.size(1) - rel_pos - 1\n",
    "# #                 for batch in range(inputs.size(0)):\n",
    "# #                     prev_query_box_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "# #                         input_tokens[\"input_ids\"][batch],\n",
    "# #                         input_tokens[\"last_token_indices\"][batch]\n",
    "# #                     )\n",
    "# #                     correct_obj_pos = prev_query_box_pos - 4\n",
    "# #                 if \"v_proj\" in layer:\n",
    "# #                     for head_idx in curr_layer_heads:\n",
    "# #                         outputs[:, :, head_idx] = g_cache[:, :, head_idx]\n",
    "\n",
    "# #                 if \"k_proj\" in layer:\n",
    "# #                     for head in curr_layer_heads:\n",
    "# #                         outputs[:, :, head] = g_cache[:, :, head]\n",
    "\n",
    "#                 if \"q_proj\" in layer:\n",
    "#                     for head in curr_layer_heads:\n",
    "#                         outputs[:, -1, head] = g_cache[:, -1, head]\n",
    "\n",
    "#         outputs = rearrange(\n",
    "#                     outputs,\n",
    "#                     \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\",\n",
    "#                     n_heads=llama_model.config.num_attention_heads,\n",
    "#                 )\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "089e404e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Fetcher Heads: 39\n",
      "Heads affecting direct logit heads: 7\n",
      "Heads at query box token: 17\n",
      "Heads at prev query box token: 4\n"
     ]
    }
   ],
   "source": [
    "with open(\"./new_masks/llama-7b/direct_logit_heads/object_value/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    value_fetcher_heads = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "with open(\"./new_masks/llama-7b/heads_affect_direct_logit/positional/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    positional_info_fetcher_heads = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "with open(\"./new_masks/llama-7b/heads_at_query_box_pos/positional/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    duplicate_token_heads = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "print(f\"Value Fetcher Heads: {len(value_fetcher_heads)}\")\n",
    "print(f\"Heads affecting direct logit heads: {len(positional_info_fetcher_heads)}\")\n",
    "print(f\"Heads at query box token: {len(duplicate_token_heads)}\")\n",
    "print(f\"Heads at prev query box token: {len(circuit_heads['heads_at_prev_box_pos'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee702e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_pos = {}\n",
    "relative_pos[0] = value_fetcher_heads\n",
    "relative_pos[0] += positional_info_fetcher_heads\n",
    "relative_pos[2] = duplicate_token_heads\n",
    "relative_pos[-1] = circuit_heads['heads_at_prev_box_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c95c5393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:32,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_count, total_count = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        with TraceDict(llama_model, llama_modules, retain_input=True, edit_output=partial(\n",
    "                                                                        cross_model_patching,\n",
    "                                                                        bi = bi,\n",
    "                                                                        relative_pos = relative_pos,\n",
    "                                                                        input_tokens = inputs)) as trace:\n",
    "                outputs = llama_model(inputs[\"input_ids\"], output_attentions=True)\n",
    "\n",
    "        for batch in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][batch]\n",
    "            pred = torch.argmax(outputs.logits[batch][inputs[\"last_token_indices\"][batch]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            total_count += 1\n",
    "\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "current_acc = round(correct_count / total_count, 2)\n",
    "print(f\"Task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb4a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7627498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "015915a2",
   "metadata": {},
   "source": [
    "Final fine-tuned model:\n",
    "\n",
    "output patching\n",
    "- full circuit: 0.73\n",
    "- Value Fetcher: 0.74\n",
    "- Position Transmitter: 0.68\n",
    "- Position Detector: 0.66\n",
    "- Structure Reader: 0.66\n",
    "\n",
    "input patching:\n",
    "- Value Fetcher\n",
    "    - query: 0.72\n",
    "    - key: 0.65\n",
    "    - value: 0.67\n",
    "    - keys + final query: 0.73\n",
    "    - keys + final query + value: 0.74\n",
    "- Position Transmitter:\n",
    "    - query: 0.68\n",
    "    - key: 0.66\n",
    "    - value: 0.65\n",
    "    - keys + final query: 0.68\n",
    "    - keys + final query + value: 0.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887138b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91bc689b",
   "metadata": {},
   "source": [
    "## Patching Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c6b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_weights(inputs=None, output=None, layer=None, bi=None):\n",
    "    inputs = inputs[0]\n",
    "    value_fetcher_curr_layer = [h for l, h in value_fetcher_heads if l == int(layer.split(\".\")[2])]\n",
    "    position_trans_curr_layer = [h for l, h in positional_info_fetcher_heads if l == int(layer.split(\".\")[2])]\n",
    "\n",
    "    if (\"q_proj\" in layer) and len(value_fetcher_curr_layer) > 0:\n",
    "        llama_w = llama_model.state_dict()[f\"{layer}.weight\"].clone()\n",
    "        fine_tuned_w = fine_tuned_model.state_dict()[f\"{layer}.weight\"].clone()\n",
    "\n",
    "        llama_w = rearrange(llama_w, \n",
    "                           \"d_model (n_heads d_head) -> d_model n_heads d_head\", \n",
    "                           n_heads=llama_model.config.num_attention_heads,\n",
    "                           )\n",
    "        fine_tuned_w = rearrange(fine_tuned_w, \n",
    "                           \"d_model (n_heads d_head) -> d_model n_heads d_head\", \n",
    "                           n_heads=fine_tuned_model.config.num_attention_heads,\n",
    "                           )\n",
    "\n",
    "#         for head_idx in value_fetcher_curr_layer:\n",
    "        llama_w = fine_tuned_w\n",
    "\n",
    "        llama_w = rearrange(llama_w, \n",
    "                           \"d_model n_heads d_head -> d_model (n_heads d_head)\", \n",
    "                           n_heads=llama_model.config.num_attention_heads,\n",
    "                           )\n",
    "\n",
    "        new_output = einsum(\n",
    "            inputs, llama_w, \"batch seq_len hidden_size, d_model hidden_size -> batch seq_len d_model\"\n",
    "        )\n",
    "        output = torch.cat((output[:, :-1], new_output[:, -1].unsqueeze(dim=1)), dim=1)\n",
    "\n",
    "    if (\"o_proj\" in layer) and len(position_trans_curr_layer) > 0:\n",
    "        inputs = rearrange(\n",
    "                inputs,\n",
    "                \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "        \n",
    "        g_cache = rearrange(\n",
    "                fine_tuned_cache[bi][layer],\n",
    "                \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "        \n",
    "        for head in position_trans_curr_layer:\n",
    "            inputs[:, -1, head] = g_cache[:, -1, head]\n",
    "        \n",
    "        inputs = rearrange(\n",
    "                inputs,\n",
    "                \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "        w_o = llama_model.state_dict()[f\"{layer}.weight\"]\n",
    "        output = einsum(\n",
    "            inputs, w_o, \"batch seq_len hidden_size, d_model hidden_size -> batch seq_len d_model\"\n",
    "        )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9aa2800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 1/63 [00:00<00:26,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 2/63 [00:00<00:25,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▍         | 3/63 [00:01<00:25,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▋         | 4/63 [00:01<00:24,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 5/63 [00:02<00:24,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|▉         | 6/63 [00:02<00:23,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█         | 7/63 [00:02<00:23,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 8/63 [00:03<00:23,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|█▍        | 9/63 [00:03<00:22,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|█▌        | 10/63 [00:04<00:22,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█▋        | 11/63 [00:04<00:21,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 19%|█▉        | 12/63 [00:05<00:21,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|██        | 13/63 [00:05<00:21,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|██▏       | 14/63 [00:05<00:20,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|██▍       | 15/63 [00:06<00:20,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▌       | 16/63 [00:06<00:19,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██▋       | 17/63 [00:07<00:19,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|██▊       | 18/63 [00:07<00:18,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|███       | 19/63 [00:08<00:18,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|███▏      | 20/63 [00:08<00:18,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 21/63 [00:08<00:17,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|███▍      | 22/63 [00:09<00:17,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 37%|███▋      | 23/63 [00:09<00:16,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|███▊      | 24/63 [00:10<00:16,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|███▉      | 25/63 [00:10<00:15,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 41%|████▏     | 26/63 [00:10<00:15,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 43%|████▎     | 27/63 [00:11<00:15,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|████▍     | 28/63 [00:11<00:14,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 46%|████▌     | 29/63 [00:12<00:14,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 48%|████▊     | 30/63 [00:12<00:13,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 49%|████▉     | 31/63 [00:13<00:13,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 51%|█████     | 32/63 [00:13<00:13,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|█████▏    | 33/63 [00:13<00:12,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 54%|█████▍    | 34/63 [00:14<00:12,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|█████▌    | 35/63 [00:14<00:11,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|█████▋    | 36/63 [00:15<00:11,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 59%|█████▊    | 37/63 [00:15<00:10,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████    | 38/63 [00:16<00:10,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 62%|██████▏   | 39/63 [00:16<00:10,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 63%|██████▎   | 40/63 [00:16<00:09,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 65%|██████▌   | 41/63 [00:17<00:09,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n",
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 42/63 [00:17<00:09,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 68%|██████▊   | 43/63 [00:18<00:08,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|██████▉   | 44/63 [00:18<00:08,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|███████▏  | 45/63 [00:19<00:07,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 73%|███████▎  | 46/63 [00:19<00:07,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|███████▍  | 47/63 [00:19<00:06,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 76%|███████▌  | 48/63 [00:20<00:06,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|███████▊  | 49/63 [00:20<00:05,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 79%|███████▉  | 50/63 [00:21<00:05,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 81%|████████  | 51/63 [00:21<00:05,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|████████▎ | 52/63 [00:21<00:04,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 84%|████████▍ | 53/63 [00:22<00:04,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|████████▌ | 54/63 [00:22<00:03,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 87%|████████▋ | 55/63 [00:23<00:03,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%|████████▉ | 56/63 [00:23<00:02,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|█████████ | 57/63 [00:24<00:02,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 92%|█████████▏| 58/63 [00:24<00:02,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 94%|█████████▎| 59/63 [00:24<00:01,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 95%|█████████▌| 60/63 [00:25<00:01,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 97%|█████████▋| 61/63 [00:25<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 98%|█████████▊| 62/63 [00:26<00:00,  2.38it/s]\n",
      "100%|██████████| 63/63 [00:26<00:00,  2.38it/s]\n",
      "63it [00:26,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.q_proj: llama: 108.36489868164062, fine-tuned: 108.38367462158203\n",
      "model.layers.15.self_attn.q_proj: llama: 108.3883056640625, fine-tuned: 108.40525817871094\n",
      "model.layers.16.self_attn.q_proj: llama: 106.8139419555664, fine-tuned: 106.8269271850586\n",
      "model.layers.17.self_attn.q_proj: llama: 104.55338287353516, fine-tuned: 104.56761932373047\n",
      "model.layers.18.self_attn.q_proj: llama: 103.43733215332031, fine-tuned: 103.45516967773438\n",
      "model.layers.19.self_attn.q_proj: llama: 101.43477630615234, fine-tuned: 101.44762420654297\n",
      "model.layers.20.self_attn.q_proj: llama: 102.77690887451172, fine-tuned: 102.79048156738281\n",
      "model.layers.21.self_attn.q_proj: llama: 99.40530395507812, fine-tuned: 99.41356658935547\n",
      "model.layers.22.self_attn.q_proj: llama: 101.00042724609375, fine-tuned: 101.01142883300781\n",
      "model.layers.23.self_attn.q_proj: llama: 97.8895492553711, fine-tuned: 97.90031433105469\n",
      "model.layers.24.self_attn.q_proj: llama: 97.97964477539062, fine-tuned: 98.0158920288086\n",
      "model.layers.28.self_attn.q_proj: llama: 97.06059265136719, fine-tuned: 97.08554077148438\n",
      "model.layers.29.self_attn.q_proj: llama: 96.15409851074219, fine-tuned: 96.20056915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_count, total_count = 0, 0\n",
    "llama_model.eval()\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(tqdm(dataloader))):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        with TraceDict(\n",
    "            llama_model,\n",
    "            llama_modules,\n",
    "            retain_input=True,\n",
    "            edit_output=partial(\n",
    "                patch_weights,\n",
    "                bi=bi,\n",
    "            ),\n",
    "        ) as _:\n",
    "            outputs = llama_model(inputs[\"input_ids\"])\n",
    "\n",
    "        for bi in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][bi]\n",
    "            pred = torch.argmax(outputs.logits[bi][inputs[\"last_token_indices\"][bi]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            # else:\n",
    "            #     print(f\"Label: {tokenizer.decode(label)}, Prediction: {tokenizer.decode(pred)}\")\n",
    "            total_count += 1\n",
    "\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eabf50b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(correct_count/total_count, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b1287",
   "metadata": {},
   "source": [
    "## Patching inputs of Value Fetcher Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2b655a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:29,  2.11it/s]\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_input_cache = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(fine_tuned_model.device)\n",
    "\n",
    "        with TraceDict(fine_tuned_model, llama_modules, retain_input=True) as cache:\n",
    "            _ = fine_tuned_model(inputs[\"input_ids\"])\n",
    "\n",
    "        for _, llama_layer in zip(goat_modules, llama_modules):\n",
    "            if \"q_proj\" in llama_layer:\n",
    "                if bi in fine_tuned_input_cache:\n",
    "                    fine_tuned_input_cache[bi][llama_layer] = cache[llama_layer].input.cpu()\n",
    "                else:\n",
    "                    fine_tuned_input_cache[bi] = {}\n",
    "                    fine_tuned_input_cache[bi][llama_layer] = cache[llama_layer].input.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88aebfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_inputs(inputs=None, output=None, layer=None, bi=None):\n",
    "    inp = inputs[0]\n",
    "    value_fetcher_curr_layer = [h for l, h in value_fetcher_heads if l == int(layer.split(\".\")[2])]\n",
    "    position_trans_curr_layer = [h for l, h in positional_info_fetcher_heads if l == int(layer.split(\".\")[2])]\n",
    "        \n",
    "    if (\"q_proj\" in layer) and len(value_fetcher_curr_layer) > 0:\n",
    "        # Patching inputs of value fetcher heads from fine-tuned to llama\n",
    "        fine_tuned_inp = fine_tuned_input_cache[bi][layer]\n",
    "\n",
    "        # Patching weights from fine-tuned to llama\n",
    "        llama_w = llama_model.state_dict()[f\"{layer}.weight\"].clone()\n",
    "        llama_w = rearrange(llama_w, \n",
    "                           \"(n_heads d_head) d_model -> n_heads d_head d_model\", \n",
    "                           n_heads=fine_tuned_model.config.num_attention_heads)\n",
    "        \n",
    "        fine_tuned_w = fine_tuned_model.state_dict()[f\"{layer}.weight\"].clone()\n",
    "        fine_tuned_w = rearrange(fine_tuned_w, \n",
    "                           \"(n_heads d_head) d_model -> n_heads d_head d_model\", \n",
    "                           n_heads=fine_tuned_model.config.num_attention_heads)\n",
    "\n",
    "        output = rearrange(output, \n",
    "                   \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\", \n",
    "                   n_heads=fine_tuned_model.config.num_attention_heads)\n",
    "\n",
    "        for head_idx in value_fetcher_curr_layer:\n",
    "            res = einsum(inp,\n",
    "                          fine_tuned_w[head_idx, :],\n",
    "                          \"batch seq_len hidden_size, d_head hidden_size -> batch seq_len d_head\")\n",
    "            output[:, -1, head_idx] = res[:, -1]\n",
    "\n",
    "        output = rearrange(output, \n",
    "                       \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\", \n",
    "                       n_heads=llama_model.config.num_attention_heads,\n",
    "                       )\n",
    "            \n",
    "            \n",
    "    if (\"o_proj\" in layer) and len(position_trans_curr_layer) > 0:\n",
    "        inp = rearrange(\n",
    "                inp,\n",
    "                \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "        \n",
    "        g_cache = rearrange(\n",
    "                fine_tuned_cache[bi][layer],\n",
    "                \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "        \n",
    "        for head in position_trans_curr_layer:\n",
    "            inp[:, -1, head] = g_cache[:, -1, head]\n",
    "        \n",
    "        inp = rearrange(\n",
    "                inp,\n",
    "                \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "        w_o = llama_model.state_dict()[f\"{layer}.weight\"]\n",
    "        output = einsum(\n",
    "            inp, w_o, \"batch seq_len hidden_size, d_model hidden_size -> batch seq_len d_model\"\n",
    "        )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aecc97f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/63 [00:00<00:26,  2.34it/s]\n",
      "  3%|▎         | 2/63 [00:00<00:25,  2.36it/s]\n",
      "  5%|▍         | 3/63 [00:01<00:25,  2.36it/s]\n",
      "  6%|▋         | 4/63 [00:01<00:24,  2.36it/s]\n",
      "  8%|▊         | 5/63 [00:02<00:24,  2.37it/s]\n",
      " 10%|▉         | 6/63 [00:02<00:24,  2.37it/s]\n",
      " 11%|█         | 7/63 [00:03<00:28,  1.99it/s]\n",
      " 13%|█▎        | 8/63 [00:03<00:26,  2.09it/s]\n",
      " 14%|█▍        | 9/63 [00:04<00:24,  2.17it/s]\n",
      " 16%|█▌        | 10/63 [00:04<00:23,  2.23it/s]\n",
      " 17%|█▋        | 11/63 [00:04<00:22,  2.27it/s]\n",
      " 19%|█▉        | 12/63 [00:05<00:22,  2.30it/s]\n",
      " 21%|██        | 13/63 [00:05<00:21,  2.31it/s]\n",
      " 22%|██▏       | 14/63 [00:06<00:21,  2.33it/s]\n",
      " 24%|██▍       | 15/63 [00:06<00:20,  2.34it/s]\n",
      " 25%|██▌       | 16/63 [00:07<00:19,  2.35it/s]\n",
      " 27%|██▋       | 17/63 [00:07<00:19,  2.36it/s]\n",
      " 29%|██▊       | 18/63 [00:07<00:19,  2.36it/s]\n",
      " 30%|███       | 19/63 [00:08<00:18,  2.37it/s]\n",
      " 32%|███▏      | 20/63 [00:08<00:18,  2.36it/s]\n",
      " 33%|███▎      | 21/63 [00:09<00:17,  2.36it/s]\n",
      " 35%|███▍      | 22/63 [00:09<00:17,  2.36it/s]\n",
      " 37%|███▋      | 23/63 [00:09<00:16,  2.36it/s]\n",
      " 38%|███▊      | 24/63 [00:10<00:16,  2.37it/s]\n",
      " 40%|███▉      | 25/63 [00:10<00:16,  2.37it/s]\n",
      " 41%|████▏     | 26/63 [00:11<00:15,  2.37it/s]\n",
      " 43%|████▎     | 27/63 [00:11<00:15,  2.37it/s]\n",
      " 44%|████▍     | 28/63 [00:12<00:14,  2.37it/s]\n",
      " 46%|████▌     | 29/63 [00:12<00:14,  2.37it/s]\n",
      " 48%|████▊     | 30/63 [00:12<00:13,  2.37it/s]\n",
      " 49%|████▉     | 31/63 [00:13<00:13,  2.37it/s]\n",
      " 51%|█████     | 32/63 [00:13<00:13,  2.37it/s]\n",
      " 52%|█████▏    | 33/63 [00:14<00:12,  2.36it/s]\n",
      " 54%|█████▍    | 34/63 [00:14<00:12,  2.36it/s]\n",
      " 56%|█████▌    | 35/63 [00:15<00:11,  2.37it/s]\n",
      " 57%|█████▋    | 36/63 [00:15<00:11,  2.37it/s]\n",
      " 59%|█████▊    | 37/63 [00:15<00:10,  2.37it/s]\n",
      " 60%|██████    | 38/63 [00:16<00:10,  2.37it/s]\n",
      " 62%|██████▏   | 39/63 [00:16<00:10,  2.37it/s]\n",
      " 63%|██████▎   | 40/63 [00:17<00:09,  2.37it/s]\n",
      " 65%|██████▌   | 41/63 [00:17<00:09,  2.37it/s]\n",
      " 67%|██████▋   | 42/63 [00:17<00:08,  2.37it/s]\n",
      " 68%|██████▊   | 43/63 [00:18<00:08,  2.37it/s]\n",
      " 70%|██████▉   | 44/63 [00:18<00:08,  2.37it/s]\n",
      " 71%|███████▏  | 45/63 [00:19<00:07,  2.37it/s]\n",
      " 73%|███████▎  | 46/63 [00:19<00:07,  2.37it/s]\n",
      " 75%|███████▍  | 47/63 [00:20<00:06,  2.37it/s]\n",
      " 76%|███████▌  | 48/63 [00:20<00:06,  2.37it/s]\n",
      " 78%|███████▊  | 49/63 [00:21<00:06,  2.15it/s]\n",
      " 79%|███████▉  | 50/63 [00:21<00:05,  2.21it/s]\n",
      " 81%|████████  | 51/63 [00:21<00:05,  2.26it/s]\n",
      " 83%|████████▎ | 52/63 [00:22<00:04,  2.29it/s]\n",
      " 84%|████████▍ | 53/63 [00:22<00:04,  2.32it/s]\n",
      " 86%|████████▌ | 54/63 [00:23<00:03,  2.33it/s]\n",
      " 87%|████████▋ | 55/63 [00:23<00:03,  2.34it/s]\n",
      " 89%|████████▉ | 56/63 [00:24<00:02,  2.35it/s]\n",
      " 90%|█████████ | 57/63 [00:24<00:02,  2.35it/s]\n",
      " 92%|█████████▏| 58/63 [00:24<00:02,  2.36it/s]\n",
      " 94%|█████████▎| 59/63 [00:25<00:01,  2.36it/s]\n",
      " 95%|█████████▌| 60/63 [00:25<00:01,  2.36it/s]\n",
      " 97%|█████████▋| 61/63 [00:26<00:00,  2.36it/s]\n",
      " 98%|█████████▊| 62/63 [00:26<00:00,  2.36it/s]\n",
      "100%|██████████| 63/63 [00:26<00:00,  2.35it/s]\n",
      "63it [00:26,  2.35it/s]\n"
     ]
    }
   ],
   "source": [
    "correct_count, total_count = 0, 0\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(tqdm(dataloader))):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        with TraceDict(\n",
    "            llama_model,\n",
    "            llama_modules,\n",
    "            retain_input=True,\n",
    "            edit_output=partial(\n",
    "                patch_inputs,\n",
    "                bi=bi,\n",
    "            ),\n",
    "        ) as _:\n",
    "            outputs = llama_model(inputs[\"input_ids\"])\n",
    "\n",
    "        for bi in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][bi]\n",
    "            pred = torch.argmax(outputs.logits[bi][inputs[\"last_token_indices\"][bi]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            # else:\n",
    "            #     print(f\"Label: {tokenizer.decode(label)}, Prediction: {tokenizer.decode(pred)}\")\n",
    "            total_count += 1\n",
    "\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b141b61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(correct_count/total_count, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2510d342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
