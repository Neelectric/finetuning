{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014efce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/nikhil_prakash/miniconda/envs/anima/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.nn import CosineSimilarity\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer\n",
    "from functools import partial\n",
    "from baukit import TraceDict\n",
    "from einops import rearrange, einsum\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly_utils import imshow, scatter\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import PeftModel\n",
    "\n",
    "import pysvelte\n",
    "import analysis_utils\n",
    "from counterfactual_datasets.entity_tracking import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(10)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "068509e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loading...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34bcd2bb4c6940b9944d8d228cf9a1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e9beef2d2b4bc38f767c6f70924da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Model Loading...\")\n",
    "# path = \"AlekseyKorshuk/vicuna-7b\"\n",
    "path = \"/data/nikhil_prakash/llama_weights/7B\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(path).to(device)\n",
    "\n",
    "base_model = \"decapoda-research/llama-7b-hf\"\n",
    "lora_weights = \"tiedong/goat-lora-7b\"\n",
    "\n",
    "goat_model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "goat_model = PeftModel.from_pretrained(\n",
    "    goat_model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={'': 0},\n",
    ")\n",
    "\n",
    "llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767b0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./box_datasets/no_instructions/alternative/Random/5/train.jsonl\"\n",
    "object_file = \"./box_datasets/objects_with_bnc_frequency.csv\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bcc0eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 600\n"
     ]
    }
   ],
   "source": [
    "raw_data = entity_tracking_example_sampler(\n",
    "    tokenizer=llama_tokenizer,\n",
    "    num_samples=600,\n",
    "    data_file=data_file,\n",
    "    # object_file=object_file,\n",
    "    few_shot=False,\n",
    "    alt_examples=True,\n",
    "    # num_ents_or_ops=3,\n",
    "    architecture=\"LLaMAForCausalLM\",\n",
    ")\n",
    "\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_data[0],\n",
    "        \"last_token_indices\": raw_data[1],\n",
    "        \"labels\": raw_data[2],\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "\n",
    "print(f\"Length of dataset: {len(dataset)}\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a177adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  The machine is in Box X, the magazine is in Box T, the key is in Box A, the fig is in Box E, the bomb is in Box M. Box X contains the\n",
      "Answer:  machine\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(\n",
    "    f\"Prompt: {llama_tokenizer.decode(dataset[idx]['input_ids'][:dataset[idx]['last_token_indices']+1])}\"\n",
    ")\n",
    "print(f\"Answer: {llama_tokenizer.decode(dataset[idx]['labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ba8f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:19,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "correct_count = 0\n",
    "goat_model.eval()\n",
    "with torch.no_grad():\n",
    "    for _, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(goat_model.device)\n",
    "\n",
    "        outputs = goat_model(input_ids=inputs[\"input_ids\"])\n",
    "\n",
    "        for bi in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][bi]\n",
    "            pred = torch.argmax(outputs.logits[bi][inputs[\"last_token_indices\"][bi]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            # else:\n",
    "            #     print(f\"Label: {tokenizer.decode(label)}, Prediction: {tokenizer.decode(pred)}\")\n",
    "            total_count += 1\n",
    "\n",
    "del outputs\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "current_acc = round(correct_count / total_count, 2)\n",
    "print(f\"Task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "719dc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"./new_pp_exps/5_boxes/reverse\"\n",
    "path = root_path + \"/direct_logit_heads.pt\"\n",
    "logit_values = torch.load(path)\n",
    "direct_logit_heads = analysis_utils.compute_topk_components(torch.load(path), k=35, largest=False)\n",
    "\n",
    "path = root_path + \"/heads_affect_direct_logit.pt\"\n",
    "logit_values = torch.load(path)\n",
    "heads_affecting_direct_logit_heads = analysis_utils.compute_topk_components(\n",
    "    torch.load(path), k=10, largest=False\n",
    ")\n",
    "\n",
    "path = root_path + \"/heads_at_query_box_pos.pt\"\n",
    "logit_values = torch.load(path)\n",
    "head_at_query_box_token = analysis_utils.compute_topk_components(\n",
    "    torch.load(path), k=15, largest=False\n",
    ")\n",
    "\n",
    "path = root_path + \"/heads_at_prev_query_box_pos.pt\"\n",
    "logit_values = torch.load(path)\n",
    "heads_at_prev_box_pos = analysis_utils.compute_topk_components(torch.load(path), k=5, largest=False)\n",
    "\n",
    "path = root_path + \"/heads_at_next_token_to_prev_box_token.pt\"\n",
    "logit_values = torch.load(path)\n",
    "heads_at_next_token_to_box_pos = analysis_utils.compute_topk_components(\n",
    "    torch.load(path), k=5, largest=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885dc16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "247e0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_modules = [[f\"model.layers.{layer}.self_attn.k_proj\", \n",
    "                  f\"model.layers.{layer}.self_attn.q_proj\",\n",
    "                  f\"model.layers.{layer}.self_attn.v_proj\",\n",
    "                 f\"model.layers.{layer}.self_attn.o_proj\"] \n",
    "                 for layer in range(32)]\n",
    "goat_modules = [[f\"base_model.model.model.layers.{layer}.self_attn.k_proj\", \n",
    "                 f\"base_model.model.model.layers.{layer}.self_attn.q_proj\",\n",
    "                 f\"base_model.model.model.layers.{layer}.self_attn.v_proj\",\n",
    "                f\"base_model.model.model.layers.{layer}.self_attn.o_proj\"] \n",
    "                for layer in range(32)]\n",
    "\n",
    "llama_modules = [item for sublist in llama_modules for item in sublist]\n",
    "goat_modules = [item for sublist in goat_modules for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95e4dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:49,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "goat_cache = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(goat_model.device)\n",
    "\n",
    "        with TraceDict(goat_model, goat_modules, retain_input=True) as cache:\n",
    "            _ = goat_model(inputs[\"input_ids\"])\n",
    "        \n",
    "        for llama_layer, goat_layer in zip(llama_modules, goat_modules):\n",
    "            if \"o_proj\" in llama_layer:\n",
    "                if bi in goat_cache:\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].input.cpu()\n",
    "                else:\n",
    "                    goat_cache[bi] = {}\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].input.cpu()\n",
    "            else:\n",
    "                if bi in goat_cache:\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].output.cpu()\n",
    "                else:\n",
    "                    goat_cache[bi] = {}\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007922e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44a686ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_model_patching(inputs, outputs, layer, heads, bi, rel_pos, input_tokens):\n",
    "    if isinstance(inputs, tuple):\n",
    "        inputs = inputs[0]\n",
    "\n",
    "    if isinstance(outputs, tuple):\n",
    "        outputs = outputs[0]\n",
    "\n",
    "    cache = rearrange(\n",
    "                goat_cache[bi][layer],\n",
    "                \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "\n",
    "    curr_layer_heads = [h for l, h in heads if l == int(layer.split(\".\")[2])]\n",
    "    \n",
    "    if \"o_proj\" in layer:\n",
    "        pass\n",
    "#         inputs = rearrange(\n",
    "#                 inputs,\n",
    "#                 \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "#                 n_heads=llama_model.config.num_attention_heads,\n",
    "#             )\n",
    "#         if rel_pos == -1:\n",
    "#             for batch in range(inputs.size(0)):\n",
    "#                 prev_query_box_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "#                     input_tokens[\"input_ids\"][batch],\n",
    "#                     input_tokens[\"last_token_indices\"][batch]\n",
    "#                 )\n",
    "#                 correct_object_pos = prev_query_box_pos - 3\n",
    "#                 for head in curr_layer_heads:\n",
    "#                     inputs[:, correct_object_pos, head] = cache[:, correct_object_pos, head]\n",
    "            \n",
    "#         else:\n",
    "#             pos = outputs.size(1) - rel_pos - 1\n",
    "#             for head in curr_layer_heads:\n",
    "#                 inputs[:, pos, head] = cache[:, pos, head]\n",
    "\n",
    "#         inputs = rearrange(\n",
    "#                 inputs,\n",
    "#                 \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\",\n",
    "#                 n_heads=llama_model.config.num_attention_heads,\n",
    "#             )\n",
    "#         w_o = llama_model.state_dict()[f\"{layer}.weight\"]\n",
    "#         outputs = einsum(\n",
    "#             inputs, w_o, \"batch seq_len hidden_size, d_model hidden_size -> batch seq_len d_model\"\n",
    "#         )\n",
    "\n",
    "    else:\n",
    "        outputs = rearrange(\n",
    "                outputs,\n",
    "                \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "                n_heads=llama_model.config.num_attention_heads,\n",
    "            )\n",
    "        \n",
    "#         if \"v_proj\" in layer:\n",
    "#             for batch in range(outputs.size(0)):\n",
    "#                 prev_query_box_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "#                     input_tokens[\"input_ids\"][batch],\n",
    "#                     input_tokens[\"last_token_indices\"][batch]\n",
    "#                 )\n",
    "#                 pos = prev_query_box_pos - 4\n",
    "# #                 pos = outputs.size(1)\n",
    "#                 for head in curr_layer_heads:\n",
    "#                     outputs[batch, pos, head] = cache[batch, pos, head]\n",
    "\n",
    "#         if \"k_proj\" in layer:\n",
    "#             for batch in range(outputs.size(0)):\n",
    "#                 prev_query_box_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "#                     input_tokens[\"input_ids\"][batch],\n",
    "#                     input_tokens[\"last_token_indices\"][batch]\n",
    "#                 )\n",
    "#                 pos = prev_query_box_pos - 4\n",
    "# #                 pos = outputs.size(1) - 2 - 1\n",
    "#                 for head in curr_layer_heads:\n",
    "#                     outputs[batch, pos, head] = cache[batch, pos, head]\n",
    "\n",
    "        if \"v_proj\" in layer:\n",
    "#             object_pos = [i for i in range(2, 35, 7)]\n",
    "#             for batch in range(outputs.size(0)):\n",
    "#                 prev_query_box_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "#                     input_tokens[\"input_ids\"][batch],\n",
    "#                     input_tokens[\"last_token_indices\"][batch]\n",
    "#                 )\n",
    "#                 pos = prev_query_box_pos - 4\n",
    "            for head in curr_layer_heads:\n",
    "                outputs[:, :, head] = cache[:, :, head]\n",
    "\n",
    "        if \"k_proj\" in layer:\n",
    "#             object_pos = [i for i in range(2, 35, 7)]\n",
    "            for head in curr_layer_heads:\n",
    "                outputs[:, :, head] = cache[:, :, head]\n",
    "\n",
    "        if \"q_proj\" in layer:\n",
    "            pos = outputs.size(1) - rel_pos - 1\n",
    "            for head in curr_layer_heads:\n",
    "                outputs[:, pos, head] = cache[:, pos, head]\n",
    "\n",
    "        outputs = rearrange(\n",
    "                    outputs,\n",
    "                    \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\",\n",
    "                    n_heads=llama_model.config.num_attention_heads,\n",
    "                )\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c95c5393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:19,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_count, total_count = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(goat_model.device)\n",
    "\n",
    "        with TraceDict(llama_model, llama_modules, retain_input=True, edit_output=partial(\n",
    "                                                                        cross_model_patching,\n",
    "                                                                        heads = direct_logit_heads,\n",
    "                                                                        bi = bi,\n",
    "                                                                        rel_pos = 0,\n",
    "                                                                        input_tokens = inputs)) as _:\n",
    "                outputs = llama_model(inputs[\"input_ids\"])\n",
    "        \n",
    "        for batch in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][batch]\n",
    "            pred = torch.argmax(outputs.logits[batch][inputs[\"last_token_indices\"][batch]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            # else:\n",
    "            #     print(f\"Label: {tokenizer.decode(label)}, Prediction: {tokenizer.decode(pred)}\")\n",
    "            total_count += 1\n",
    "        \n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "current_acc = round(correct_count / total_count, 2)\n",
    "print(f\"Task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6609c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "11926ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_pos = [i for i in range(2, 35, 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f8834172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The block is in Box N, the bus is in Box J, the dress is in Box L, the wire is in Box K, the boot is in Box V. Box K contains the'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.decode(inputs[\"input_ids\"][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
