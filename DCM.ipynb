{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f490849ccf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from baukit import TraceDict\n",
    "from einops import rearrange, einsum\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from datasets import Dataset\n",
    "from model_aligner_script import load_data\n",
    "from counterfactual_datasets.entity_tracking import object_alignment_example_sampler\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file ./llama_7b/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"./llama_7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file ./llama_7b/pytorch_model.bin.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.0.dev0\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.11s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at ./llama_7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file ./llama_7b/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.0.dev0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"./llama_7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModelForCausalLM.from_pretrained(path).to(DEVICE)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HEADS = model.config.num_attention_heads\n",
    "HEAD_SIZE = model.config.hidden_size // NUM_HEADS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desiderata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = \"./box_datasets/no_instructions/original/3/train.jsonl\"\n",
    "object_file_path = \"./box_datasets/objects_with_bnc_frequency.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  400\n",
      "Eval size:  50\n",
      "Test size:  50\n"
     ]
    }
   ],
   "source": [
    "objValueFetcher_train, objValueFetcher_eval, objValueFetcher_test = load_data(\n",
    "    tokenizer=tokenizer,\n",
    "    data_size=500,\n",
    "    aligner_func=object_alignment_example_sampler,\n",
    "    data_file=data_file_path,\n",
    "    num_ents_or_ops=3,\n",
    "    batch_size=40,\n",
    "    architecture=\"\",\n",
    "    object_file=object_file_path,\n",
    "    alt_examples=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "desiderata_train = [objValueFetcher_train]\n",
    "desiderata_eval = [objValueFetcher_eval]\n",
    "desiderata_valid = [objValueFetcher_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Box 0 contains the sheet, Box 1 contains the tunic, Box 2 contains the incense. Box 1 contains\n",
      " Box 0 contains the sheet, Box 1 contains the fig, Box 2 contains the incense. Box 1 contains\n",
      " fig\n"
     ]
    }
   ],
   "source": [
    "data = next(enumerate(desiderata_train[0]))[1]\n",
    "bi = 1\n",
    "print(tokenizer.decode(data[\"base_input_ids\"][bi][:data[\"base_input_last_pos\"][bi]]))\n",
    "print(tokenizer.decode(data[\"source_input_ids\"][bi][:data[\"source_input_last_pos\"][bi]]))\n",
    "print(tokenizer.decode(data[\"labels\"][bi]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Binary Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = [f\"model.layers.{i}.self_attn.o_proj\" for i in range(model.config.num_hidden_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_activations_train = {}\n",
    "\n",
    "for di, desid_train in enumerate(desiderata_train):\n",
    "    from_activations_train[di] = {}\n",
    "\n",
    "    for bi, inputs in enumerate(desid_train):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(DEVICE)\n",
    "\n",
    "        from_activations_train[di][bi] = {}\n",
    "        with torch.no_grad():\n",
    "            with TraceDict(model, modules, retain_input=True) as trace:\n",
    "                _ = model(inputs[\"source_input_ids\"])\n",
    "\n",
    "                for module in modules:\n",
    "                    if \"self_attn\" in module:\n",
    "                        from_activations_train[di][bi][module] = trace[module].input.detach().cpu()\n",
    "                    else:\n",
    "                        from_activations_train[di][bi][module] = trace[module].output.detach().cpu()\n",
    "\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(\"cpu\")\n",
    "        \n",
    "        del trace\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_w_heads = []\n",
    "for module in modules:\n",
    "    if \"self_attn\" in module:\n",
    "        for head in range(model.config.num_attention_heads):\n",
    "            modules_w_heads.append(f\"{module}.{head}\")\n",
    "    else:\n",
    "        modules_w_heads.append(module)\n",
    "\n",
    "mask_dict = {module: i for i, module in enumerate(modules_w_heads)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_output(inputs, output, layer, mask, from_activations, to_last_token_pos, from_last_token_pos):\n",
    "    if \"self_attn\" in layer:\n",
    "        inp = inputs[0]\n",
    "        from_activations[layer] = from_activations[layer].to(DEVICE)\n",
    "\n",
    "        # Computing the output of each head in this layer after the intervention\n",
    "        for head_idx in range(NUM_HEADS):\n",
    "            head_start = head_idx * HEAD_SIZE\n",
    "            head_end = (head_idx + 1) * HEAD_SIZE\n",
    "            abl_amt = mask[mask_dict[f\"{layer}.{head_idx}\"]]\n",
    "\n",
    "            for bi in range(inp.shape[0]):\n",
    "                intervention = abl_amt * inp[bi, to_last_token_pos[bi], head_start:head_end].clone() + (1 - abl_amt) * from_activations[layer][bi, from_last_token_pos[bi], head_start:head_end]\n",
    "                inp[bi, to_last_token_pos[bi], head_start:head_end] = intervention\n",
    "\n",
    "        from_activations[layer] = from_activations[layer].to(\"cpu\")\n",
    "\n",
    "        weights = model.state_dict()[f\"{layer}.weight\"]\n",
    "        mod_output = einsum(inp, weights, \"batch seq_len hidden_size, d_model hidden_size -> batch seq_len d_model\")\n",
    "\n",
    "        del weights\n",
    "        torch.cuda.empty_cache()\n",
    "        return mod_output\n",
    "\n",
    "    else:\n",
    "        assert False, \"shouldn't be here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, bi: 0, Loss: -33.346099853515625, Target logits: 5.55768346786499\n",
      "#Zero heads: 0\n",
      "epoch: 0, bi: 1, Loss: -33.410945892333984, Target logits: 5.935157775878906\n",
      "#Zero heads: 0\n",
      "epoch: 0, bi: 2, Loss: -33.81093978881836, Target logits: 6.2650275230407715\n",
      "#Zero heads: 0\n",
      "epoch: 0, bi: 3, Loss: -33.954158782958984, Target logits: 6.501003265380859\n",
      "#Zero heads: 0\n",
      "epoch: 0, bi: 4, Loss: -43.0528564453125, Target logits: 8.271024703979492\n",
      "#Zero heads: 0\n",
      "epoch: 0, bi: 5, Loss: -32.17881393432617, Target logits: 6.680758953094482\n",
      "#Zero heads: 1\n",
      "epoch: 0, bi: 6, Loss: -37.13197326660156, Target logits: 7.72890567779541\n",
      "#Zero heads: 12\n",
      "epoch: 0, bi: 7, Loss: -37.0977783203125, Target logits: 7.933606147766113\n",
      "#Zero heads: 13\n",
      "epoch: 0, bi: 8, Loss: -38.54935836791992, Target logits: 8.368754386901855\n",
      "#Zero heads: 15\n",
      "epoch: 0, bi: 9, Loss: -41.75432586669922, Target logits: 9.07913875579834\n",
      "#Zero heads: 15\n",
      "epoch: 1, bi: 0, Loss: -42.205509185791016, Target logits: 9.314103126525879\n",
      "#Zero heads: 17\n",
      "epoch: 1, bi: 1, Loss: -42.26966094970703, Target logits: 9.477717399597168\n",
      "#Zero heads: 17\n",
      "epoch: 1, bi: 2, Loss: -39.98371887207031, Target logits: 9.204680442810059\n",
      "#Zero heads: 17\n",
      "epoch: 1, bi: 3, Loss: -40.484764099121094, Target logits: 9.379290580749512\n",
      "#Zero heads: 18\n",
      "epoch: 1, bi: 4, Loss: -48.28007888793945, Target logits: 10.764396667480469\n",
      "#Zero heads: 17\n",
      "epoch: 1, bi: 5, Loss: -35.213348388671875, Target logits: 8.64429759979248\n",
      "#Zero heads: 17\n",
      "epoch: 1, bi: 6, Loss: -39.349639892578125, Target logits: 9.370357513427734\n",
      "#Zero heads: 17\n",
      "epoch: 1, bi: 7, Loss: -40.17856216430664, Target logits: 9.541704177856445\n",
      "#Zero heads: 17\n",
      "epoch: 1, bi: 8, Loss: -41.129920959472656, Target logits: 9.7311429977417\n",
      "#Zero heads: 17\n",
      "epoch: 1, bi: 9, Loss: -43.529541015625, Target logits: 10.149754524230957\n",
      "#Zero heads: 18\n",
      "epoch: 2, bi: 0, Loss: -43.292537689208984, Target logits: 10.117583274841309\n",
      "#Zero heads: 18\n",
      "epoch: 2, bi: 1, Loss: -43.60039138793945, Target logits: 10.177176475524902\n",
      "#Zero heads: 18\n",
      "epoch: 2, bi: 2, Loss: -40.963172912597656, Target logits: 9.742195129394531\n",
      "#Zero heads: 18\n",
      "epoch: 2, bi: 3, Loss: -41.380455017089844, Target logits: 9.819262504577637\n",
      "#Zero heads: 18\n",
      "epoch: 2, bi: 4, Loss: -49.28847885131836, Target logits: 11.131339073181152\n",
      "#Zero heads: 18\n",
      "epoch: 2, bi: 5, Loss: -35.806453704833984, Target logits: 8.86458969116211\n",
      "#Zero heads: 18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">36</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">33 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>rounded = torch.round(mask.data)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">34 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"#Zero heads: {</span>(rounded<span style=\"color: #808080; text-decoration-color: #808080\"> </span>==<span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>).nonzero().shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>36 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss.backward()                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">37 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>optimizer.step()                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">39 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">del</span> output                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m36\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m33 \u001b[0m\u001b[2m│   │   │   \u001b[0mrounded = torch.round(mask.data)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m34 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m#Zero heads: \u001b[0m\u001b[33m{\u001b[0m(rounded\u001b[90m \u001b[0m==\u001b[90m \u001b[0m\u001b[94m0\u001b[0m).nonzero().shape[\u001b[94m0\u001b[0m]\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m35 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m36 \u001b[2m│   │   │   \u001b[0mloss.backward()                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m37 \u001b[0m\u001b[2m│   │   │   \u001b[0moptimizer.step()                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m38 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m39 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdel\u001b[0m output                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mbackward\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 487 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mbackward\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = torch.ones(len(modules_w_heads), requires_grad=True, device=DEVICE, dtype=torch.float)\n",
    "optimizer = torch.optim.Adam([mask], lr=1e-1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for di, desid_train in enumerate(desiderata_train):\n",
    "        for bi, inputs in enumerate(desid_train):\n",
    "            mask.data.clamp_(0, 1)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with TraceDict(\n",
    "                model, \n",
    "                modules, \n",
    "                edit_output=partial(\n",
    "                    edit_output, \n",
    "                    mask=mask,\n",
    "                    from_activations=from_activations_train[di][bi], \n",
    "                    to_last_token_pos=inputs[\"base_input_last_pos\"],\n",
    "                    from_last_token_pos=inputs[\"source_input_last_pos\"]\n",
    "                )\n",
    "            ) as _:\n",
    "                output = model(inputs[\"base_input_ids\"].to(DEVICE))\n",
    "\n",
    "            target_logits = 0\n",
    "            for idx in range(inputs[\"base_input_ids\"].size(0)):\n",
    "                target = inputs[\"labels\"][idx]\n",
    "                target_logits += output.logits[idx, inputs[\"base_input_last_pos\"][idx], target]\n",
    "            target_logits /= inputs[\"base_input_ids\"].size(0)\n",
    "\n",
    "            # maximize the target logits => minimize the negative target logits\n",
    "            # minimize the number of heads => maximize #ones in the mask\n",
    "            loss = 6*-target_logits + torch.sum(1 - mask)\n",
    "            print(f\"epoch: {epoch}, bi: {bi}, Loss: {loss.item()}, Target logits: {target_logits.item()}\")\n",
    "            rounded = torch.round(mask.data)\n",
    "            print(f\"#Zero heads: {(rounded == 0).nonzero().shape[0]}\")\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        del output\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_heads = []\n",
    "inverse_mask_dict = {v: k for k, v in mask_dict.items()}\n",
    "\n",
    "for mask_idx in (mask == 0).nonzero()[:, 0]:\n",
    "    layer = inverse_mask_dict[mask_idx.item()]\n",
    "    layer_idx = int(layer.split('.')[2])\n",
    "    head_idx = int(layer.split('.')[-1])\n",
    "    masked_heads.append([layer_idx, head_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[21, 3], [21, 25], [23, 20], [23, 24], [24, 5], [24, 7], [28, 17]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "masked_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del output\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded = torch.round(mask)\n",
    "(rounded == 0).nonzero().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Learned Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rounded = [torch.round(mask) for mask in masks.values()]\n",
    "# (rounded[0] == 0).nonzero().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = [f\"model.layers.{i}.self_attn.o_proj\" for i in range(model.config.num_hidden_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_activations_valid = {}\n",
    "\n",
    "for di, desid_train in enumerate(desiderata_train):\n",
    "    source_activations_valid[di] = {}\n",
    "\n",
    "    for bi, inputs in enumerate(desid_train):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(DEVICE)\n",
    "\n",
    "        source_activations_valid[di][bi] = {}\n",
    "        with torch.no_grad():\n",
    "            with TraceDict(model, modules, retain_input=True) as trace:\n",
    "                _ = model(inputs[\"base_input_ids\"])\n",
    "\n",
    "                for module in modules:\n",
    "                    if \"self_attn\" in module:\n",
    "                        source_activations_valid[di][bi][module] = trace[module].input.detach().cpu()\n",
    "                    else:\n",
    "                        source_activations_valid[di][bi][module] = trace[module].output.detach().cpu()\n",
    "\n",
    "        del trace\n",
    "        torch.cuda.empty_cache()\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for round_mask in [torch.ones(model.config.num_hidden_layers * model.config.num_attention_heads)]:\n",
    "        count, total = 0, 0\n",
    "        for di, desid_valid in enumerate(desiderata_train):\n",
    "            accuracy = []\n",
    "            for bi, inputs in enumerate(desid_train):\n",
    "                with TraceDict(\n",
    "                    model,\n",
    "                    modules,\n",
    "                    edit_output=partial(\n",
    "                        dummy_edit,\n",
    "                        mask=round_mask,\n",
    "                        from_activations=source_activations_valid[di][bi],\n",
    "                        to_last_token_pos=inputs[\"source_input_last_pos\"],\n",
    "                        from_last_token_pos=inputs[\"base_input_last_pos\"],\n",
    "                    ),\n",
    "                ) as _:\n",
    "                    outputs = model(inputs['source_input_ids'].to(DEVICE))\n",
    "\n",
    "                for i in range(inputs['source_input_ids'].size(0)):\n",
    "                    logits = outputs.logits[i, inputs['source_input_last_pos'][i]]\n",
    "                    pred = torch.argmax(logits, dim=-1)\n",
    "                    label = inputs['labels'][i]\n",
    "\n",
    "                    if pred == label:\n",
    "                        count += 1\n",
    "\n",
    "                total += inputs['source_input_ids'].size(0)\n",
    "            \n",
    "        print(f'Accuracy: {count / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nothing\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Box 0 contains jacket, Box 1 contains nothing, Box 2 contains lantern. Box 2 contains\"\n",
    "tokens = tokenizer(prompt, return_tensors='pt').input_ids.to(DEVICE)\n",
    "output = model(tokens)\n",
    "pred = torch.argmax(output.logits[0, -1], dim=-1)\n",
    "print(tokenizer.decode(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Box 0 contains cash, Box 1 contains contrabass, Box 2 contains nametag. Box 0 contains</s></s></s>'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['source_input_ids'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anima",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
