{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from torch.nn import CosineSimilarity\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer\n",
    "from functools import partial\n",
    "from baukit import TraceDict\n",
    "from einops import rearrange, einsum\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly_utils import imshow, scatter\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import seaborn as sns\n",
    "from peft import PeftModel\n",
    "import pickle\n",
    "\n",
    "import pysvelte\n",
    "import analysis_utils\n",
    "from counterfactual_datasets.entity_tracking import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed = 30\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "transformers.set_seed(seed)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loading...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfbc92637b094296bb6958c1e8589a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Model Loading...\")\n",
    "# path = \"AlekseyKorshuk/vicuna-7b\"\n",
    "# path = \"/data/nikhil_prakash/goat-finetuning/drawn-moon-15/\"\n",
    "# path = \"/data/nikhil_prakash/llama_weights/7B\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(path).to(device)\n",
    "\n",
    "base_model = \"decapoda-research/llama-7b-hf\"\n",
    "lora_weights = \"tiedong/goat-lora-7b\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"hf-internal-testing/llama-tokenizer\", padding_side=\"right\"\n",
    ")\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./box_datasets/no_instructions/alternative/Random/7/train.jsonl\"\n",
    "object_file = \"./box_datasets/filtered_objects_with_bnc_frequency.csv\"\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 500\n"
     ]
    }
   ],
   "source": [
    "raw_data = entity_tracking_example_sampler(\n",
    "    tokenizer=tokenizer,\n",
    "    num_samples=500,\n",
    "    data_file=data_file,\n",
    "    # object_file=object_file,\n",
    "    few_shot=False,\n",
    "    alt_examples=True,\n",
    "    # num_ents_or_ops=3,\n",
    "    architecture=\"LLaMAForCausalLM\",\n",
    ")\n",
    "\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_data[0],\n",
    "        \"last_token_indices\": raw_data[1],\n",
    "        \"labels\": raw_data[2],\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "\n",
    "print(f\"Length of dataset: {len(dataset)}\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <s>The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E, the bill is in Box M, the cross is in Box K, the map is in Box D. Box X contains the\n",
      "Answer: document\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(\n",
    "    f\"Prompt: {tokenizer.decode(dataset[idx]['input_ids'][:dataset[idx]['last_token_indices']+1])}\"\n",
    ")\n",
    "print(f\"Answer: {tokenizer.decode(dataset[idx]['labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:20,  2.24s/it]\n",
      " 20%|██        | 2/10 [00:04<00:17,  2.15s/it]\n",
      " 30%|███       | 3/10 [00:06<00:14,  2.13s/it]\n",
      " 40%|████      | 4/10 [00:08<00:12,  2.11s/it]\n",
      " 50%|█████     | 5/10 [00:10<00:10,  2.10s/it]\n",
      " 60%|██████    | 6/10 [00:12<00:08,  2.10s/it]\n",
      " 70%|███████   | 7/10 [00:14<00:06,  2.10s/it]\n",
      " 80%|████████  | 8/10 [00:16<00:04,  2.10s/it]\n",
      " 90%|█████████ | 9/10 [00:18<00:02,  2.09s/it]\n",
      "100%|██████████| 10/10 [00:21<00:00,  2.11s/it]\n",
      "10it [00:21,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "correct_count = 0\n",
    "model.eval()\n",
    "errors = defaultdict(int)\n",
    "with torch.no_grad():\n",
    "    for _, output in tqdm(enumerate(tqdm(dataloader))):\n",
    "        for k, v in output.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                output[k] = v.to(model.device)\n",
    "\n",
    "        outputs = model(input_ids=output[\"input_ids\"])\n",
    "\n",
    "        for bi in range(output[\"labels\"].size(0)):\n",
    "            label = output[\"labels\"][bi]\n",
    "            pred = torch.argmax(outputs.logits[bi][output[\"last_token_indices\"][bi]])\n",
    "            box_label = output[\"input_ids\"][bi][output[\"last_token_indices\"][bi] - 2]\n",
    "            prev_box_label_pos = output[\"input_ids\"][bi].eq(box_label).nonzero()[:, 0][0].item()\n",
    "            prev_box_label_index = prev_box_label_pos // 8 + 1\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            else:\n",
    "                errors[prev_box_label_index] += 1\n",
    "                # print(prev_box_label_pos, prev_box_label_index)\n",
    "                # print(f\"Label: {tokenizer.decode(label)}, Prediction: {tokenizer.decode(pred)}\")\n",
    "            total_count += 1\n",
    "\n",
    "del outputs\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "current_acc = round(correct_count / total_count, 2)\n",
    "print(f\"Task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Counterfactual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boxes = 7\n",
    "raw_data = box_index_aligner_examples(\n",
    "    tokenizer,\n",
    "    num_samples=50,\n",
    "    data_file=f\"./box_datasets/no_instructions/alternative/Random/{num_boxes}/train.jsonl\",\n",
    "    # object_file=\"./box_datasets/objects_with_bnc_frequency.csv\",\n",
    "    architecture=\"LLaMAForCausalLM\",\n",
    "    few_shot=False,\n",
    "    alt_examples=True,\n",
    "    num_ents_or_ops=num_boxes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokens = raw_data[0]\n",
    "base_last_token_indices = raw_data[1]\n",
    "source_tokens = raw_data[2]\n",
    "source_last_token_indices = raw_data[3]\n",
    "correct_answer_token = raw_data[4]\n",
    "# incorrect_answer_token = raw_data[6]\n",
    "\n",
    "base_tokens = torch.cat([t.unsqueeze(dim=0) for t in base_tokens], dim=0).to(device)\n",
    "source_tokens = torch.cat([t.unsqueeze(dim=0) for t in source_tokens], dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>The paper is in Box D, the shell is in Box U, the car is in Box V, the television is in Box O, the drink is in Box S, the fan is in Box C, the sheet is in Box Z. Box V contains the\n",
      "<s>The clock is in Box M, the bomb is in Box J, the newspaper is in Box G, the letter is in Box L, the suit is in Box Y, the computer is in Box R, the wheel is in Box V. Box R contains the\n",
      "car\n",
      "\n",
      "<s>The paper is in Box D, the shell is in Box U, the car is in Box V, the television is in Box O, the drink is in Box S, the fan is in Box C, the sheet is in Box Z. Box O contains the\n",
      "<s>The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E, the bill is in Box M, the cross is in Box K, the map is in Box D. Box S contains the\n",
      "television\n",
      "\n",
      "<s>The paper is in Box D, the shell is in Box U, the car is in Box V, the television is in Box O, the drink is in Box S, the fan is in Box C, the sheet is in Box Z. Box S contains the\n",
      "<s>The ticket is in Box N, the book is in Box J, the gift is in Box W, the coat is in Box Y, the rose is in Box K, the wheel is in Box G, the brick is in Box V. Box I contains the\n",
      "drink\n",
      "\n",
      "<s>The paper is in Box D, the shell is in Box U, the car is in Box V, the television is in Box O, the drink is in Box S, the fan is in Box C, the sheet is in Box Z. Box C contains the\n",
      "<s>The paper is in Box D, the shell is in Box U, the car is in Box V, the television is in Box O, the drink is in Box S, the fan is in Box C, the sheet is in Box Z. Box M contains the\n",
      "fan\n",
      "\n",
      "<s>The paper is in Box D, the shell is in Box U, the car is in Box V, the television is in Box O, the drink is in Box S, the fan is in Box C, the sheet is in Box Z. Box Z contains the\n",
      "<s>The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E, the bill is in Box M, the cross is in Box K, the map is in Box D. Box W contains the\n",
      "sheet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(-6, -1):\n",
    "    print(tokenizer.decode(raw_data[0][i][: raw_data[1][i] + 1]))\n",
    "    print(tokenizer.decode(raw_data[2][i][: raw_data[3][i] + 1]))\n",
    "    print(tokenizer.decode(raw_data[4][i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Path Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_points = [\n",
    "    f\"base_model.model.model.layers.{layer}.self_attn.o_proj\" for layer in range(model.config.num_hidden_layers)\n",
    "]\n",
    "\n",
    "# hook_points += [f\"model.layers.{layer}.mlp\" for layer in range(model.config.num_hidden_layers)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Step 1\n",
    "    with TraceDict(\n",
    "        model,\n",
    "        hook_points,\n",
    "        retain_input=True,\n",
    "    ) as clean_cache:\n",
    "        _ = model(base_tokens)\n",
    "\n",
    "    with TraceDict(\n",
    "        model,\n",
    "        hook_points,\n",
    "        retain_input=True,\n",
    "    ) as corrupt_cache:\n",
    "        _ = model(source_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patching_heads(\n",
    "    inputs,\n",
    "    output,\n",
    "    layer,\n",
    "    sender_layer,\n",
    "    sender_head,\n",
    "    clean_last_token_indices,\n",
    "    corrupt_last_token_indices,\n",
    "    rel_pos,\n",
    "):\n",
    "    \"\"\"\n",
    "    rel_pos: Represents the token position relative to the \"real\" (non-padded) last token in the sequence. All the heads at this position and subsequent positions need to patched from clean run, except the sender head at this position.\n",
    "    \"\"\"\n",
    "\n",
    "    input = inputs[0]\n",
    "    batch_size = input.size(0)\n",
    "\n",
    "    if \"o_proj\" in layer:\n",
    "        input = rearrange(\n",
    "            input,\n",
    "            \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "            n_heads=model.config.num_attention_heads,\n",
    "        )\n",
    "        clean_head_outputs = rearrange(\n",
    "            clean_cache[layer].input,\n",
    "            \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "            n_heads=model.config.num_attention_heads,\n",
    "        )\n",
    "        corrupt_head_outputs = rearrange(\n",
    "            corrupt_cache[layer].input,\n",
    "            \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "            n_heads=model.config.num_attention_heads,\n",
    "        )\n",
    "\n",
    "        layer = int(layer.split(\".\")[4])\n",
    "        if sender_layer == layer:\n",
    "            for bi in range(batch_size):\n",
    "                if rel_pos == -1:\n",
    "                    # Computing the previous query box label token position\n",
    "                    clean_prev_box_label_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "                        base_tokens[bi], clean_last_token_indices[bi]\n",
    "                    )\n",
    "#                     corrupt_prev_box_label_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "#                         source_tokens[bi], corrupt_last_token_indices[bi]\n",
    "#                     )\n",
    "\n",
    "                    # Since, queery box may not present in the corrupt prompt, patch in\n",
    "                    # the output of heads from any random box label token\n",
    "                    corrupt_prev_box_label_pos = random.choice(range(6, 49, 7))\n",
    "                else:\n",
    "                    clean_prev_box_label_pos = clean_last_token_indices[bi] - rel_pos\n",
    "                    corrupt_prev_box_label_pos = corrupt_last_token_indices[bi] - rel_pos\n",
    "\n",
    "                for pos in range(clean_prev_box_label_pos, clean_last_token_indices[bi] + 1):\n",
    "                    for head_ind in range(model.config.num_attention_heads):\n",
    "                        if head_ind == sender_head and pos == clean_prev_box_label_pos:\n",
    "                            input[bi, pos, sender_head] = corrupt_head_outputs[\n",
    "                                bi, corrupt_prev_box_label_pos, sender_head\n",
    "                            ]\n",
    "                        else:\n",
    "                            input[bi, pos, head_ind] = clean_head_outputs[bi, pos, head_ind]\n",
    "\n",
    "        else:\n",
    "            for bi in range(batch_size):\n",
    "                if rel_pos == -1:\n",
    "                    # Computing the previous query box label token position\n",
    "                    clean_prev_box_label_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "                        base_tokens[bi], clean_last_token_indices[bi]\n",
    "                    )\n",
    "                else:\n",
    "                    clean_prev_box_label_pos = clean_last_token_indices[bi] - rel_pos\n",
    "\n",
    "                for pos in range(clean_prev_box_label_pos, clean_last_token_indices[bi] + 1):\n",
    "                    input[bi, pos] = clean_head_outputs[bi, pos]\n",
    "\n",
    "        input = rearrange(\n",
    "            input,\n",
    "            \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\",\n",
    "            n_heads=model.config.num_attention_heads,\n",
    "        )\n",
    "\n",
    "        w_o = model.base_model.model.model.layers[layer].self_attn.o_proj.weight\n",
    "        output = einsum(\n",
    "            input,\n",
    "            w_o,\n",
    "            \"batch seq_len hidden_size, d_model hidden_size -> batch seq_len d_model\",\n",
    "        )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patching_receiver_heads(\n",
    "    output, layer, patched_cache, receiver_heads, clean_last_token_indices, rel_pos\n",
    "):\n",
    "    batch_size = output.size(0)\n",
    "    receiver_heads_in_curr_layer = [h for l, h in receiver_heads if l == int(layer.split(\".\")[4])]\n",
    "\n",
    "    output = rearrange(\n",
    "        output,\n",
    "        \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "        n_heads=model.config.num_attention_heads,\n",
    "    )\n",
    "    patched_head_outputs = rearrange(\n",
    "        patched_cache[layer].output,\n",
    "        \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "        n_heads=model.config.num_attention_heads,\n",
    "    )\n",
    "\n",
    "    # Patch in the output of the receiver heads from patched run\n",
    "    for receiver_head in receiver_heads_in_curr_layer:\n",
    "        for bi in range(batch_size):\n",
    "            if rel_pos == -1:\n",
    "                # Computing the previous query box label token position\n",
    "                clean_prev_box_label_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "                    base_tokens[bi], clean_last_token_indices[bi]\n",
    "                )\n",
    "            else:\n",
    "                clean_prev_box_label_pos = clean_last_token_indices[bi] - rel_pos\n",
    "\n",
    "            output[bi, clean_prev_box_label_pos, receiver_head] = patched_head_outputs[\n",
    "                bi, clean_prev_box_label_pos, receiver_head\n",
    "            ]\n",
    "\n",
    "    output = rearrange(\n",
    "        output,\n",
    "        \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\",\n",
    "        n_heads=model.config.num_attention_heads,\n",
    "    )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base_model.model.model.layers.13.self_attn.v_proj', 'base_model.model.model.layers.6.self_attn.v_proj', 'base_model.model.model.layers.9.self_attn.v_proj', 'base_model.model.model.layers.7.self_attn.v_proj', 'base_model.model.model.layers.11.self_attn.v_proj', 'base_model.model.model.layers.10.self_attn.v_proj', 'base_model.model.model.layers.1.self_attn.v_proj']\n",
      "[[10, 3], [13, 14], [9, 2], [9, 7], [11, 23], [6, 10], [11, 24], [9, 10], [1, 9], [7, 17]]\n"
     ]
    }
   ],
   "source": [
    "receiver_heads = heads_at_query_box_pos\n",
    "\n",
    "receiver_layers = list(\n",
    "    set([f\"base_model.model.model.layers.{layer}.self_attn.v_proj\" for layer, _ in receiver_heads])\n",
    ")\n",
    "\n",
    "# receiver_heads = [[layer, head] for layer, head in [[21, 3]]]\n",
    "\n",
    "print(receiver_layers)\n",
    "print(receiver_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [1:36:50<00:00, 181.57s/it]\n"
     ]
    }
   ],
   "source": [
    "path_patching_logits = torch.zeros(\n",
    "    model.config.num_hidden_layers, model.config.num_attention_heads\n",
    ").to(device)\n",
    "batch_size = base_tokens.size(0)\n",
    "apply_softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "for layer in tqdm(range(model.config.num_hidden_layers)):\n",
    "    for head in range(model.config.num_attention_heads):\n",
    "        with torch.no_grad():\n",
    "            # Step 2\n",
    "            with TraceDict(\n",
    "                model,\n",
    "                hook_points + receiver_layers,\n",
    "                retain_input=True,\n",
    "                edit_output=partial(\n",
    "                    patching_heads,\n",
    "                    sender_layer=layer,\n",
    "                    sender_head=head,\n",
    "                    clean_last_token_indices=base_last_token_indices,\n",
    "                    corrupt_last_token_indices=source_last_token_indices,\n",
    "                    rel_pos=-1,\n",
    "                ),\n",
    "            ) as patched_cache:\n",
    "                _ = model(base_tokens)\n",
    "\n",
    "            # Step 3\n",
    "            with TraceDict(\n",
    "                model,\n",
    "                receiver_layers,\n",
    "                retain_input=True,\n",
    "                edit_output=partial(\n",
    "                    patching_receiver_heads,\n",
    "                    patched_cache=patched_cache,\n",
    "                    receiver_heads=receiver_heads,\n",
    "                    clean_last_token_indices=base_last_token_indices,\n",
    "                    rel_pos=-1,\n",
    "                ),\n",
    "            ) as _:\n",
    "                patched_out = model(base_tokens)\n",
    "\n",
    "            for bi in range(batch_size):\n",
    "                logits = apply_softmax(patched_out.logits[bi, base_last_token_indices[bi]])\n",
    "                path_patching_logits[layer, head] += (logits[correct_answer_token[bi]]).item()\n",
    "\n",
    "            path_patching_logits[layer, head] = path_patching_logits[layer, head] / batch_size\n",
    "\n",
    "del patched_out\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(path_patching_logits, \"./new_pp_exps/post_submission/goat-7b/seed_30/heads_at_prev_query_box_pos.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head influencing object info fetcher heads: [[10, 3], [13, 14], [9, 2], [9, 7], [11, 23], [6, 10], [11, 24], [9, 10], [1, 9], [7, 17]]\n"
     ]
    }
   ],
   "source": [
    "heads_at_prev_query_box_pos = analysis_utils.compute_topk_components(\n",
    "    path_patching_logits, k=5,largest=False\n",
    ")\n",
    "print(f\"Head influencing object info fetcher heads: {heads_at_query_box_pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./box_datasets/no_instructions/alternative/Random/7/train.jsonl\"\n",
    "object_file = \"./box_datasets/filtered_objects_with_bnc_frequency.csv\"\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 500\n"
     ]
    }
   ],
   "source": [
    "raw_data = generate_data_for_eval(\n",
    "    tokenizer=tokenizer,\n",
    "    num_samples=3500,\n",
    "    data_file=data_file,\n",
    "    num_boxes=7,\n",
    ")\n",
    "\n",
    "ablate_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_data[0],\n",
    "        \"last_token_indices\": raw_data[1],\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "\n",
    "print(f\"Length of dataset: {len(ablate_dataset)}\")\n",
    "\n",
    "ablate_dataloader = DataLoader(ablate_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <s>The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E, the bill is in Box M, the cross is in Box K, the map is in Box D. Box X contains the\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(\n",
    "    f\"Prompt: {tokenizer.decode(dataset[idx]['input_ids'][:dataset[idx]['last_token_indices']+1])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:19,  2.15s/it]\n",
      " 20%|██        | 2/10 [00:04<00:17,  2.13s/it]\n",
      " 30%|███       | 3/10 [00:06<00:14,  2.12s/it]\n",
      " 40%|████      | 4/10 [00:08<00:12,  2.11s/it]\n",
      " 50%|█████     | 5/10 [00:10<00:10,  2.11s/it]\n",
      " 60%|██████    | 6/10 [00:12<00:08,  2.11s/it]\n",
      " 70%|███████   | 7/10 [00:14<00:06,  2.11s/it]\n",
      " 80%|████████  | 8/10 [00:16<00:04,  2.11s/it]\n",
      " 90%|█████████ | 9/10 [00:19<00:02,  2.11s/it]\n",
      "100%|██████████| 10/10 [00:21<00:00,  2.11s/it]\n",
      "10it [00:21,  2.11s/it]\n"
     ]
    }
   ],
   "source": [
    "if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "    modules = [f\"model.layers.{layer}.self_attn.o_proj\" for layer in range(32)]\n",
    "else:\n",
    "    modules = [f\"base_model.model.model.layers.{layer}.self_attn.o_proj\" for layer in range(32)]\n",
    "\n",
    "mean_activations = {}\n",
    "with torch.no_grad():\n",
    "    # Assuming a single batch\n",
    "    for _, output in tqdm(enumerate(tqdm(ablate_dataloader))):\n",
    "        for k, v in output.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                output[k] = v.to(model.device)\n",
    "\n",
    "        with TraceDict(model, modules, retain_input=True) as cache:\n",
    "            _ = model(output[\"input_ids\"])\n",
    "\n",
    "        for layer in modules:\n",
    "            if \"self_attn\" in layer:\n",
    "                if layer in mean_activations:\n",
    "                    mean_activations[layer] += torch.mean(cache[layer].input, dim=0)\n",
    "                else:\n",
    "                    mean_activations[layer] = torch.mean(cache[layer].input, dim=0)\n",
    "            else:\n",
    "                if layer in mean_activations:\n",
    "                    mean_activations[layer] += torch.mean(cache[layer].output, dim=0)\n",
    "                else:\n",
    "                    mean_activations[layer] = torch.mean(cache[layer].output, dim=0)\n",
    "\n",
    "        del cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    for layer in modules:\n",
    "        mean_activations[layer] /= len(ablate_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_ablate(inputs, output, layer, circuit_components, mean_activations, input_tokens):\n",
    "    if isinstance(inputs, tuple):\n",
    "        inputs = inputs[0]\n",
    "\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]\n",
    "\n",
    "    inputs = rearrange(\n",
    "        inputs,\n",
    "        \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "        n_heads=model.config.num_attention_heads,\n",
    "    )\n",
    "\n",
    "    mean_act = rearrange(\n",
    "        mean_activations[layer],\n",
    "        \"seq_len (n_heads d_head) -> 1 seq_len n_heads d_head\",\n",
    "        n_heads=model.config.num_attention_heads,\n",
    "    )\n",
    "\n",
    "    last_pos = inputs.size(1) - 1\n",
    "    for bi in range(inputs.size(0)):\n",
    "        prev_query_box_pos = analysis_utils.compute_prev_query_box_pos(\n",
    "            input_tokens[bi], input_tokens[bi].size(0) - 1\n",
    "        )\n",
    "        for token_pos in range(inputs.size(1)):\n",
    "            if (\n",
    "                token_pos != prev_query_box_pos\n",
    "                and token_pos != last_pos\n",
    "                and token_pos != last_pos - 2\n",
    "                and token_pos != prev_query_box_pos + 1\n",
    "            ):\n",
    "                inputs[bi, token_pos, :] = mean_act[0, token_pos, :]\n",
    "            elif token_pos == prev_query_box_pos:\n",
    "                for head_idx in range(model.config.num_attention_heads):\n",
    "                    if head_idx not in circuit_components[-1][layer]:\n",
    "                        inputs[bi, token_pos, head_idx] = mean_act[0, token_pos, head_idx]\n",
    "            elif token_pos == prev_query_box_pos + 1:\n",
    "                for head_idx in range(model.config.num_attention_heads):\n",
    "                    if head_idx not in circuit_components[-2][layer]:\n",
    "                        inputs[bi, token_pos, head_idx] = mean_act[0, token_pos, head_idx]\n",
    "            elif token_pos == last_pos:\n",
    "                for head_idx in range(model.config.num_attention_heads):\n",
    "                    if head_idx not in circuit_components[0][layer]:\n",
    "                        inputs[bi, token_pos, head_idx] = mean_act[0, token_pos, head_idx]\n",
    "            elif token_pos == last_pos - 2:\n",
    "                for head_idx in range(model.config.num_attention_heads):\n",
    "                    if head_idx not in circuit_components[2][layer]:\n",
    "                        inputs[bi, token_pos, head_idx] = mean_act[0, token_pos, head_idx]\n",
    "\n",
    "    inputs = rearrange(\n",
    "        inputs,\n",
    "        \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\",\n",
    "        n_heads=model.config.num_attention_heads,\n",
    "    )\n",
    "    w_o = model.state_dict()[f\"{layer}.weight\"]\n",
    "    output = einsum(\n",
    "        inputs, w_o, \"batch seq_len hidden_size, d_model hidden_size -> batch seq_len d_model\"\n",
    "    )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader, modules, circuit_components, mean_activations):\n",
    "    correct_count, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for _, output in enumerate(tqdm(dataloader)):\n",
    "            for k, v in output.items():\n",
    "                if v is not None and isinstance(v, torch.Tensor):\n",
    "                    output[k] = v.to(model.device)\n",
    "    \n",
    "            with TraceDict(\n",
    "                model,\n",
    "                modules,\n",
    "                retain_input=True,\n",
    "                edit_output=partial(\n",
    "                    mean_ablate,\n",
    "                    circuit_components=circuit_components,\n",
    "                    mean_activations=mean_activations,\n",
    "                    input_tokens=output[\"input_ids\"],\n",
    "                ),\n",
    "            ) as _:\n",
    "                outputs = model(output[\"input_ids\"])\n",
    "    \n",
    "            for bi in range(output[\"labels\"].size(0)):\n",
    "                label = output[\"labels\"][bi]\n",
    "                pred = torch.argmax(outputs.logits[bi][output[\"last_token_indices\"][bi]])\n",
    "    \n",
    "                if label == pred:\n",
    "                    correct_count += 1\n",
    "                # else:\n",
    "                #     print(f\"Label: {tokenizer.decode(label)}, Prediction: {tokenizer.decode(pred)}\")\n",
    "                total_count += 1\n",
    "    \n",
    "            del outputs\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    current_acc = round(correct_count / total_count, 2)\n",
    "    print(f\"Task accuracy: {current_acc}\")\n",
    "    return current_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 15 30 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:06<00:00,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [47, 15, 25, 10]\n",
    "n_value_fetcher = 61\n",
    "n_pos_trans = 15\n",
    "n_pos_detect = 30\n",
    "n_struct_read = 5\n",
    "\n",
    "circuit_components = {}\n",
    "circuit_components[0] = defaultdict(list)\n",
    "circuit_components[2] = defaultdict(list)\n",
    "circuit_components[-1] = defaultdict(list)\n",
    "circuit_components[-2] = defaultdict(list)\n",
    "\n",
    "root_path = \"./new_pp_exps/post_submission/goat-7b/seed_30\"\n",
    "path = root_path + \"/direct_logit_heads.pt\"\n",
    "\n",
    "direct_logit_heads = analysis_utils.compute_topk_components(torch.load(path), k=n_value_fetcher, largest=False)\n",
    "\n",
    "path = root_path + \"/heads_affect_direct_logit.pt\"\n",
    "heads_affecting_direct_logit_heads = analysis_utils.compute_topk_components(\n",
    "    torch.load(path), k=n_pos_trans, largest=False\n",
    ")\n",
    "\n",
    "path = root_path + \"/heads_at_query_box_pos.pt\"\n",
    "head_at_query_box_token = analysis_utils.compute_topk_components(\n",
    "    torch.load(path), k=n_pos_detect, largest=False\n",
    ")\n",
    "\n",
    "path = root_path + \"/heads_at_prev_query_box_pos.pt\"\n",
    "heads_at_prev_box_pos = analysis_utils.compute_topk_components(torch.load(path), k=n_struct_read, largest=False)\n",
    "\n",
    "intersection = []\n",
    "for head in direct_logit_heads:\n",
    "    if head in heads_affecting_direct_logit_heads:\n",
    "        intersection.append(head)\n",
    "\n",
    "for head in intersection:\n",
    "    direct_logit_heads.remove(head)\n",
    "\n",
    "print(\n",
    "    len(direct_logit_heads),\n",
    "    len(heads_affecting_direct_logit_heads),\n",
    "    len(head_at_query_box_token),\n",
    "    len(heads_at_prev_box_pos),\n",
    ")\n",
    "\n",
    "for layer_idx, head in direct_logit_heads:\n",
    "    if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "        layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "    else:\n",
    "        layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "    circuit_components[0][layer].append(head)\n",
    "\n",
    "for layer_idx, head in heads_affecting_direct_logit_heads:\n",
    "    if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "        layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "    else:\n",
    "        layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "    circuit_components[0][layer].append(head)\n",
    "\n",
    "\n",
    "for layer_idx, head in head_at_query_box_token:\n",
    "    if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "        layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "    else:\n",
    "        layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "    circuit_components[2][layer].append(head)\n",
    "\n",
    "\n",
    "for layer_idx, head in heads_at_prev_box_pos:\n",
    "    if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "        layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "    else:\n",
    "        layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "    circuit_components[-1][layer].append(head)\n",
    "\n",
    "for pos in circuit_components.keys():\n",
    "    for layer_idx in circuit_components[pos].keys():\n",
    "        circuit_components[pos][layer_idx] = list(set(circuit_components[pos][layer_idx]))\n",
    "\n",
    "eval(model, dataloader, modules, circuit_components, mean_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
