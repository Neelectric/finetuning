{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from torch.nn import CosineSimilarity\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer\n",
    "from functools import partial\n",
    "from baukit import TraceDict\n",
    "from einops import rearrange, einsum\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly_utils import imshow, scatter\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import seaborn as sns\n",
    "from peft import PeftModel\n",
    "import pickle\n",
    "\n",
    "import pysvelte\n",
    "import analysis_utils\n",
    "from counterfactual_datasets.entity_tracking import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.65s/it]\n"
     ]
    }
   ],
   "source": [
    "path = \"./llama_7b/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModelForCausalLM.from_pretrained(path).to(device)\n",
    "\n",
    "# base_model = \"decapoda-research/llama-7b-hf\"\n",
    "# lora_weights = \"tiedong/goat-lora-7b\"\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(\n",
    "#     \"hf-internal-testing/llama-tokenizer\", padding_side=\"right\"\n",
    "# )\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#     base_model,\n",
    "#     load_in_8bit=False,\n",
    "#     torch_dtype=torch.float32,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# model = PeftModel.from_pretrained(\n",
    "#     model,\n",
    "#     lora_weights,\n",
    "#     torch_dtype=torch.float32,\n",
    "#     device_map={\"\": 0},\n",
    "# )\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boxes = 7\n",
    "batch_size = 8\n",
    "data_file_path = f\"./box_datasets/no_instructions/alternative/Random/{num_boxes}/train.jsonl\"\n",
    "object_file_path = \"./box_datasets/filtered_objects_with_bnc_frequency.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(raw_data, batch_size):\n",
    "\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"base_input_ids\": raw_data[0],\n",
    "            \"base_input_last_pos\": raw_data[1],\n",
    "            \"source_input_ids\": raw_data[2],\n",
    "            \"source_input_last_pos\": raw_data[3],\n",
    "            \"labels\": raw_data[4],\n",
    "        }\n",
    "    ).with_format(\"torch\")\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = add_raw_text_at_start(\n",
    "        tokenizer=tokenizer,\n",
    "        num_samples=500,\n",
    "        data_file=data_file_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = load_data(\n",
    "    raw_data=raw_data, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E, the bill is in Box M, the cross is in Box K, the map is in Box D. Box X contains the\n",
      " There are three boxes, Box PP, Box BB and Box AA, the plant is in Box D, the fig is in Box E, the brick is in Box K, the radio is in Box M, the book is in Box S, the magnet is in Box C, the rock is in Box Z. Box M contains the\n",
      " game\n"
     ]
    }
   ],
   "source": [
    "data = next(enumerate(dataloader))[1]\n",
    "bi = 0\n",
    "print(tokenizer.decode(data[\"base_input_ids\"][bi][: data[\"base_input_last_pos\"][bi] + 1]))\n",
    "print(tokenizer.decode(data[\"source_input_ids\"][bi][: data[\"source_input_last_pos\"][bi] + 1]))\n",
    "print(tokenizer.decode(data[\"labels\"][bi]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./new_masks/llama-7b/heads_affect_direct_logit/positional/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    positional_info_fetcher_heads = json.loads(data[0].split(\": \")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = [f\"model.layers.{i}.self_attn.o_proj\" for i in range(32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_cache = {}\n",
    "for bi, inputs in enumerate(dataloader):\n",
    "    for k, v in inputs.items():\n",
    "        if v is not None and isinstance(v, torch.Tensor):\n",
    "            inputs[k] = v.to(model.device)\n",
    "    \n",
    "    with TraceDict(model, modules, retain_input=True) as cache:\n",
    "        _ = model(inputs[\"source_input_ids\"])\n",
    "\n",
    "    for module in modules:\n",
    "        if bi in source_cache:\n",
    "            source_cache[bi][module] = cache[module].input.detach().cpu()\n",
    "        else:\n",
    "            source_cache[bi] = {module: cache[module].input.detach().cpu()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patching(inputs, output, layer, patching_heads, bi):\n",
    "    if isinstance(inputs, tuple):\n",
    "        inputs = inputs[0]\n",
    "    \n",
    "    inputs = rearrange(inputs, \n",
    "                       \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "                       n_heads=model.config.num_attention_heads)\n",
    "    \n",
    "    cache = rearrange(source_cache[bi][layer],\n",
    "                        \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\",\n",
    "                        n_heads=model.config.num_attention_heads)\n",
    "\n",
    "    for rel_pos in patching_heads.keys():\n",
    "        layer_index = int(layer.split(\".\")[2])\n",
    "        curr_layer_heads = [h for l, h in patching_heads[rel_pos] if l == layer_index]\n",
    "\n",
    "        # pos = inputs.size(1) - rel_pos - 1\n",
    "        for head in curr_layer_heads:\n",
    "            inputs[:, -1, head] = cache[:, -1, head]\n",
    "\n",
    "    inputs = rearrange(inputs,\n",
    "                        \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\",\n",
    "                        n_heads=model.config.num_attention_heads)\n",
    "    \n",
    "    w_o = model.state_dict()[f\"{layer}.weight\"]\n",
    "    output = einsum(inputs, \n",
    "                    w_o, \n",
    "                    \"batch seq_len hidden_size, d_model hidden_size -> batch seq_len d_model\")\n",
    "    \n",
    "    del w_o\n",
    "    torch.cuda.empty_cache()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "patching_heads = {\n",
    "    0: positional_info_fetcher_heads\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:36,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_count, total_count = 0, 0\n",
    "for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "    for k, v in inputs.items():\n",
    "        if v is not None and isinstance(v, torch.Tensor):\n",
    "            inputs[k] = v.to(model.device)\n",
    "\n",
    "    with TraceDict(model, modules, retain_input=True, edit_output=partial(patching, \n",
    "                                                                          patching_heads = patching_heads,\n",
    "                                                                          bi = bi)) as cache:\n",
    "        outputs = model(inputs[\"base_input_ids\"])\n",
    "\n",
    "    for idx in range(inputs[\"base_input_ids\"].size(0)):\n",
    "        label = inputs[\"labels\"][idx].item()\n",
    "        pred = torch.argmax(outputs.logits[idx, -1], dim=-1).item()\n",
    "\n",
    "        if label == pred:\n",
    "            correct_count += 1\n",
    "        total_count += 1\n",
    "\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Accuracy: {correct_count / total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Positional Info relative to what?\n",
    "# Raw text at the start - 0.312\n",
    "# Raw text at the end - 0.36\n",
    "# Additional tokens between box and object - 0.398\n",
    "\n",
    "Is the model keeping tracking of boxes?\n",
    "# One additional segment at the start - 0.224\n",
    "# One additional segment at the end - 0.316\n",
    "# Additional boxes before correct segment - 0.2\n",
    "\n",
    "Semantic association and token order seem to be important\n",
    "# Incorrect box segment index - 0.162\n",
    "# Box object order altered - 0.156\n",
    "# object is not in the box - 0.154\n",
    "\n",
    "Commas are irrelevant\n",
    "# No comma - 0.407\n",
    "# Commas after objects - 0.386"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional desiderata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"additional_desiderata_results.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_text_start: (34.56, 1.75)\n",
      "raw_text_end: (33.32, 1.61)\n",
      "additional_tokens_btw_obj_and_box: (38.24, 1.63)\n",
      "add_segment_start: (24.6, 1.33)\n",
      "add_segment_end: (30.98, 2.02)\n",
      "add_boxes_before_correct_segment: (19.66, 1.2)\n",
      "incorrect_box_segment_index: (17.1, 1.77)\n",
      "Box_object_altered_order: (14.18, 1.66)\n",
      "object_not_in_box: (17.18, 1.4)\n",
      "no_comma: (37.5, 1.24)\n",
      "comma_after_object: (36.58, 1.89)\n"
     ]
    }
   ],
   "source": [
    "for desideratum in data.keys():\n",
    "    values = data[desideratum]\n",
    "    std = np.std(values)\n",
    "    print(f\"{desideratum}: {round(np.mean(values), 2), round(std, 2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anima",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
