{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_nikhil/.conda/envs/anima/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fad0dd10cd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from data.data_utils import *\n",
    "\n",
    "from pp_utils import eval_circuit_performance, get_mean_activations\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed = 5\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"hf-internal-testing/llama-tokenizer\", padding_side=\"right\"\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 500\n"
     ]
    }
   ],
   "source": [
    "data_file = \"../data/dataset.jsonl\"\n",
    "batch_size = 50\n",
    "\n",
    "raw_data = entity_tracking_example_sampler(\n",
    "    tokenizer=tokenizer,\n",
    "    num_samples=500,\n",
    "    data_file=data_file,\n",
    "    architecture=\"LlamaForCausalLM\",\n",
    ")\n",
    "\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_data[0],\n",
    "        \"last_token_indices\": raw_data[1],\n",
    "        \"labels\": raw_data[2],\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "\n",
    "print(f\"Length of dataset: {len(dataset)}\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/circuits/llama_circuit.json\", \"r\") as f:\n",
    "    llama_circuit_heads = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Fetcher Heads: 40\n",
      "Position Transmitter Heads: 7\n",
      "Position Detector Heads: 20\n",
      "Structure Reader Heads: 5\n",
      "Total Heads: 72\n"
     ]
    }
   ],
   "source": [
    "print(f\"Value Fetcher Heads: {len(llama_circuit_heads['value_fetcher'])}\")\n",
    "print(f\"Position Transmitter Heads: {len(llama_circuit_heads['pos_transmitter'])}\")\n",
    "print(f\"Position Detector Heads: {len(llama_circuit_heads['pos_detector'])}\")\n",
    "print(f\"Structure Reader Heads: {len(llama_circuit_heads['struct_reader'])}\")\n",
    "print(f\"Total Heads: {len(llama_circuit_heads['value_fetcher']) + len(llama_circuit_heads['pos_transmitter']) + len(llama_circuit_heads['pos_detector']) + len(llama_circuit_heads['struct_reader'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_circuit(model, circuit_heads):\n",
    "    circuit_components = {}\n",
    "    circuit_components[0] = defaultdict(list)\n",
    "    circuit_components[2] = defaultdict(list)\n",
    "    circuit_components[-1] = defaultdict(list)\n",
    "\n",
    "    for layer_idx, head in circuit_heads['value_fetcher']:\n",
    "        if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "            layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        else:\n",
    "            layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        circuit_components[0][layer].append(head)\n",
    "\n",
    "    for layer_idx, head in circuit_heads['pos_transmitter']:\n",
    "        if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "            layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        else:\n",
    "            layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        circuit_components[0][layer].append(head)\n",
    "\n",
    "\n",
    "    for layer_idx, head in circuit_heads['pos_detector']:\n",
    "        if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "            layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        else:\n",
    "            layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        circuit_components[2][layer].append(head)\n",
    "\n",
    "\n",
    "    for layer_idx, head in circuit_heads['struct_reader']:\n",
    "        if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "            layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        else:\n",
    "            layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        circuit_components[-1][layer].append(head)\n",
    "    \n",
    "    return circuit_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_performance(model, dataloader):\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    model.eval()\n",
    "    errors = defaultdict(int)\n",
    "    with torch.no_grad():\n",
    "        for _, output in tqdm(enumerate(tqdm(dataloader))):\n",
    "            for k, v in output.items():\n",
    "                if v is not None and isinstance(v, torch.Tensor):\n",
    "                    output[k] = v.to(model.device)\n",
    "\n",
    "            outputs = model(input_ids=output[\"input_ids\"])\n",
    "\n",
    "            for bi in range(output[\"labels\"].size(0)):\n",
    "                label = output[\"labels\"][bi]\n",
    "                pred = torch.argmax(outputs.logits[bi][output[\"last_token_indices\"][bi]])\n",
    "                box_label = output[\"input_ids\"][bi][output[\"last_token_indices\"][bi] - 2]\n",
    "                prev_box_label_pos = (\n",
    "                    output[\"input_ids\"][bi].eq(box_label).nonzero()[:, 0][0].item()\n",
    "                )\n",
    "                prev_box_label_index = prev_box_label_pos // 8 + 1\n",
    "\n",
    "                if label == pred:\n",
    "                    correct_count += 1\n",
    "                total_count += 1\n",
    "\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    current_acc = round(correct_count / total_count, 2)\n",
    "    return current_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Circuit Performance: Llama-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.68s/it]\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/local_nikhil/Projects/llama_weights/7B\"\n",
    "\n",
    "# Delete model if present in memory\n",
    "if \"model\" in locals():\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mean activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.16s/it]\n"
     ]
    }
   ],
   "source": [
    "circuit_components = get_circuit(model, llama_circuit_heads)\n",
    "\n",
    "mean_activations, modules = get_mean_activations(\n",
    "    model, tokenizer, data_file, num_samples=500, batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.16s/it]\n",
      "10it [00:21,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance 0.66\n",
      "Circuit Performance 0.66\n",
      "Faithfulness: 1.0\n"
     ]
    }
   ],
   "source": [
    "model_acc = eval_model_performance(model, dataloader)\n",
    "print(f\"Model Performance {model_acc}\")\n",
    "\n",
    "circuit_acc = eval_circuit_performance(model, dataloader, modules, circuit_components, mean_activations)\n",
    "print(f\"Circuit Performance {circuit_acc}\")\n",
    "\n",
    "print(f\"Faithfulness: {round(circuit_acc/model_acc, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Circuit Performance: Vicuna-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 14/14 [00:09<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"AlekseyKorshuk/vicuna-7b\"\n",
    "\n",
    "# Delete model if present in memory\n",
    "if \"model\" in locals():\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mean activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "circuit_components = get_circuit(model, llama_circuit_heads)\n",
    "\n",
    "mean_activations, modules = get_mean_activations(\n",
    "    model, tokenizer, data_file, num_samples=500, batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.16s/it]\n",
      "10it [00:21,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance 0.67\n",
      "Circuit Performance 0.65\n",
      "Faithfulness: 0.97\n"
     ]
    }
   ],
   "source": [
    "model_acc = eval_model_performance(model, dataloader)\n",
    "print(f\"Model Performance {model_acc}\")\n",
    "\n",
    "circuit_acc = eval_circuit_performance(model, dataloader, modules, circuit_components, mean_activations)\n",
    "print(f\"Circuit Performance {circuit_acc}\")\n",
    "\n",
    "print(f\"Faithfulness: {round(circuit_acc/model_acc, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Circuit Performance: Goat-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:14<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "base_model = \"decapoda-research/llama-7b-hf\"\n",
    "lora_weights = \"tiedong/goat-lora-7b\"\n",
    "\n",
    "# Delete model if present in memory\n",
    "if \"model\" in locals():\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={'': 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mean activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "circuit_components = get_circuit(model, llama_circuit_heads)\n",
    "\n",
    "mean_activations, modules = get_mean_activations(\n",
    "    model, tokenizer, data_file, num_samples=500, batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.26s/it]\n",
      "10it [00:22,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance 0.82\n",
      "Circuit Performance 0.73\n",
      "Faithfulness: 0.89\n"
     ]
    }
   ],
   "source": [
    "model_acc = eval_model_performance(model, dataloader)\n",
    "print(f\"Model Performance {model_acc}\")\n",
    "\n",
    "circuit_acc = eval_circuit_performance(model, dataloader, modules, circuit_components, mean_activations)\n",
    "print(f\"Circuit Performance {circuit_acc}\")\n",
    "\n",
    "print(f\"Faithfulness: {round(circuit_acc/model_acc, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anima",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
