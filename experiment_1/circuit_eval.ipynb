{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa94ba29ed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from data.data_utils import *\n",
    "\n",
    "from pp_utils import (\n",
    "    compute_topk_components,\n",
    "    eval_circuit_performance,\n",
    "    get_mean_activations,\n",
    "    get_random_circuit,\n",
    "    get_model_and_tokenizer,\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "seed = 5\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcfbc6e356c4346bf5624b5b8fd1ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer = LlamaTokenizer.from_pretrained(\n",
    "#     \"hf-internal-testing/llama-tokenizer\", padding_side=\"right\"\n",
    "# )\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# model, tokenizer = get_model_and_tokenizer(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "# model, tokenizer = get_model_and_tokenizer(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "\n",
    "# model, tokenizer = get_model_and_tokenizer(\"allenai/OLMo-2-1124-7B-DPO\")\n",
    "model, tokenizer = get_model_and_tokenizer(\"allenai/OLMo-2-1124-7B-stage1-step462000-tokens1938B\")\n",
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 500\n"
     ]
    }
   ],
   "source": [
    "data_file = \"../data/dataset.jsonl\"\n",
    "batch_size = 50\n",
    "\n",
    "raw_data = sample_box_data(\n",
    "    tokenizer=tokenizer,\n",
    "    num_samples=500,\n",
    "    data_file=data_file,\n",
    ")\n",
    "\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_data[0],\n",
    "        \"last_token_indices\": raw_data[1],\n",
    "        \"labels\": raw_data[2],\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "\n",
    "print(f\"Length of dataset: {len(dataset)}\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/circuits/llama_circuit.json\", \"r\") as f:\n",
    "    llama_circuit = json.load(f)\n",
    "\n",
    "with open(\"./results/circuits/goat_circuit.json\", \"r\") as f:\n",
    "    goat_circuit = json.load(f)\n",
    "\n",
    "with open(\n",
    "    \"./results/circuits/float_circuit.json\", \"r\") as f:\n",
    "    float_circuit = json.load(f)\n",
    "\n",
    "# meta_llama_Llama_3_1_8B_path = \"./results/path_patching/meta-llama/Llama-3.1-8B_circuit/results.json\"\n",
    "meta_llama_Llama_3_1_8B_path = \"./results/minimality/meta-llama/Llama-3.1-8B-Instruct_circuit/results.json\"\n",
    "with open(\n",
    "    meta_llama_Llama_3_1_8B_path, \"r\"\n",
    "    ) as f:\n",
    "    meta_llama_Llama_3_1_8B_circuit = json.load(f)\n",
    "    # print(\"LOADING meta-llama/Llama-3.1-8B_circuit FROM PATH PATCHING RESULTS, NOT MINIMALITY OR COMPLETENESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.1-8B Circuit\n",
      "Value Fetcher Heads: 14\n",
      "Position Transmitter Heads: 15\n",
      "Position Detector Heads: 23\n",
      "Structure Reader Heads: 14\n",
      "Total Heads: 66\n"
     ]
    }
   ],
   "source": [
    "print(\"meta-llama/Llama-3.1-8B Circuit\")\n",
    "print(f\"Value Fetcher Heads: {len(meta_llama_Llama_3_1_8B_circuit['value_fetcher'])}\")\n",
    "print(f\"Position Transmitter Heads: {len(meta_llama_Llama_3_1_8B_circuit['pos_transmitter'])}\")\n",
    "print(f\"Position Detector Heads: {len(meta_llama_Llama_3_1_8B_circuit['pos_detector'])}\")\n",
    "print(f\"Structure Reader Heads: {len(meta_llama_Llama_3_1_8B_circuit['struct_reader'])}\")\n",
    "print(\n",
    "    f\"Total Heads: {len(meta_llama_Llama_3_1_8B_circuit['value_fetcher']) + len(meta_llama_Llama_3_1_8B_circuit['pos_transmitter']) + len(meta_llama_Llama_3_1_8B_circuit['pos_detector']) + len(meta_llama_Llama_3_1_8B_circuit['struct_reader'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama Circuit\n",
      "Value Fetcher Heads: 40\n",
      "Position Transmitter Heads: 7\n",
      "Position Detector Heads: 20\n",
      "Structure Reader Heads: 5\n",
      "Total Heads: 72\n"
     ]
    }
   ],
   "source": [
    "print(\"Llama Circuit\")\n",
    "print(f\"Value Fetcher Heads: {len(llama_circuit['value_fetcher'])}\")\n",
    "print(f\"Position Transmitter Heads: {len(llama_circuit['pos_transmitter'])}\")\n",
    "print(f\"Position Detector Heads: {len(llama_circuit['pos_detector'])}\")\n",
    "print(f\"Structure Reader Heads: {len(llama_circuit['struct_reader'])}\")\n",
    "print(\n",
    "    f\"Total Heads: {len(llama_circuit['value_fetcher']) + len(llama_circuit['pos_transmitter']) + len(llama_circuit['pos_detector']) + len(llama_circuit['struct_reader'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOAT CIRCUIT\n",
      "Value Fetcher Heads: 68\n",
      "Position Transmitter Heads: 28\n",
      "Position Detector Heads: 40\n",
      "Structure Reader Heads: 39\n",
      "Total Heads: 175\n"
     ]
    }
   ],
   "source": [
    "print(\"GOAT CIRCUIT\")\n",
    "print(f\"Value Fetcher Heads: {len(goat_circuit['value_fetcher'])}\")\n",
    "print(f\"Position Transmitter Heads: {len(goat_circuit['pos_transmitter'])}\")\n",
    "print(f\"Position Detector Heads: {len(goat_circuit['pos_detector'])}\")\n",
    "print(f\"Structure Reader Heads: {len(goat_circuit['struct_reader'])}\")\n",
    "print(\n",
    "    f\"Total Heads: {len(goat_circuit['value_fetcher']) + len(goat_circuit['pos_transmitter']) + len(goat_circuit['pos_detector']) + len(goat_circuit['struct_reader'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float Circuit\n",
      "Value Fetcher Heads: 68\n",
      "Position Transmitter Heads: 29\n",
      "Position Detector Heads: 40\n",
      "Structure Reader Heads: 38\n",
      "Total Heads: 175\n"
     ]
    }
   ],
   "source": [
    "print(\"Float Circuit\")\n",
    "print(f\"Value Fetcher Heads: {len(float_circuit['value_fetcher'])}\")\n",
    "print(f\"Position Transmitter Heads: {len(float_circuit['pos_transmitter'])}\")\n",
    "print(f\"Position Detector Heads: {len(float_circuit['pos_detector'])}\")\n",
    "print(f\"Structure Reader Heads: {len(float_circuit['struct_reader'])}\")\n",
    "print(\n",
    "    f\"Total Heads: {len(float_circuit['value_fetcher']) + len(float_circuit['pos_transmitter']) + len(float_circuit['pos_detector']) + len(float_circuit['struct_reader'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_circuit(model, circuit_heads):\n",
    "    circuit_components = {}\n",
    "    circuit_components[0] = defaultdict(list)\n",
    "    circuit_components[2] = defaultdict(list)\n",
    "    circuit_components[-1] = defaultdict(list)\n",
    "\n",
    "    for layer_idx, head in circuit_heads[\"value_fetcher\"]:\n",
    "        if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "            layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        else:\n",
    "            layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        circuit_components[0][layer].append(head)\n",
    "\n",
    "    for layer_idx, head in circuit_heads[\"pos_transmitter\"]:\n",
    "        if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "            layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        else:\n",
    "            layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        circuit_components[0][layer].append(head)\n",
    "\n",
    "    for layer_idx, head in circuit_heads[\"pos_detector\"]:\n",
    "        if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "            layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        else:\n",
    "            layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        circuit_components[2][layer].append(head)\n",
    "\n",
    "    for layer_idx, head in circuit_heads[\"struct_reader\"]:\n",
    "        if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "            layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        else:\n",
    "            layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        circuit_components[-1][layer].append(head)\n",
    "\n",
    "    return circuit_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_performance(model, dataloader):\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, output in tqdm(enumerate(tqdm(dataloader))):\n",
    "            for k, v in output.items():\n",
    "                if v is not None and isinstance(v, torch.Tensor):\n",
    "                    output[k] = v.to(model.device)\n",
    "\n",
    "            outputs = model(input_ids=output[\"input_ids\"])\n",
    "\n",
    "            for bi in range(output[\"labels\"].size(0)):\n",
    "                label = output[\"labels\"][bi]\n",
    "                pred = torch.argmax(\n",
    "                    outputs.logits[bi][output[\"last_token_indices\"][bi]]\n",
    "                )\n",
    "                decoded_label = tokenizer.decode(label)\n",
    "                decoded_pred = tokenizer.decode(pred)\n",
    "\n",
    "                if label == pred:\n",
    "                    correct_count += 1\n",
    "                elif decoded_label.strip() == decoded_pred.strip():\n",
    "                    correct_count += 1\n",
    "                total_count += 1\n",
    "\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    current_acc = round(correct_count / total_count, 6)\n",
    "    return current_acc * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Circuit Performance: meta-llama/Llama-3.1-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mean activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.62it/s]\n"
     ]
    }
   ],
   "source": [
    "mean_activations, modules = get_mean_activations(\n",
    "    model, tokenizer, data_file, num_samples=500, batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.81it/s]\n",
      "10it [00:03,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance 74.6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:06<00:57,  6.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Performance \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m circuit_components \u001b[38;5;241m=\u001b[39m get_circuit(model, meta_llama_Llama_3_1_8B_circuit)\n\u001b[0;32m----> 5\u001b[0m circuit_acc \u001b[38;5;241m=\u001b[39m \u001b[43meval_circuit_performance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuit_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_activations\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircuit Performance \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcircuit_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m random_circuit_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/repos/finetuning/experiment_1/pp_utils.py:898\u001b[0m, in \u001b[0;36meval_circuit_performance\u001b[0;34m(model, dataloader, modules, circuit_components, mean_activations, ablate_non_vital_pos, print_evals, disable_tqdm)\u001b[0m\n\u001b[1;32m    883\u001b[0m         inp[k] \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TraceDict(\n\u001b[1;32m    886\u001b[0m     model,\n\u001b[1;32m    887\u001b[0m     modules,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    896\u001b[0m     ),\n\u001b[1;32m    897\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m _:\n\u001b[0;32m--> 898\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(inp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m    901\u001b[0m     label \u001b[38;5;241m=\u001b[39m inp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m][bi]\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/transformers/models/olmo2/modeling_olmo2.py:808\u001b[0m, in \u001b[0;36mOlmo2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 808\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/transformers/models/olmo2/modeling_olmo2.py:566\u001b[0m, in \u001b[0;36mOlmo2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    555\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    556\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    563\u001b[0m         position_embeddings,\n\u001b[1;32m    564\u001b[0m     )\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/transformers/models/olmo2/modeling_olmo2.py:250\u001b[0m, in \u001b[0;36mOlmo2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    262\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/transformers/models/olmo2/modeling_olmo2.py:205\u001b[0m, in \u001b[0;36mOlmo2Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    195\u001b[0m     query_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    202\u001b[0m )\n\u001b[1;32m    204\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 205\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1803\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1801\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1803\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1806\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/baukit/nethook.py:73\u001b[0m, in \u001b[0;36mTrace.__init__.<locals>.retain_hook\u001b[0;34m(m, inputs, output)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretain_hook\u001b[39m(m, inputs, output):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m edit_output:\n\u001b[0;32m---> 73\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43minvoke_with_optional_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43medit_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retain_input:\n\u001b[1;32m     77\u001b[0m         retainer\u001b[38;5;241m.\u001b[39minput \u001b[38;5;241m=\u001b[39m recursive_copy(\n\u001b[1;32m     78\u001b[0m             inputs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m inputs,\n\u001b[1;32m     79\u001b[0m             clone\u001b[38;5;241m=\u001b[39mclone,\n\u001b[1;32m     80\u001b[0m             detach\u001b[38;5;241m=\u001b[39mdetach,\n\u001b[1;32m     81\u001b[0m             retain_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     82\u001b[0m         )  \u001b[38;5;66;03m# retain_grad applies to output only.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetuning/lib/python3.10/site-packages/baukit/nethook.py:471\u001b[0m, in \u001b[0;36minvoke_with_optional_args\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argspec\u001b[38;5;241m.\u001b[39mvarargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     pass_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args[used_pos:])\n\u001b[0;32m--> 471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpass_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpass_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/finetuning/experiment_1/pp_utils.py:-1\u001b[0m, in \u001b[0;36mmean_ablate\u001b[0;34m(inputs, output, layer, model, circuit_components, mean_activations, input_tokens, ablate_non_vital_pos)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_acc = eval_model_performance(model, dataloader)\n",
    "print(f\"Model Performance {model_acc}\\n\")\n",
    "\n",
    "circuit_components = get_circuit(model, meta_llama_Llama_3_1_8B_circuit)\n",
    "circuit_acc = eval_circuit_performance(\n",
    "    model, dataloader, modules, circuit_components, mean_activations\n",
    ")\n",
    "print(f\"Circuit Performance {circuit_acc}\\n\")\n",
    "\n",
    "random_circuit_acc = 0\n",
    "n_iters = 10\n",
    "for _ in range(n_iters):\n",
    "    random_circuit_components = get_random_circuit(model, meta_llama_Llama_3_1_8B_circuit)\n",
    "    random_circuit_acc += eval_circuit_performance(\n",
    "        model, dataloader, modules, random_circuit_components, mean_activations\n",
    "    )\n",
    "random_circuit_acc = round(random_circuit_acc / n_iters, 2)\n",
    "print(f\"Random Circuit Performance {random_circuit_acc}\")\n",
    "\n",
    "print(f\"Faithfulness: {round(circuit_acc/model_acc, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Circuit Performance: Llama-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/home/local_nikhil/Projects/llama_weights/7B\"\n",
    "\n",
    "# Delete model if present in memory\n",
    "if \"model\" in locals():\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained(path).to(device)\n",
    "model, tokenizer = get_model_and_tokenizer(\"llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_activations, modules = get_mean_activations(\n",
    "    model, tokenizer, data_file, num_samples=500, batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_acc = eval_model_performance(model, dataloader)\n",
    "print(f\"Model Performance {model_acc}\\n\")\n",
    "\n",
    "circuit_components = get_circuit(model, llama_circuit)\n",
    "circuit_acc = eval_circuit_performance(\n",
    "    model, dataloader, modules, circuit_components, mean_activations\n",
    ")\n",
    "print(f\"Circuit Performance {circuit_acc}\\n\")\n",
    "\n",
    "random_circuit_acc = 0\n",
    "n_iters = 10\n",
    "for _ in range(n_iters):\n",
    "    random_circuit_components = get_random_circuit(model, llama_circuit)\n",
    "    random_circuit_acc += eval_circuit_performance(\n",
    "        model, dataloader, modules, random_circuit_components, mean_activations\n",
    "    )\n",
    "random_circuit_acc = round(random_circuit_acc / n_iters, 2)\n",
    "print(f\"Random Circuit Performance {random_circuit_acc}\")\n",
    "\n",
    "print(f\"Faithfulness: {round(circuit_acc/model_acc, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Circuit Performance: Vicuna-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"AlekseyKorshuk/vicuna-7b\"\n",
    "\n",
    "# Delete model if present in memory\n",
    "if \"model\" in locals():\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_activations, modules = get_mean_activations(\n",
    "    model, tokenizer, data_file, num_samples=500, batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_acc = eval_model_performance(model, dataloader)\n",
    "print(f\"Model Performance {model_acc}\\n\")\n",
    "\n",
    "circuit_components = get_circuit(model, llama_circuit)\n",
    "circuit_acc = eval_circuit_performance(\n",
    "    model, dataloader, modules, circuit_components, mean_activations\n",
    ")\n",
    "print(f\"Circuit Performance {circuit_acc}\\n\")\n",
    "\n",
    "random_circuit_acc = 0\n",
    "n_iters = 10\n",
    "for _ in range(n_iters):\n",
    "    random_circuit_components = get_random_circuit(model, llama_circuit)\n",
    "    random_circuit_acc += eval_circuit_performance(\n",
    "        model, dataloader, modules, random_circuit_components, mean_activations\n",
    "    )\n",
    "random_circuit_acc = round(random_circuit_acc / n_iters, 2)\n",
    "print(f\"Random Circuit Performance {random_circuit_acc}\\n\")\n",
    "\n",
    "print(f\"Faithfulness: {round(circuit_acc/model_acc, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Circuit Performance: Goat-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"decapoda-research/llama-7b-hf\"\n",
    "lora_weights = \"tiedong/goat-lora-7b\"\n",
    "\n",
    "# Delete model if present in memory\n",
    "if \"model\" in locals():\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_activations, modules = get_mean_activations(\n",
    "    model, tokenizer, data_file, num_samples=500, batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_acc = eval_model_performance(model, dataloader)\n",
    "print(f\"Model Performance: {model_acc}\\n\")\n",
    "\n",
    "circuit_components = get_circuit(model, llama_circuit)\n",
    "circuit_acc = eval_circuit_performance(\n",
    "    model, dataloader, modules, circuit_components, mean_activations\n",
    ")\n",
    "print(f\"Circuit Performance: {circuit_acc}\\n\")\n",
    "\n",
    "random_circuit_acc = 0\n",
    "n_iters = 10\n",
    "for _ in range(n_iters):\n",
    "    random_circuit_components = get_random_circuit(model, llama_circuit)\n",
    "    random_circuit_acc += eval_circuit_performance(\n",
    "        model, dataloader, modules, random_circuit_components, mean_activations\n",
    "    )\n",
    "random_circuit_acc = round(random_circuit_acc / n_iters, 2)\n",
    "print(f\"Random Circuit Performance {random_circuit_acc}\\n\")\n",
    "\n",
    "print(f\"Faithfulness: {round(circuit_acc/model_acc, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Circuit Performance: Float-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"nikhil07prakash/float-7b\"\n",
    "\n",
    "# Delete model if present in memory\n",
    "if \"model\" in locals():\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_activations, modules = get_mean_activations(\n",
    "    model, tokenizer, data_file, num_samples=500, batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_acc = eval_model_performance(model, dataloader)\n",
    "print(f\"Model Performance {model_acc}\")\n",
    "\n",
    "circuit_components = get_circuit(model, llama_circuit)\n",
    "circuit_acc = eval_circuit_performance(\n",
    "    model, dataloader, modules, circuit_components, mean_activations\n",
    ")\n",
    "print(f\"Circuit Performance {circuit_acc}\")\n",
    "\n",
    "random_circuit_acc = 0\n",
    "n_iters = 10\n",
    "for _ in range(n_iters):\n",
    "    random_circuit_components = get_random_circuit(model, llama_circuit)\n",
    "    random_circuit_acc += eval_circuit_performance(\n",
    "        model, dataloader, modules, random_circuit_components, mean_activations\n",
    "    )\n",
    "random_circuit_acc = round(random_circuit_acc / n_iters, 2)\n",
    "print(f\"Random Circuit Performance {random_circuit_acc}\\n\")\n",
    "\n",
    "print(f\"Faithfulness: {round(circuit_acc/model_acc, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuit Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_groups = [\"value_fetcher\", \"pos_transmitter\", \"pos_detector\", \"struct_reader\"]\n",
    "\n",
    "for head_group in head_groups:\n",
    "    intersection = 0\n",
    "    for l, h in llama_circuit[head_group]:\n",
    "        if [l, h] in float_circuit[head_group]:\n",
    "            intersection += 1\n",
    "\n",
    "    precision = round(intersection / len(llama_circuit[head_group]), 2)\n",
    "    recall = round(intersection / len(float_circuit[head_group]), 2)\n",
    "\n",
    "    print(\n",
    "        f\"{head_group}: {len(llama_circuit[head_group])} | {len(float_circuit[head_group])} | {intersection} | {precision} | {recall}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_value_fetcher = 58\n",
    "n_pos_trans = 10\n",
    "n_pos_detect = 25\n",
    "n_struct_read = 5\n",
    "pp_llama_root = \"./results/path_patching/llama_circuit\"\n",
    "\n",
    "path = f\"{pp_llama_root}/value_fetcher.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "top_value_fetcher_llama_heads, top_value_fetcher_llama_score = compute_topk_components(\n",
    "    patching_scores, k=n_value_fetcher, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "path = f\"{pp_llama_root}/pos_transmitter.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "(\n",
    "    top_pos_transmitter_llama_heads,\n",
    "    top_pos_transmitter_llama_score,\n",
    ") = compute_topk_components(\n",
    "    patching_scores, k=n_pos_trans, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "path = f\"{pp_llama_root}/pos_detector.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "top_pos_detector_llama_heads, top_pos_detector_llama_score = compute_topk_components(\n",
    "    patching_scores, k=n_pos_detect, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "path = f\"{pp_llama_root}/struct_reader.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "top_struct_reader_llama_heads, top_struct_reader_llama_score = compute_topk_components(\n",
    "    patching_scores, k=n_struct_read, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "# If a head is present in both top value_fetcher and position_transmitter, then\n",
    "# remove it from the top value_fetcher heads.\n",
    "intersection = []\n",
    "for head in top_value_fetcher_llama_heads:\n",
    "    if head in top_pos_transmitter_llama_heads:\n",
    "        intersection.append(head)\n",
    "\n",
    "for head in intersection:\n",
    "    idx = top_value_fetcher_llama_heads.index(head)\n",
    "    top_value_fetcher_llama_heads.pop(idx)\n",
    "    top_value_fetcher_llama_score.pop(idx)\n",
    "\n",
    "print(f\"Value Fetcher: {len(top_value_fetcher_llama_heads)}\")\n",
    "print(f\"Position Transmitter: {len(top_pos_transmitter_llama_heads)}\")\n",
    "print(f\"Position Detector: {len(top_pos_detector_llama_heads)}\")\n",
    "print(f\"Structure Reader: {len(top_struct_reader_llama_heads)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_value_fetcher = llama_circuit[\"value_fetcher\"]\n",
    "llama_pos_transmitter = llama_circuit[\"pos_transmitter\"]\n",
    "llama_pos_detector = llama_circuit[\"pos_detector\"]\n",
    "llama_struct_reader = llama_circuit[\"struct_reader\"]\n",
    "\n",
    "print(\n",
    "    len(llama_value_fetcher),\n",
    "    len(llama_pos_transmitter),\n",
    "    len(llama_pos_detector),\n",
    "    len(llama_struct_reader),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tuple of (head, patching_score, position) for each group in llama circuit\n",
    "llama_value_fetcher_tuple = []\n",
    "for head, score in zip(top_value_fetcher_llama_heads, top_value_fetcher_llama_score):\n",
    "    llama_value_fetcher_tuple.append((head, abs(score), 0))\n",
    "\n",
    "llama_pos_transmitter_tuple = []\n",
    "for head, score in zip(\n",
    "    top_pos_transmitter_llama_heads, top_pos_transmitter_llama_score\n",
    "):\n",
    "    llama_pos_transmitter_tuple.append((head, abs(score), 0))\n",
    "\n",
    "llama_pos_detector_tuple = []\n",
    "for head, score in zip(top_pos_detector_llama_heads, top_pos_detector_llama_score):\n",
    "    llama_pos_detector_tuple.append((head, abs(score), 2))\n",
    "\n",
    "llama_struct_reader_tuple = []\n",
    "for head, score in zip(top_struct_reader_llama_heads, top_struct_reader_llama_score):\n",
    "    llama_struct_reader_tuple.append((head, abs(score), -1))\n",
    "\n",
    "llama_circuit_tuple = (\n",
    "    llama_value_fetcher_tuple\n",
    "    + llama_pos_transmitter_tuple\n",
    "    + llama_pos_detector_tuple\n",
    "    + llama_struct_reader_tuple\n",
    ")\n",
    "llama_circuit_tuple = sorted(llama_circuit_tuple, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_value_fetcher = 102\n",
    "n_pos_trans = 30\n",
    "n_pos_detect = 50\n",
    "n_struct_read = 40\n",
    "pp_float_root = \"./results/path_patching/float_circuit\"\n",
    "\n",
    "path = f\"{pp_float_root}/value_fetcher.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "top_value_fetcher_float_heads, top_value_fetcher_float_score = compute_topk_components(\n",
    "    patching_scores, k=n_value_fetcher, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "path = f\"{pp_float_root}/pos_transmitter.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "(\n",
    "    top_pos_transmitter_float_heads,\n",
    "    top_pos_transmitter_float_score,\n",
    ") = compute_topk_components(\n",
    "    patching_scores, k=n_pos_trans, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "path = f\"{pp_float_root}/pos_detector.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "top_pos_detector_float_heads, top_pos_detector_float_score = compute_topk_components(\n",
    "    patching_scores, k=n_pos_detect, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "path = f\"{pp_float_root}/struct_reader.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "top_struct_reader_float_heads, top_struct_reader_float_score = compute_topk_components(\n",
    "    patching_scores, k=n_struct_read, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "# If a head is present in both top value_fetcher and position_transmitter, then\n",
    "# remove it from the top value_fetcher heads.\n",
    "intersection = []\n",
    "for head in top_value_fetcher_float_heads:\n",
    "    if head in top_pos_transmitter_float_heads:\n",
    "        intersection.append(head)\n",
    "\n",
    "for head in intersection:\n",
    "    idx = top_value_fetcher_float_heads.index(head)\n",
    "    top_value_fetcher_float_heads.pop(idx)\n",
    "    top_value_fetcher_float_score.pop(idx)\n",
    "\n",
    "print(f\"Value Fetcher: {len(top_value_fetcher_float_heads)}\")\n",
    "print(f\"Position Transmitter: {len(top_pos_transmitter_float_heads)}\")\n",
    "print(f\"Position Detector: {len(top_pos_detector_float_heads)}\")\n",
    "print(f\"Structure Reader: {len(top_struct_reader_float_heads)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_value_fetcher = float_circuit[\"value_fetcher\"]\n",
    "float_pos_transmitter = float_circuit[\"pos_transmitter\"]\n",
    "float_pos_detector = float_circuit[\"pos_detector\"]\n",
    "float_struct_reader = float_circuit[\"struct_reader\"]\n",
    "\n",
    "print(\n",
    "    len(float_value_fetcher),\n",
    "    len(float_pos_transmitter),\n",
    "    len(float_pos_detector),\n",
    "    len(float_struct_reader),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tuple of (head, patching_score, position) for each group in Goat circuit\n",
    "float_value_fetcher_tuple = []\n",
    "for head, score in zip(top_value_fetcher_float_heads, top_value_fetcher_float_score):\n",
    "    float_value_fetcher_tuple.append((head, abs(score), 0))\n",
    "\n",
    "float_pos_transmitter_tuple = []\n",
    "for head, score in zip(top_pos_transmitter_float_heads, top_pos_transmitter_float_score):\n",
    "    float_pos_transmitter_tuple.append((head, abs(score), 0))\n",
    "\n",
    "float_pos_detector_tuple = []\n",
    "for head, score in zip(top_pos_detector_float_heads, top_pos_detector_float_score):\n",
    "    float_pos_detector_tuple.append((head, abs(score), 2))\n",
    "\n",
    "float_struct_reader_tuple = []\n",
    "for head, score in zip(top_struct_reader_float_heads, top_struct_reader_float_score):\n",
    "    float_struct_reader_tuple.append((head, abs(score), -1))\n",
    "\n",
    "float_circuit_tuple = (\n",
    "    float_value_fetcher_tuple\n",
    "    + float_pos_transmitter_tuple\n",
    "    + float_pos_detector_tuple\n",
    "    + float_struct_reader_tuple\n",
    ")\n",
    "float_circuit_tuple = sorted(float_circuit_tuple, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each head in float_circuit_tuple, find the corresponding head in llama_circuit_tuple\n",
    "# and append it to a new list. If the head is not present in llama_circuit_tuple, then\n",
    "# append it the same list with a patching score of 0.\n",
    "ordered_llama_circuit_tuple = []\n",
    "for f_head, f_score, f_pos in float_circuit_tuple:\n",
    "    found = False\n",
    "    for l_head, l_score, l_pos in llama_circuit_tuple:\n",
    "        if f_head == l_head and f_pos == l_pos:\n",
    "            ordered_llama_circuit_tuple.append((l_head, l_score, l_pos))\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "    if not found:\n",
    "        ordered_llama_circuit_tuple.append((f_head, 0, f_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 5), sharex=True)\n",
    "\n",
    "# Use times new roman font\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 18\n",
    "\n",
    "ax1.bar(\n",
    "    [i for i in range(len(float_circuit_tuple))],\n",
    "    [score for _, score, _ in float_circuit_tuple],\n",
    "    color=\"red\",\n",
    ")\n",
    "for i in range(len(float_circuit_tuple)):\n",
    "    if (\n",
    "        float_circuit_tuple[i][0] not in float_value_fetcher\n",
    "        and float_circuit_tuple[i][0] not in float_pos_transmitter\n",
    "        and float_circuit_tuple[i][0] not in float_pos_detector\n",
    "        and float_circuit_tuple[i][0] not in float_struct_reader\n",
    "    ):\n",
    "        ax1.bar(\n",
    "            i,\n",
    "            float_circuit_tuple[i][1],\n",
    "            color=\"silver\",\n",
    "        )\n",
    "\n",
    "red_patch = mpatches.Patch(color=\"red\", label=\"Heads in FLoat Circuit\")\n",
    "silver_patch = mpatches.Patch(color=\"silver\", label=\"Heads NOT in FLoat Circuit\")\n",
    "ax1.legend(handles=[red_patch, silver_patch], loc=\"upper right\")\n",
    "\n",
    "ax2.bar(\n",
    "    [i for i in range(len(ordered_llama_circuit_tuple))],\n",
    "    [score for _, score, _ in ordered_llama_circuit_tuple],\n",
    "    color=\"green\",\n",
    ")\n",
    "for i in range(len(ordered_llama_circuit_tuple)):\n",
    "    if (\n",
    "        ordered_llama_circuit_tuple[i][0] not in llama_value_fetcher\n",
    "        and ordered_llama_circuit_tuple[i][0] not in llama_pos_transmitter\n",
    "        and ordered_llama_circuit_tuple[i][0] not in llama_pos_detector\n",
    "        and ordered_llama_circuit_tuple[i][0] not in llama_struct_reader\n",
    "    ):\n",
    "        ax2.bar(\n",
    "            i,\n",
    "            ordered_llama_circuit_tuple[i][1],\n",
    "            color=\"silver\",\n",
    "        )\n",
    "\n",
    "green_patch = mpatches.Patch(color=\"green\", label=\"Heads in Llama Circuit\")\n",
    "silver_patch = mpatches.Patch(color=\"silver\", label=\"Heads NOT in Llama Circuit\")\n",
    "ax2.legend(handles=[green_patch, silver_patch], loc=\"lower right\")\n",
    "\n",
    "# Remove the blank space before and after the bar chart\n",
    "plt.xlim(-1, 200 - 0.5)\n",
    "\n",
    "# Reduce the gap between the two subplots\n",
    "plt.subplots_adjust(hspace=0.01)\n",
    "# Flip the y-axis of the second subplot\n",
    "ax2.invert_yaxis()\n",
    "# Remove 0.0 from the y-axis of the second subplot\n",
    "ax1.set_yticks([0.1, 0.2, 0.3])\n",
    "ax2.set_yticks([0.1, 0.2, 0.3])\n",
    "\n",
    "# Set a single y-axis label for both subplots\n",
    "fig.text(0.07, 0.5, \"Absolute Patching Score\", va=\"center\", rotation=\"vertical\")\n",
    "\n",
    "ax2.set_xlabel(\"Heads\")\n",
    "ax1.set_title(\"Causal Impact of Heads in Llama and FLoat Circuit\")\n",
    "\n",
    "# Remove plox plot frame\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1.spines[\"right\"].set_visible(False)\n",
    "ax1.spines[\"bottom\"].set_visible(False)\n",
    "ax1.spines[\"left\"].set_visible(False)\n",
    "\n",
    "ax2.spines[\"top\"].set_visible(False)\n",
    "ax2.spines[\"right\"].set_visible(False)\n",
    "ax2.spines[\"bottom\"].set_visible(False)\n",
    "ax2.spines[\"left\"].set_visible(False)\n",
    "\n",
    "# Save the plot\n",
    "# plt.plot()\n",
    "plt.savefig(\"./results/figures/float_llama_causal_impact.pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anima",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
