{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014efce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "from functools import partial\n",
    "from baukit import TraceDict\n",
    "from einops import rearrange, einsum\n",
    "from tqdm import tqdm\n",
    "\n",
    "from cmap_utils import get_model_and_tokenizer, load_data, eval_model_performance, cmap_in, cmap_out\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc9e7e3",
   "metadata": {},
   "source": [
    "### Loading Models and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e0c6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01303ed7cd6240968f3de896241b2c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03cfd48f7ebe4ade83b997229353f6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_model, tokenizer = get_model_and_tokenizer(model_name=\"llama\", device=device)\n",
    "goat_model, _ = get_model_and_tokenizer(model_name=\"naive\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2bbc13",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767b0032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 500\n"
     ]
    }
   ],
   "source": [
    "data_file = \"../data/dataset.jsonl\"\n",
    "dataloader = load_data(tokenizer=tokenizer, data_file=data_file, num_samples=500, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7764d3e",
   "metadata": {},
   "source": [
    "### Models Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc32e9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/63 [00:00<00:32,  1.91it/s]\n",
      "  3%|▎         | 2/63 [00:00<00:26,  2.28it/s]\n",
      "  5%|▍         | 3/63 [00:01<00:24,  2.43it/s]\n",
      "  6%|▋         | 4/63 [00:01<00:23,  2.50it/s]\n",
      "  8%|▊         | 5/63 [00:02<00:22,  2.55it/s]\n",
      " 10%|▉         | 6/63 [00:02<00:22,  2.58it/s]\n",
      " 11%|█         | 7/63 [00:02<00:21,  2.60it/s]\n",
      " 13%|█▎        | 8/63 [00:03<00:21,  2.61it/s]\n",
      " 14%|█▍        | 9/63 [00:03<00:20,  2.62it/s]\n",
      " 16%|█▌        | 10/63 [00:03<00:20,  2.62it/s]\n",
      " 17%|█▋        | 11/63 [00:04<00:19,  2.63it/s]\n",
      " 19%|█▉        | 12/63 [00:04<00:19,  2.63it/s]\n",
      " 21%|██        | 13/63 [00:05<00:19,  2.63it/s]\n",
      " 22%|██▏       | 14/63 [00:05<00:18,  2.62it/s]\n",
      " 24%|██▍       | 15/63 [00:05<00:18,  2.62it/s]\n",
      " 25%|██▌       | 16/63 [00:06<00:17,  2.62it/s]\n",
      " 27%|██▋       | 17/63 [00:06<00:17,  2.62it/s]\n",
      " 29%|██▊       | 18/63 [00:06<00:17,  2.62it/s]\n",
      " 30%|███       | 19/63 [00:07<00:16,  2.62it/s]\n",
      " 32%|███▏      | 20/63 [00:07<00:16,  2.62it/s]\n",
      " 33%|███▎      | 21/63 [00:08<00:16,  2.62it/s]\n",
      " 35%|███▍      | 22/63 [00:08<00:15,  2.62it/s]\n",
      " 37%|███▋      | 23/63 [00:08<00:15,  2.62it/s]\n",
      " 38%|███▊      | 24/63 [00:09<00:14,  2.62it/s]\n",
      " 40%|███▉      | 25/63 [00:09<00:14,  2.62it/s]\n",
      " 41%|████▏     | 26/63 [00:10<00:14,  2.62it/s]\n",
      " 43%|████▎     | 27/63 [00:10<00:13,  2.62it/s]\n",
      " 44%|████▍     | 28/63 [00:10<00:13,  2.62it/s]\n",
      " 46%|████▌     | 29/63 [00:11<00:12,  2.62it/s]\n",
      " 48%|████▊     | 30/63 [00:11<00:12,  2.62it/s]\n",
      " 49%|████▉     | 31/63 [00:11<00:12,  2.62it/s]\n",
      " 51%|█████     | 32/63 [00:12<00:11,  2.62it/s]\n",
      " 52%|█████▏    | 33/63 [00:12<00:11,  2.62it/s]\n",
      " 54%|█████▍    | 34/63 [00:13<00:11,  2.62it/s]\n",
      " 56%|█████▌    | 35/63 [00:13<00:10,  2.63it/s]\n",
      " 57%|█████▋    | 36/63 [00:13<00:10,  2.63it/s]\n",
      " 59%|█████▊    | 37/63 [00:14<00:09,  2.63it/s]\n",
      " 60%|██████    | 38/63 [00:14<00:09,  2.63it/s]\n",
      " 62%|██████▏   | 39/63 [00:14<00:09,  2.63it/s]\n",
      " 63%|██████▎   | 40/63 [00:15<00:08,  2.63it/s]\n",
      " 65%|██████▌   | 41/63 [00:15<00:08,  2.63it/s]\n",
      " 67%|██████▋   | 42/63 [00:16<00:07,  2.63it/s]\n",
      " 68%|██████▊   | 43/63 [00:16<00:07,  2.63it/s]\n",
      " 70%|██████▉   | 44/63 [00:16<00:07,  2.63it/s]\n",
      " 71%|███████▏  | 45/63 [00:17<00:06,  2.63it/s]\n",
      " 73%|███████▎  | 46/63 [00:17<00:06,  2.62it/s]\n",
      " 75%|███████▍  | 47/63 [00:18<00:06,  2.62it/s]\n",
      " 76%|███████▌  | 48/63 [00:18<00:05,  2.62it/s]\n",
      " 78%|███████▊  | 49/63 [00:18<00:05,  2.62it/s]\n",
      " 79%|███████▉  | 50/63 [00:19<00:04,  2.62it/s]\n",
      " 81%|████████  | 51/63 [00:19<00:04,  2.62it/s]\n",
      " 83%|████████▎ | 52/63 [00:19<00:04,  2.62it/s]\n",
      " 84%|████████▍ | 53/63 [00:20<00:03,  2.62it/s]\n",
      " 86%|████████▌ | 54/63 [00:20<00:03,  2.62it/s]\n",
      " 87%|████████▋ | 55/63 [00:21<00:03,  2.62it/s]\n",
      " 89%|████████▉ | 56/63 [00:21<00:02,  2.62it/s]\n",
      " 90%|█████████ | 57/63 [00:21<00:02,  2.62it/s]\n",
      " 92%|█████████▏| 58/63 [00:22<00:01,  2.62it/s]\n",
      " 94%|█████████▎| 59/63 [00:22<00:01,  2.62it/s]\n",
      " 95%|█████████▌| 60/63 [00:23<00:01,  2.62it/s]\n",
      " 97%|█████████▋| 61/63 [00:23<00:00,  2.62it/s]\n",
      " 98%|█████████▊| 62/63 [00:23<00:00,  2.62it/s]\n",
      "100%|██████████| 63/63 [00:23<00:00,  2.63it/s]\n",
      "63it [00:23,  2.63it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/63 [00:00<00:23,  2.61it/s]\n",
      "  3%|▎         | 2/63 [00:00<00:23,  2.62it/s]\n",
      "  5%|▍         | 3/63 [00:01<00:22,  2.62it/s]\n",
      "  6%|▋         | 4/63 [00:01<00:22,  2.62it/s]\n",
      "  8%|▊         | 5/63 [00:01<00:22,  2.62it/s]\n",
      " 10%|▉         | 6/63 [00:02<00:21,  2.62it/s]\n",
      " 11%|█         | 7/63 [00:02<00:21,  2.62it/s]\n",
      " 13%|█▎        | 8/63 [00:03<00:20,  2.62it/s]\n",
      " 14%|█▍        | 9/63 [00:03<00:20,  2.62it/s]\n",
      " 16%|█▌        | 10/63 [00:03<00:20,  2.62it/s]\n",
      " 17%|█▋        | 11/63 [00:04<00:19,  2.62it/s]\n",
      " 19%|█▉        | 12/63 [00:04<00:19,  2.62it/s]\n",
      " 21%|██        | 13/63 [00:04<00:19,  2.62it/s]\n",
      " 22%|██▏       | 14/63 [00:05<00:18,  2.62it/s]\n",
      " 24%|██▍       | 15/63 [00:05<00:18,  2.62it/s]\n",
      " 25%|██▌       | 16/63 [00:06<00:17,  2.62it/s]\n",
      " 27%|██▋       | 17/63 [00:06<00:17,  2.62it/s]\n",
      " 29%|██▊       | 18/63 [00:06<00:17,  2.62it/s]\n",
      " 30%|███       | 19/63 [00:07<00:16,  2.62it/s]\n",
      " 32%|███▏      | 20/63 [00:07<00:16,  2.62it/s]\n",
      " 33%|███▎      | 21/63 [00:08<00:16,  2.62it/s]\n",
      " 35%|███▍      | 22/63 [00:08<00:15,  2.62it/s]\n",
      " 37%|███▋      | 23/63 [00:08<00:15,  2.62it/s]\n",
      " 38%|███▊      | 24/63 [00:09<00:14,  2.62it/s]\n",
      " 40%|███▉      | 25/63 [00:09<00:14,  2.62it/s]\n",
      " 41%|████▏     | 26/63 [00:09<00:14,  2.62it/s]\n",
      " 43%|████▎     | 27/63 [00:10<00:13,  2.62it/s]\n",
      " 44%|████▍     | 28/63 [00:10<00:13,  2.62it/s]\n",
      " 46%|████▌     | 29/63 [00:11<00:12,  2.62it/s]\n",
      " 48%|████▊     | 30/63 [00:11<00:12,  2.62it/s]\n",
      " 49%|████▉     | 31/63 [00:11<00:12,  2.62it/s]\n",
      " 51%|█████     | 32/63 [00:12<00:11,  2.62it/s]\n",
      " 52%|█████▏    | 33/63 [00:12<00:11,  2.63it/s]\n",
      " 54%|█████▍    | 34/63 [00:12<00:11,  2.63it/s]\n",
      " 56%|█████▌    | 35/63 [00:13<00:10,  2.63it/s]\n",
      " 57%|█████▋    | 36/63 [00:13<00:10,  2.63it/s]\n",
      " 59%|█████▊    | 37/63 [00:14<00:09,  2.63it/s]\n",
      " 60%|██████    | 38/63 [00:14<00:09,  2.63it/s]\n",
      " 62%|██████▏   | 39/63 [00:14<00:09,  2.63it/s]\n",
      " 63%|██████▎   | 40/63 [00:15<00:08,  2.63it/s]\n",
      " 65%|██████▌   | 41/63 [00:15<00:08,  2.63it/s]\n",
      " 67%|██████▋   | 42/63 [00:16<00:07,  2.63it/s]\n",
      " 68%|██████▊   | 43/63 [00:16<00:07,  2.63it/s]\n",
      " 70%|██████▉   | 44/63 [00:16<00:07,  2.63it/s]\n",
      " 71%|███████▏  | 45/63 [00:17<00:06,  2.63it/s]\n",
      " 73%|███████▎  | 46/63 [00:17<00:06,  2.63it/s]\n",
      " 75%|███████▍  | 47/63 [00:17<00:06,  2.63it/s]\n",
      " 76%|███████▌  | 48/63 [00:18<00:05,  2.63it/s]\n",
      " 78%|███████▊  | 49/63 [00:18<00:05,  2.63it/s]\n",
      " 79%|███████▉  | 50/63 [00:19<00:04,  2.63it/s]\n",
      " 81%|████████  | 51/63 [00:19<00:04,  2.63it/s]\n",
      " 83%|████████▎ | 52/63 [00:19<00:04,  2.63it/s]\n",
      " 84%|████████▍ | 53/63 [00:20<00:03,  2.63it/s]\n",
      " 86%|████████▌ | 54/63 [00:20<00:03,  2.63it/s]\n",
      " 87%|████████▋ | 55/63 [00:20<00:03,  2.63it/s]\n",
      " 89%|████████▉ | 56/63 [00:21<00:02,  2.63it/s]\n",
      " 90%|█████████ | 57/63 [00:21<00:02,  2.63it/s]\n",
      " 92%|█████████▏| 58/63 [00:22<00:01,  2.63it/s]\n",
      " 94%|█████████▎| 59/63 [00:22<00:01,  2.63it/s]\n",
      " 95%|█████████▌| 60/63 [00:22<00:01,  2.63it/s]\n",
      " 97%|█████████▋| 61/63 [00:23<00:00,  2.63it/s]\n",
      " 98%|█████████▊| 62/63 [00:23<00:00,  2.63it/s]\n",
      "100%|██████████| 63/63 [00:23<00:00,  2.64it/s]\n",
      "63it [00:23,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA accuracy: 0.66\n",
      "Goat accuracy: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llama_acc = eval_model_performance(llama_model, dataloader, device)\n",
    "goat_acc = eval_model_performance(goat_model, dataloader, device)\n",
    "\n",
    "print(f\"LLAMA accuracy: {llama_acc}\")\n",
    "print(f\"Goat accuracy: {goat_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ce7ad",
   "metadata": {},
   "source": [
    "### Loading Model Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "247e0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_modules = [[f\"model.layers.{layer}.self_attn.k_proj\", \n",
    "                  f\"model.layers.{layer}.self_attn.q_proj\",\n",
    "                  f\"model.layers.{layer}.self_attn.v_proj\",\n",
    "                 f\"model.layers.{layer}.self_attn.o_proj\"] \n",
    "                 for layer in range(llama_model.config.num_hidden_layers)]\n",
    "goat_modules = [[f\"model.layers.{layer}.self_attn.k_proj\", \n",
    "                 f\"model.layers.{layer}.self_attn.q_proj\",\n",
    "                 f\"model.layers.{layer}.self_attn.v_proj\",\n",
    "                f\"model.layers.{layer}.self_attn.o_proj\"] \n",
    "                for layer in range(goat_model.config.num_hidden_layers)]\n",
    "\n",
    "llama_modules = [item for sublist in llama_modules for item in sublist]\n",
    "goat_modules = [item for sublist in goat_modules for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95e4dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "goat_cache: 63it [00:44,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "goat_cache = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader), desc=\"goat_cache\"):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(goat_model.device)\n",
    "\n",
    "        with TraceDict(goat_model, goat_modules, retain_input=True) as cache:\n",
    "            _ = goat_model(inputs[\"input_ids\"])\n",
    "        \n",
    "        for goat_layer, llama_layer in zip(goat_modules, llama_modules):\n",
    "            if \"o_proj\" in llama_layer and \"o_proj\" in goat_layer:\n",
    "                if bi in goat_cache:\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].input.cpu()\n",
    "                else:\n",
    "                    goat_cache[bi] = {}\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].input.cpu()\n",
    "            else:\n",
    "                if bi in goat_cache:\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].output.cpu()\n",
    "                else:\n",
    "                    goat_cache[bi] = {}\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9b229d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_cache: 63it [00:44,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "llama_cache = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader), desc=\"llama_cache\"):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        with TraceDict(llama_model, llama_modules, retain_input=True) as cache:\n",
    "            _ = llama_model(inputs[\"input_ids\"])\n",
    "        \n",
    "        for llama_layer in llama_modules:\n",
    "            if \"o_proj\" in llama_layer:\n",
    "                if bi in llama_cache:\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].input.cpu()\n",
    "                else:\n",
    "                    llama_cache[bi] = {}\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].input.cpu()\n",
    "            else:\n",
    "                if bi in llama_cache:\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].output.cpu()\n",
    "                else:\n",
    "                    llama_cache[bi] = {}\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].output.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f118f",
   "metadata": {},
   "source": [
    "### Loading circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35e2f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../experiment_1/results/circuits/llama_circuit.json\", \"r\") as f:\n",
    "    llama_circuit = json.load(f)\n",
    "\n",
    "with open(\"../experiment_1/results/circuits/goat_circuit.json\", \"r\") as f:\n",
    "    goat_circuit = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "089e404e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Fetcher Heads: 40\n",
      "Heads affecting direct logit heads: 5\n",
      "Heads at query box token: 14\n",
      "Heads at prev query box token: 5\n"
     ]
    }
   ],
   "source": [
    "with open(\"../experiment_2/results/DCM/llama_circuit/value_fetcher/object_value/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    llama_value_fetcher = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "with open(\"../experiment_2/results/DCM/llama_circuit/pos_transmitter/positional/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    llama_pos_transmitter = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "with open(\"../experiment_2/results/DCM/llama_circuit/pos_detector/positional/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    llama_pos_detector = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "llama_struct_reader = llama_circuit[\"struct_reader\"]\n",
    "\n",
    "print(f\"Value Fetcher Heads: {len(llama_value_fetcher)}\")\n",
    "print(f\"Heads affecting direct logit heads: {len(llama_pos_transmitter)}\")\n",
    "print(f\"Heads at query box token: {len(llama_pos_detector)}\")\n",
    "print(f\"Heads at prev query box token: {len(llama_struct_reader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d49c29b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Fetcher Heads: 56\n",
      "Heads affecting direct logit heads: 15\n",
      "Heads at query box token: 18\n",
      "Heads at prev query box token: 39\n"
     ]
    }
   ],
   "source": [
    "with open(\"../experiment_2/results/DCM/goat_circuit/value_fetcher/object_value/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    goat_value_fetcher = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "with open(\"../experiment_2/results/DCM/goat_circuit/pos_transmitter/positional/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    goat_pos_transmitter = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "with open(\"../experiment_2/results/DCM/goat_circuit/pos_detector/positional/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    goat_pos_detector = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "goat_struct_reader = goat_circuit[\"struct_reader\"]\n",
    "\n",
    "print(f\"Value Fetcher Heads: {len(goat_value_fetcher)}\")\n",
    "print(f\"Heads affecting direct logit heads: {len(goat_pos_transmitter)}\")\n",
    "print(f\"Heads at query box token: {len(goat_pos_detector)}\")\n",
    "print(f\"Heads at prev query box token: {len(goat_struct_reader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2b096",
   "metadata": {},
   "source": [
    "### CMAP (output patching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee702e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full circuit (Select group of heads for CMAP accordingly)\n",
    "pos_heads_dict = {}\n",
    "# pos_heads_dict[0] = goat_value_fetcher\n",
    "pos_heads_dict[0] = goat_circuit['pos_transmitter']\n",
    "# pos_heads_dict[2] = goat_pos_detector\n",
    "# pos_heads_dict[-1] = goat_circuit['struct_reader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c95c5393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:28,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_count, total_count = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        with TraceDict(llama_model,\n",
    "                       llama_modules,\n",
    "                       retain_input=True,\n",
    "                       edit_output=partial(\n",
    "                            cmap_out,\n",
    "                            model = llama_model,\n",
    "                            goat_cache = goat_cache,\n",
    "                            bi = bi,\n",
    "                            pos_heads_dict = pos_heads_dict,\n",
    "                            input_tokens = inputs)) as _:\n",
    "                outputs = llama_model(inputs[\"input_ids\"], output_attentions=True)\n",
    "\n",
    "        for bi in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][bi]\n",
    "            pred = torch.argmax(outputs.logits[bi][inputs[\"last_token_indices\"][bi]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            total_count += 1\n",
    "\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "current_acc = round(correct_count / total_count, 2)\n",
    "print(f\"Task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741276db",
   "metadata": {},
   "source": [
    "Output CMAP Results (Goat -> Llama):\n",
    "- Full Circuit: 0.82\n",
    "- Value Fetcher: 0.82\n",
    "- Position Transmitter: 0.78\n",
    "- Position Detector: 0.62\n",
    "- Structure Reader: 0.65\n",
    "- Heads in Group B: 0.81\n",
    "\n",
    "Output CMAP Results (Naive -> Llama):\n",
    "- Full Circuit: 0.82\n",
    "- Value Fetcher: 0.81\n",
    "- Position Transmitter: 0.74\n",
    "- Position Detector: 0.52\n",
    "- Structure Reader: 0.64\n",
    "- Heads in Group B: 0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa17d88",
   "metadata": {},
   "source": [
    "### CMAP (input patching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "d2eb4a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select group of heads for CMAP accordingly\n",
    "pos_heads_dict = {}\n",
    "# pos_heads_dict[0] = llama_value_fetcher\n",
    "pos_heads_dict[0] = goat_pos_transmitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "b7627498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [01:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_count, total_count = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        with TraceDict(llama_model,\n",
    "                       llama_modules,\n",
    "                       retain_input=True,\n",
    "                       edit_output=partial(\n",
    "                            cmap_in,\n",
    "                            model = llama_model,\n",
    "                            goat_cache = goat_cache,\n",
    "                            llama_cache = llama_cache,\n",
    "                            patching_component = [\"q_proj\", \"k_proj\", \"v_proj\"], #Options: \"q_proj\" (query), \"k_proj\" (key), \"v_proj\" (value)\n",
    "                            bi = bi,\n",
    "                            pos_heads_dict = pos_heads_dict,\n",
    "                            input_tokens = inputs)) as _:\n",
    "                outputs = llama_model(inputs[\"input_ids\"], output_attentions=True)\n",
    "\n",
    "        for bi in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][bi]\n",
    "            pred = torch.argmax(outputs.logits[bi][inputs[\"last_token_indices\"][bi]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            total_count += 1\n",
    "\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "current_acc = round(correct_count / total_count, 2)\n",
    "print(f\"Task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a83b98",
   "metadata": {},
   "source": [
    "Input CMAP Results:\n",
    "- Value Fetcher:\n",
    "    - Query: 0.77\n",
    "    - Key: 0.63\n",
    "    - Value: 0.69\n",
    "    - QK: 0.76\n",
    "    - QKV: 0.77\n",
    "- Position Transmitter:\n",
    "    - Query: 0.66\n",
    "    - Key: 0.65\n",
    "    - Value: 0.69\n",
    "    - QK: 0.65\n",
    "    - QKV: 0.67"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc689b",
   "metadata": {},
   "source": [
    "### Patching Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95c6b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_weights(inputs=None, \n",
    "                  output=None, \n",
    "                  layer=None, \n",
    "                  bi=None, \n",
    "                  patching_weight_matrix=None,\n",
    "                  weight_patching_heads=None,\n",
    "                  output_patching_heads=None,\n",
    "                  weight_rel_pos=None,\n",
    "                  output_rel_pos=None):\n",
    "    if isinstance(inputs, tuple):\n",
    "        inputs = inputs[0]\n",
    "\n",
    "    layer_idx = int(layer.split(\".\")[2])\n",
    "    weight_patching_heads_curr_layer = [h for l, h in weight_patching_heads if l == layer_idx]\n",
    "    output_patching_heads_curr_layer = [h for l, h in output_patching_heads if l == layer_idx]\n",
    "    \n",
    "    num_heads = llama_model.config.num_attention_heads\n",
    "    head_size = llama_model.config.hidden_size // num_heads\n",
    "\n",
    "    if (patching_weight_matrix in layer) and len(weight_patching_heads_curr_layer) > 0:\n",
    "        llama_w = llama_model.state_dict()[f\"{layer}.weight\"].clone()\n",
    "        \n",
    "        w_o = goat_model.state_dict()[f\"base_model.model.{layer}.weight\"].clone()\n",
    "        lora_A = goat_model.state_dict()[f\"base_model.model.{layer}.lora_A.default.weight\"].clone()\n",
    "        lora_B = goat_model.state_dict()[f\"base_model.model.{layer}.lora_B.default.weight\"].clone()\n",
    "        goat_w = w_o + lora_B @ lora_A\n",
    "\n",
    "        for head_idx in weight_patching_heads_curr_layer:\n",
    "            head_start = head_idx * head_size\n",
    "            head_end = (head_idx + 1) * head_size\n",
    "            llama_w[head_start:head_end, :] = goat_w[head_start:head_end, :]\n",
    "\n",
    "        output = inputs @ llama_w.T\n",
    "        pos = inputs.size(1) - weight_rel_pos - 1\n",
    "#         output = torch.cat((output[:, :pos], new_output[:, pos].unsqueeze(dim=1)), dim=1)\n",
    "    \n",
    "    if (\"o_proj\" in layer) and (len(output_patching_heads_curr_layer) > 0):\n",
    "        inputs = rearrange(inputs, \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\", n_heads=32)\n",
    "        gcache = rearrange(goat_cache[bi][layer], \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\", n_heads=32)\n",
    "\n",
    "        pos = inputs.size(1) - output_rel_pos - 1\n",
    "        for head_idx in output_patching_heads_curr_layer:\n",
    "            inputs[:, pos, head_idx] = gcache[:, pos, head_idx]\n",
    "        \n",
    "        inputs = rearrange(inputs, \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\", n_heads=32)\n",
    "        w_o = llama_model.state_dict()[f\"{layer}.weight\"]\n",
    "        output = einsum(\n",
    "            inputs, w_o, \"batch seq_len hidden_size, d_model hidden_size -> batch seq_len d_model\"\n",
    "        )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9aa2800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/63 [00:00<00:27,  2.23it/s]\n",
      "  3%|▎         | 2/63 [00:00<00:26,  2.33it/s]\n",
      "  5%|▍         | 3/63 [00:01<00:25,  2.37it/s]\n",
      "  6%|▋         | 4/63 [00:01<00:24,  2.39it/s]\n",
      "  8%|▊         | 5/63 [00:02<00:24,  2.40it/s]\n",
      " 10%|▉         | 6/63 [00:02<00:23,  2.40it/s]\n",
      " 11%|█         | 7/63 [00:02<00:23,  2.41it/s]\n",
      " 13%|█▎        | 8/63 [00:03<00:22,  2.41it/s]\n",
      " 14%|█▍        | 9/63 [00:03<00:22,  2.41it/s]\n",
      " 16%|█▌        | 10/63 [00:04<00:21,  2.41it/s]\n",
      " 17%|█▋        | 11/63 [00:04<00:21,  2.41it/s]\n",
      " 19%|█▉        | 12/63 [00:05<00:21,  2.42it/s]\n",
      " 21%|██        | 13/63 [00:05<00:20,  2.42it/s]\n",
      " 22%|██▏       | 14/63 [00:05<00:20,  2.42it/s]\n",
      " 24%|██▍       | 15/63 [00:06<00:20,  2.39it/s]\n",
      " 25%|██▌       | 16/63 [00:06<00:19,  2.40it/s]\n",
      " 27%|██▋       | 17/63 [00:07<00:19,  2.35it/s]\n",
      " 29%|██▊       | 18/63 [00:07<00:19,  2.37it/s]\n",
      " 30%|███       | 19/63 [00:07<00:18,  2.38it/s]\n",
      " 32%|███▏      | 20/63 [00:08<00:17,  2.39it/s]\n",
      " 33%|███▎      | 21/63 [00:08<00:17,  2.40it/s]\n",
      " 35%|███▍      | 22/63 [00:09<00:17,  2.41it/s]\n",
      " 37%|███▋      | 23/63 [00:09<00:16,  2.41it/s]\n",
      " 38%|███▊      | 24/63 [00:10<00:16,  2.41it/s]\n",
      " 40%|███▉      | 25/63 [00:10<00:15,  2.41it/s]\n",
      " 41%|████▏     | 26/63 [00:10<00:15,  2.41it/s]\n",
      " 43%|████▎     | 27/63 [00:11<00:14,  2.41it/s]\n",
      " 44%|████▍     | 28/63 [00:11<00:14,  2.41it/s]\n",
      " 46%|████▌     | 29/63 [00:12<00:14,  2.41it/s]\n",
      " 48%|████▊     | 30/63 [00:12<00:13,  2.41it/s]\n",
      " 49%|████▉     | 31/63 [00:12<00:13,  2.41it/s]\n",
      " 51%|█████     | 32/63 [00:13<00:12,  2.41it/s]\n",
      " 52%|█████▏    | 33/63 [00:13<00:12,  2.36it/s]\n",
      " 54%|█████▍    | 34/63 [00:14<00:12,  2.37it/s]\n",
      " 56%|█████▌    | 35/63 [00:14<00:11,  2.38it/s]\n",
      " 57%|█████▋    | 36/63 [00:15<00:11,  2.39it/s]\n",
      " 59%|█████▊    | 37/63 [00:15<00:10,  2.40it/s]\n",
      " 60%|██████    | 38/63 [00:15<00:10,  2.40it/s]\n",
      " 62%|██████▏   | 39/63 [00:16<00:09,  2.41it/s]\n",
      " 63%|██████▎   | 40/63 [00:16<00:09,  2.41it/s]\n",
      " 65%|██████▌   | 41/63 [00:17<00:09,  2.40it/s]\n",
      " 67%|██████▋   | 42/63 [00:17<00:08,  2.34it/s]\n",
      " 68%|██████▊   | 43/63 [00:17<00:08,  2.36it/s]\n",
      " 70%|██████▉   | 44/63 [00:18<00:07,  2.38it/s]\n",
      " 71%|███████▏  | 45/63 [00:18<00:07,  2.39it/s]\n",
      " 73%|███████▎  | 46/63 [00:19<00:07,  2.40it/s]\n",
      " 75%|███████▍  | 47/63 [00:19<00:06,  2.40it/s]\n",
      " 76%|███████▌  | 48/63 [00:20<00:06,  2.41it/s]\n",
      " 78%|███████▊  | 49/63 [00:20<00:05,  2.41it/s]\n",
      " 79%|███████▉  | 50/63 [00:20<00:05,  2.41it/s]\n",
      " 81%|████████  | 51/63 [00:21<00:04,  2.41it/s]\n",
      " 83%|████████▎ | 52/63 [00:21<00:04,  2.42it/s]\n",
      " 84%|████████▍ | 53/63 [00:22<00:04,  2.42it/s]\n",
      " 86%|████████▌ | 54/63 [00:22<00:03,  2.42it/s]\n",
      " 87%|████████▋ | 55/63 [00:22<00:03,  2.42it/s]\n",
      " 89%|████████▉ | 56/63 [00:23<00:02,  2.42it/s]\n",
      " 90%|█████████ | 57/63 [00:23<00:02,  2.42it/s]\n",
      " 92%|█████████▏| 58/63 [00:24<00:02,  2.42it/s]\n",
      " 94%|█████████▎| 59/63 [00:24<00:01,  2.42it/s]\n",
      " 95%|█████████▌| 60/63 [00:24<00:01,  2.41it/s]\n",
      " 97%|█████████▋| 61/63 [00:25<00:00,  2.41it/s]\n",
      " 98%|█████████▊| 62/63 [00:25<00:00,  2.41it/s]\n",
      "100%|██████████| 63/63 [00:26<00:00,  2.42it/s]\n",
      "63it [00:26,  2.42it/s]\n"
     ]
    }
   ],
   "source": [
    "correct_count, total_count = 0, 0\n",
    "llama_model.eval()\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(tqdm(dataloader))):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        with TraceDict(\n",
    "            llama_model,\n",
    "            llama_modules,\n",
    "            retain_input=True,\n",
    "            edit_output=partial(\n",
    "                patch_weights,\n",
    "                bi=bi,\n",
    "                patching_weight_matrix=\"v_proj\",\n",
    "                weight_patching_heads=goat_pos_transmitter,\n",
    "                output_patching_heads=[],\n",
    "                weight_rel_pos=0,\n",
    "                output_rel_pos=2,\n",
    "            ),\n",
    "        ) as _:\n",
    "            outputs = llama_model(inputs[\"input_ids\"])\n",
    "\n",
    "        for bi in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][bi]\n",
    "            pred = torch.argmax(outputs.logits[bi][inputs[\"last_token_indices\"][bi]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            total_count += 1\n",
    "\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eabf50b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(correct_count/total_count, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443bff23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae1b1287",
   "metadata": {},
   "source": [
    "### Patching inputs to K/Q/V matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "b2b655a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:34,  1.83it/s]\n"
     ]
    }
   ],
   "source": [
    "goat_input_cache = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(goat_model.device)\n",
    "\n",
    "        with TraceDict(goat_model, goat_modules, retain_input=True) as cache:\n",
    "            _ = goat_model(inputs[\"input_ids\"])\n",
    "\n",
    "        for goat_layer, llama_layer in zip(goat_modules, llama_modules):\n",
    "            if bi in goat_input_cache:\n",
    "                goat_input_cache[bi][llama_layer] = cache[goat_layer].input.cpu()\n",
    "            else:\n",
    "                goat_input_cache[bi] = {}\n",
    "                goat_input_cache[bi][llama_layer] = cache[goat_layer].input.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "88aebfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_inputs(inputs=None, output=None, layer=None, bi=None):\n",
    "    if isinstance(inputs, tuple):\n",
    "        inputs = inputs[0]\n",
    "\n",
    "    position_trans_curr_layer = [h for l, h in goat_pos_transmitter if l == int(layer.split(\".\")[2])]\n",
    "\n",
    "    num_heads = llama_model.config.num_attention_heads\n",
    "    head_size = llama_model.config.hidden_size // num_heads\n",
    "\n",
    "    if (\"v_proj\" in layer) and len(position_trans_curr_layer) > 0:\n",
    "        goat_inp = goat_input_cache[bi][layer]\n",
    "\n",
    "        llama_w = llama_model.state_dict()[f\"{layer}.weight\"].clone()\n",
    "\n",
    "        output = rearrange(output, \n",
    "                   \"batch seq_len (n_heads d_head) -> batch seq_len n_heads d_head\", \n",
    "                   n_heads=goat_model.config.num_attention_heads)\n",
    "\n",
    "        for head_idx in position_trans_curr_layer:\n",
    "            head_start = head_idx * head_size\n",
    "            head_end = (head_idx + 1) * head_size\n",
    "\n",
    "            # Computing the output of q_proj in llama using its corresponding input in goat\n",
    "            res = goat_inp.cuda() @ llama_w[head_start:head_end, :].T\n",
    "\n",
    "            # Since we only patch o/p of q_proj at the last token in the previous\n",
    "            # experiment, here also we patch in the input of q_proj only at the \n",
    "            # last token.\n",
    "            output[:, :, head_idx] = res[:, :]\n",
    "\n",
    "        output = rearrange(output, \n",
    "                       \"batch seq_len n_heads d_head -> batch seq_len (n_heads d_head)\", \n",
    "                       n_heads=llama_model.config.num_attention_heads)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "aecc97f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/63 [00:00<00:28,  2.20it/s]\n",
      "  3%|▎         | 2/63 [00:00<00:26,  2.32it/s]\n",
      "  5%|▍         | 3/63 [00:01<00:25,  2.37it/s]\n",
      "  6%|▋         | 4/63 [00:01<00:24,  2.38it/s]\n",
      "  8%|▊         | 5/63 [00:02<00:24,  2.40it/s]\n",
      " 10%|▉         | 6/63 [00:02<00:23,  2.40it/s]\n",
      " 11%|█         | 7/63 [00:02<00:23,  2.41it/s]\n",
      " 13%|█▎        | 8/63 [00:03<00:22,  2.41it/s]\n",
      " 14%|█▍        | 9/63 [00:03<00:22,  2.41it/s]\n",
      " 16%|█▌        | 10/63 [00:04<00:22,  2.41it/s]\n",
      " 17%|█▋        | 11/63 [00:04<00:21,  2.40it/s]\n",
      " 19%|█▉        | 12/63 [00:05<00:21,  2.41it/s]\n",
      " 21%|██        | 13/63 [00:05<00:20,  2.41it/s]\n",
      " 22%|██▏       | 14/63 [00:05<00:20,  2.41it/s]\n",
      " 24%|██▍       | 15/63 [00:06<00:19,  2.41it/s]\n",
      " 25%|██▌       | 16/63 [00:06<00:19,  2.41it/s]\n",
      " 27%|██▋       | 17/63 [00:07<00:19,  2.40it/s]\n",
      " 29%|██▊       | 18/63 [00:07<00:18,  2.40it/s]\n",
      " 30%|███       | 19/63 [00:07<00:18,  2.40it/s]\n",
      " 32%|███▏      | 20/63 [00:08<00:17,  2.39it/s]\n",
      " 33%|███▎      | 21/63 [00:08<00:17,  2.40it/s]\n",
      " 35%|███▍      | 22/63 [00:09<00:17,  2.40it/s]\n",
      " 37%|███▋      | 23/63 [00:09<00:16,  2.41it/s]\n",
      " 38%|███▊      | 24/63 [00:10<00:16,  2.41it/s]\n",
      " 40%|███▉      | 25/63 [00:10<00:15,  2.41it/s]\n",
      " 41%|████▏     | 26/63 [00:10<00:15,  2.41it/s]\n",
      " 43%|████▎     | 27/63 [00:11<00:14,  2.41it/s]\n",
      " 44%|████▍     | 28/63 [00:11<00:14,  2.37it/s]\n",
      " 46%|████▌     | 29/63 [00:12<00:14,  2.36it/s]\n",
      " 48%|████▊     | 30/63 [00:12<00:13,  2.37it/s]\n",
      " 49%|████▉     | 31/63 [00:12<00:13,  2.38it/s]\n",
      " 51%|█████     | 32/63 [00:13<00:12,  2.39it/s]\n",
      " 52%|█████▏    | 33/63 [00:13<00:12,  2.39it/s]\n",
      " 54%|█████▍    | 34/63 [00:14<00:12,  2.40it/s]\n",
      " 56%|█████▌    | 35/63 [00:14<00:11,  2.40it/s]\n",
      " 57%|█████▋    | 36/63 [00:15<00:11,  2.40it/s]\n",
      " 59%|█████▊    | 37/63 [00:15<00:10,  2.40it/s]\n",
      " 60%|██████    | 38/63 [00:15<00:10,  2.40it/s]\n",
      " 62%|██████▏   | 39/63 [00:16<00:09,  2.41it/s]\n",
      " 63%|██████▎   | 40/63 [00:16<00:09,  2.41it/s]\n",
      " 65%|██████▌   | 41/63 [00:17<00:09,  2.41it/s]\n",
      " 67%|██████▋   | 42/63 [00:17<00:08,  2.40it/s]\n",
      " 68%|██████▊   | 43/63 [00:17<00:08,  2.40it/s]\n",
      " 70%|██████▉   | 44/63 [00:18<00:07,  2.40it/s]\n",
      " 71%|███████▏  | 45/63 [00:18<00:07,  2.41it/s]\n",
      " 73%|███████▎  | 46/63 [00:19<00:07,  2.40it/s]\n",
      " 75%|███████▍  | 47/63 [00:19<00:06,  2.39it/s]\n",
      " 76%|███████▌  | 48/63 [00:20<00:06,  2.40it/s]\n",
      " 78%|███████▊  | 49/63 [00:20<00:05,  2.40it/s]\n",
      " 79%|███████▉  | 50/63 [00:20<00:05,  2.40it/s]\n",
      " 81%|████████  | 51/63 [00:21<00:04,  2.41it/s]\n",
      " 83%|████████▎ | 52/63 [00:21<00:04,  2.41it/s]\n",
      " 84%|████████▍ | 53/63 [00:22<00:04,  2.41it/s]\n",
      " 86%|████████▌ | 54/63 [00:22<00:03,  2.42it/s]\n",
      " 87%|████████▋ | 55/63 [00:22<00:03,  2.42it/s]\n",
      " 89%|████████▉ | 56/63 [00:23<00:03,  1.85it/s]\n",
      " 90%|█████████ | 57/63 [00:24<00:03,  1.99it/s]\n",
      " 92%|█████████▏| 58/63 [00:24<00:02,  2.10it/s]\n",
      " 94%|█████████▎| 59/63 [00:25<00:01,  2.18it/s]\n",
      " 95%|█████████▌| 60/63 [00:25<00:01,  2.25it/s]\n",
      " 97%|█████████▋| 61/63 [00:25<00:00,  2.29it/s]\n",
      " 98%|█████████▊| 62/63 [00:26<00:00,  2.33it/s]\n",
      "100%|██████████| 63/63 [00:26<00:00,  2.38it/s]\n",
      "63it [00:26,  2.38it/s]\n"
     ]
    }
   ],
   "source": [
    "correct_count, total_count = 0, 0\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(tqdm(dataloader))):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        with TraceDict(\n",
    "            llama_model,\n",
    "            llama_modules,\n",
    "            retain_input=True,\n",
    "            edit_output=partial(\n",
    "                patch_inputs,\n",
    "                bi=bi,\n",
    "            ),\n",
    "        ) as _:\n",
    "            outputs = llama_model(inputs[\"input_ids\"])\n",
    "\n",
    "        for bi in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][bi]\n",
    "            pred = torch.argmax(outputs.logits[bi][inputs[\"last_token_indices\"][bi]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            total_count += 1\n",
    "\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "b141b61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(correct_count/total_count, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b4c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
