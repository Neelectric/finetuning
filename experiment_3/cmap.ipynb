{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014efce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "from functools import partial\n",
    "from baukit import TraceDict\n",
    "from einops import rearrange, einsum\n",
    "from tqdm import tqdm\n",
    "\n",
    "from cmap_utils import get_model_and_tokenizer, load_data, eval_model_performance, cmap_in, cmap_out\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc9e7e3",
   "metadata": {},
   "source": [
    "# Loading Models and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e0c6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfaee5f26d342f9811c8fe1a95e923e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbae39be37c649b0bdf487bf4cc2bfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_model, tokenizer = get_model_and_tokenizer(model_name=\"llama\", device=device)\n",
    "# goat_model, _ = get_model_and_tokenizer(model_name=\"goat\", device=device)\n",
    "float_model, _ = get_model_and_tokenizer(model_name=\"float\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2bbc13",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767b0032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 500\n"
     ]
    }
   ],
   "source": [
    "data_file = \"../data/dataset.jsonl\"\n",
    "dataloader = load_data(tokenizer=tokenizer, data_file=data_file, num_samples=500, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f118f",
   "metadata": {},
   "source": [
    "# Loading circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e2f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../experiment_1/results/circuits/llama_circuit.json\", \"r\") as f:\n",
    "    llama_circuit = json.load(f)\n",
    "\n",
    "with open(\"../experiment_1/results/circuits/goat_circuit.json\", \"r\") as f:\n",
    "    goat_circuit = json.load(f)\n",
    "\n",
    "with open(\"../experiment_1/results/circuits/float_circuit.json\", \"r\") as f:\n",
    "    float_circuit = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "089e404e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Fetcher Heads: 40\n",
      "Heads affecting direct logit heads: 5\n",
      "Heads at query box token: 14\n",
      "Heads at prev query box token: 5\n"
     ]
    }
   ],
   "source": [
    "with open(\"../experiment_2/results/DCM/llama_circuit/value_fetcher/object_value/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    llama_value_fetcher = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "with open(\"../experiment_2/results/DCM/llama_circuit/pos_transmitter/positional/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    llama_pos_transmitter = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "with open(\"../experiment_2/results/DCM/llama_circuit/pos_detector/positional/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    llama_pos_detector = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "llama_struct_reader = llama_circuit[\"struct_reader\"]\n",
    "\n",
    "print(f\"Value Fetcher Heads: {len(llama_value_fetcher)}\")\n",
    "print(f\"Heads affecting direct logit heads: {len(llama_pos_transmitter)}\")\n",
    "print(f\"Heads at query box token: {len(llama_pos_detector)}\")\n",
    "print(f\"Heads at prev query box token: {len(llama_struct_reader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d49c29b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Fetcher Heads: 56\n",
      "Heads affecting direct logit heads: 15\n",
      "Heads at query box token: 18\n",
      "Heads at prev query box token: 39\n"
     ]
    }
   ],
   "source": [
    "with open(\"../experiment_2/results/DCM/goat_circuit/value_fetcher/object_value/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    goat_value_fetcher = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "with open(\"../experiment_2/results/DCM/goat_circuit/pos_transmitter/positional/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    goat_pos_transmitter = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "with open(\"../experiment_2/results/DCM/goat_circuit/pos_detector/positional/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    goat_pos_detector = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "goat_struct_reader = goat_circuit[\"struct_reader\"]\n",
    "\n",
    "print(f\"Value Fetcher Heads: {len(goat_value_fetcher)}\")\n",
    "print(f\"Heads affecting direct logit heads: {len(goat_pos_transmitter)}\")\n",
    "print(f\"Heads at query box token: {len(goat_pos_detector)}\")\n",
    "print(f\"Heads at prev query box token: {len(goat_struct_reader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d85b2d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Fetcher Heads: 60\n",
      "Heads affecting direct logit heads: 13\n",
      "Heads at query box token: 22\n",
      "Heads at prev query box token: 38\n"
     ]
    }
   ],
   "source": [
    "with open(\"../experiment_2/results/DCM/float_circuit/value_fetcher/object_value/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    float_value_fetcher = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "with open(\"../experiment_2/results/DCM/float_circuit/pos_transmitter/positional/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    float_pos_transmitter = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "with open(\"../experiment_2/results/DCM/float_circuit/pos_detector/positional/0.01.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    float_pos_detector = json.loads(data[0].split(\": \")[1])\n",
    "\n",
    "float_struct_reader = float_circuit[\"struct_reader\"]\n",
    "\n",
    "print(f\"Value Fetcher Heads: {len(float_value_fetcher)}\")\n",
    "print(f\"Heads affecting direct logit heads: {len(float_pos_transmitter)}\")\n",
    "print(f\"Heads at query box token: {len(float_pos_detector)}\")\n",
    "print(f\"Heads at prev query box token: {len(float_struct_reader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e3df4",
   "metadata": {},
   "source": [
    "# CMAP (Goat -> Llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7764d3e",
   "metadata": {},
   "source": [
    "### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc32e9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/63 [00:00<00:32,  1.90it/s]\n",
      "  3%|▎         | 2/63 [00:00<00:26,  2.27it/s]\n",
      "  5%|▍         | 3/63 [00:01<00:24,  2.42it/s]\n",
      "  6%|▋         | 4/63 [00:01<00:23,  2.50it/s]\n",
      "  8%|▊         | 5/63 [00:02<00:22,  2.54it/s]\n",
      " 10%|▉         | 6/63 [00:02<00:22,  2.57it/s]\n",
      " 11%|█         | 7/63 [00:02<00:21,  2.59it/s]\n",
      " 13%|█▎        | 8/63 [00:03<00:21,  2.60it/s]\n",
      " 14%|█▍        | 9/63 [00:03<00:20,  2.61it/s]\n",
      " 16%|█▌        | 10/63 [00:03<00:20,  2.61it/s]\n",
      " 17%|█▋        | 11/63 [00:04<00:19,  2.62it/s]\n",
      " 19%|█▉        | 12/63 [00:04<00:19,  2.62it/s]\n",
      " 21%|██        | 13/63 [00:05<00:19,  2.62it/s]\n",
      " 22%|██▏       | 14/63 [00:05<00:18,  2.62it/s]\n",
      " 24%|██▍       | 15/63 [00:05<00:18,  2.62it/s]\n",
      " 25%|██▌       | 16/63 [00:06<00:17,  2.62it/s]\n",
      " 27%|██▋       | 17/63 [00:06<00:17,  2.62it/s]\n",
      " 29%|██▊       | 18/63 [00:07<00:17,  2.62it/s]\n",
      " 30%|███       | 19/63 [00:07<00:16,  2.62it/s]\n",
      " 32%|███▏      | 20/63 [00:07<00:16,  2.62it/s]\n",
      " 33%|███▎      | 21/63 [00:08<00:16,  2.62it/s]\n",
      " 35%|███▍      | 22/63 [00:08<00:15,  2.62it/s]\n",
      " 37%|███▋      | 23/63 [00:08<00:15,  2.63it/s]\n",
      " 38%|███▊      | 24/63 [00:09<00:14,  2.63it/s]\n",
      " 40%|███▉      | 25/63 [00:09<00:14,  2.63it/s]\n",
      " 41%|████▏     | 26/63 [00:10<00:14,  2.63it/s]\n",
      " 43%|████▎     | 27/63 [00:10<00:13,  2.63it/s]\n",
      " 44%|████▍     | 28/63 [00:10<00:13,  2.63it/s]\n",
      " 46%|████▌     | 29/63 [00:11<00:12,  2.63it/s]\n",
      " 48%|████▊     | 30/63 [00:11<00:12,  2.63it/s]\n",
      " 49%|████▉     | 31/63 [00:11<00:12,  2.63it/s]\n",
      " 51%|█████     | 32/63 [00:12<00:11,  2.63it/s]\n",
      " 52%|█████▏    | 33/63 [00:12<00:11,  2.63it/s]\n",
      " 54%|█████▍    | 34/63 [00:13<00:11,  2.62it/s]\n",
      " 56%|█████▌    | 35/63 [00:13<00:10,  2.62it/s]\n",
      " 57%|█████▋    | 36/63 [00:13<00:10,  2.62it/s]\n",
      " 59%|█████▊    | 37/63 [00:14<00:09,  2.62it/s]\n",
      " 60%|██████    | 38/63 [00:14<00:09,  2.63it/s]\n",
      " 62%|██████▏   | 39/63 [00:15<00:09,  2.62it/s]\n",
      " 63%|██████▎   | 40/63 [00:15<00:08,  2.63it/s]\n",
      " 65%|██████▌   | 41/63 [00:15<00:08,  2.63it/s]\n",
      " 67%|██████▋   | 42/63 [00:16<00:07,  2.63it/s]\n",
      " 68%|██████▊   | 43/63 [00:16<00:07,  2.63it/s]\n",
      " 70%|██████▉   | 44/63 [00:16<00:07,  2.63it/s]\n",
      " 71%|███████▏  | 45/63 [00:17<00:06,  2.63it/s]\n",
      " 73%|███████▎  | 46/63 [00:17<00:06,  2.63it/s]\n",
      " 75%|███████▍  | 47/63 [00:18<00:06,  2.63it/s]\n",
      " 76%|███████▌  | 48/63 [00:18<00:05,  2.63it/s]\n",
      " 78%|███████▊  | 49/63 [00:18<00:05,  2.63it/s]\n",
      " 79%|███████▉  | 50/63 [00:19<00:04,  2.63it/s]\n",
      " 81%|████████  | 51/63 [00:19<00:04,  2.63it/s]\n",
      " 83%|████████▎ | 52/63 [00:19<00:04,  2.63it/s]\n",
      " 84%|████████▍ | 53/63 [00:20<00:03,  2.63it/s]\n",
      " 86%|████████▌ | 54/63 [00:20<00:03,  2.63it/s]\n",
      " 87%|████████▋ | 55/63 [00:21<00:03,  2.63it/s]\n",
      " 89%|████████▉ | 56/63 [00:21<00:02,  2.63it/s]\n",
      " 90%|█████████ | 57/63 [00:21<00:02,  2.63it/s]\n",
      " 92%|█████████▏| 58/63 [00:22<00:01,  2.63it/s]\n",
      " 94%|█████████▎| 59/63 [00:22<00:01,  2.63it/s]\n",
      " 95%|█████████▌| 60/63 [00:22<00:01,  2.63it/s]\n",
      " 97%|█████████▋| 61/63 [00:23<00:00,  2.63it/s]\n",
      " 98%|█████████▊| 62/63 [00:23<00:00,  2.63it/s]\n",
      "100%|██████████| 63/63 [00:23<00:00,  2.63it/s]\n",
      "63it [00:23,  2.63it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/63 [00:00<00:24,  2.53it/s]\n",
      "  3%|▎         | 2/63 [00:00<00:23,  2.55it/s]\n",
      "  5%|▍         | 3/63 [00:01<00:23,  2.55it/s]\n",
      "  6%|▋         | 4/63 [00:01<00:23,  2.55it/s]\n",
      "  8%|▊         | 5/63 [00:01<00:22,  2.56it/s]\n",
      " 10%|▉         | 6/63 [00:02<00:22,  2.56it/s]\n",
      " 11%|█         | 7/63 [00:02<00:21,  2.56it/s]\n",
      " 13%|█▎        | 8/63 [00:03<00:21,  2.56it/s]\n",
      " 14%|█▍        | 9/63 [00:03<00:21,  2.56it/s]\n",
      " 16%|█▌        | 10/63 [00:03<00:20,  2.56it/s]\n",
      " 17%|█▋        | 11/63 [00:04<00:20,  2.56it/s]\n",
      " 19%|█▉        | 12/63 [00:04<00:19,  2.56it/s]\n",
      " 21%|██        | 13/63 [00:05<00:19,  2.56it/s]\n",
      " 22%|██▏       | 14/63 [00:05<00:19,  2.56it/s]\n",
      " 24%|██▍       | 15/63 [00:05<00:18,  2.56it/s]\n",
      " 25%|██▌       | 16/63 [00:06<00:18,  2.56it/s]\n",
      " 27%|██▋       | 17/63 [00:06<00:17,  2.56it/s]\n",
      " 29%|██▊       | 18/63 [00:07<00:17,  2.56it/s]\n",
      " 30%|███       | 19/63 [00:07<00:17,  2.56it/s]\n",
      " 32%|███▏      | 20/63 [00:07<00:16,  2.56it/s]\n",
      " 33%|███▎      | 21/63 [00:08<00:16,  2.56it/s]\n",
      " 35%|███▍      | 22/63 [00:08<00:16,  2.56it/s]\n",
      " 37%|███▋      | 23/63 [00:08<00:15,  2.56it/s]\n",
      " 38%|███▊      | 24/63 [00:09<00:15,  2.56it/s]\n",
      " 40%|███▉      | 25/63 [00:09<00:14,  2.56it/s]\n",
      " 41%|████▏     | 26/63 [00:10<00:14,  2.56it/s]\n",
      " 43%|████▎     | 27/63 [00:10<00:14,  2.56it/s]\n",
      " 44%|████▍     | 28/63 [00:10<00:13,  2.56it/s]\n",
      " 46%|████▌     | 29/63 [00:11<00:13,  2.56it/s]\n",
      " 48%|████▊     | 30/63 [00:11<00:12,  2.56it/s]\n",
      " 49%|████▉     | 31/63 [00:12<00:12,  2.56it/s]\n",
      " 51%|█████     | 32/63 [00:12<00:12,  2.56it/s]\n",
      " 52%|█████▏    | 33/63 [00:12<00:11,  2.56it/s]\n",
      " 54%|█████▍    | 34/63 [00:13<00:11,  2.56it/s]\n",
      " 56%|█████▌    | 35/63 [00:13<00:10,  2.56it/s]\n",
      " 57%|█████▋    | 36/63 [00:14<00:10,  2.56it/s]\n",
      " 59%|█████▊    | 37/63 [00:14<00:10,  2.56it/s]\n",
      " 60%|██████    | 38/63 [00:14<00:09,  2.56it/s]\n",
      " 62%|██████▏   | 39/63 [00:15<00:09,  2.56it/s]\n",
      " 63%|██████▎   | 40/63 [00:15<00:08,  2.56it/s]\n",
      " 65%|██████▌   | 41/63 [00:16<00:08,  2.56it/s]\n",
      " 67%|██████▋   | 42/63 [00:16<00:08,  2.56it/s]\n",
      " 68%|██████▊   | 43/63 [00:16<00:07,  2.56it/s]\n",
      " 70%|██████▉   | 44/63 [00:17<00:07,  2.56it/s]\n",
      " 71%|███████▏  | 45/63 [00:17<00:07,  2.56it/s]\n",
      " 73%|███████▎  | 46/63 [00:17<00:06,  2.56it/s]\n",
      " 75%|███████▍  | 47/63 [00:18<00:06,  2.56it/s]\n",
      " 76%|███████▌  | 48/63 [00:18<00:05,  2.56it/s]\n",
      " 78%|███████▊  | 49/63 [00:19<00:05,  2.56it/s]\n",
      " 79%|███████▉  | 50/63 [00:19<00:05,  2.56it/s]\n",
      " 81%|████████  | 51/63 [00:19<00:04,  2.56it/s]\n",
      " 83%|████████▎ | 52/63 [00:20<00:04,  2.56it/s]\n",
      " 84%|████████▍ | 53/63 [00:20<00:03,  2.56it/s]\n",
      " 86%|████████▌ | 54/63 [00:21<00:03,  2.56it/s]\n",
      " 87%|████████▋ | 55/63 [00:21<00:03,  2.56it/s]\n",
      " 89%|████████▉ | 56/63 [00:21<00:02,  2.56it/s]\n",
      " 90%|█████████ | 57/63 [00:22<00:02,  2.56it/s]\n",
      " 92%|█████████▏| 58/63 [00:22<00:01,  2.56it/s]\n",
      " 94%|█████████▎| 59/63 [00:23<00:01,  2.56it/s]\n",
      " 95%|█████████▌| 60/63 [00:23<00:01,  2.56it/s]\n",
      " 97%|█████████▋| 61/63 [00:23<00:00,  2.56it/s]\n",
      " 98%|█████████▊| 62/63 [00:24<00:00,  2.56it/s]\n",
      "100%|██████████| 63/63 [00:24<00:00,  2.58it/s]\n",
      "63it [00:24,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA accuracy: 0.66\n",
      "Goat accuracy: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llama_acc = eval_model_performance(llama_model, dataloader, device)\n",
    "goat_acc = eval_model_performance(goat_model, dataloader, device)\n",
    "\n",
    "print(f\"LLAMA accuracy: {llama_acc}\")\n",
    "print(f\"Goat accuracy: {goat_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ce7ad",
   "metadata": {},
   "source": [
    "### Loading Model Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "247e0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_modules = [[f\"model.layers.{layer}.self_attn.k_proj\", \n",
    "                  f\"model.layers.{layer}.self_attn.q_proj\",\n",
    "                  f\"model.layers.{layer}.self_attn.v_proj\",\n",
    "                 f\"model.layers.{layer}.self_attn.o_proj\"] \n",
    "                 for layer in range(llama_model.config.num_hidden_layers)]\n",
    "goat_modules = [[f\"base_model.model.model.layers.{layer}.self_attn.k_proj\", \n",
    "                 f\"base_model.model.model.layers.{layer}.self_attn.q_proj\",\n",
    "                 f\"base_model.model.model.layers.{layer}.self_attn.v_proj\",\n",
    "                f\"base_model.model.model.layers.{layer}.self_attn.o_proj\"] \n",
    "                for layer in range(goat_model.config.num_hidden_layers)]\n",
    "\n",
    "llama_modules = [item for sublist in llama_modules for item in sublist]\n",
    "goat_modules = [item for sublist in goat_modules for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e4dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "goat_cache: 63it [00:53,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "goat_cache = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader), desc=\"goat_cache\"):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(goat_model.device)\n",
    "\n",
    "        with TraceDict(goat_model, goat_modules, retain_input=True) as cache:\n",
    "            _ = goat_model(inputs[\"input_ids\"])\n",
    "        \n",
    "        for goat_layer, llama_layer in zip(goat_modules, llama_modules):\n",
    "            if \"o_proj\" in llama_layer and \"o_proj\" in goat_layer:\n",
    "                if bi in goat_cache:\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].input.cpu()\n",
    "                else:\n",
    "                    goat_cache[bi] = {}\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].input.cpu()\n",
    "            else:\n",
    "                if bi in goat_cache:\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].output.cpu()\n",
    "                else:\n",
    "                    goat_cache[bi] = {}\n",
    "                    goat_cache[bi][llama_layer] = cache[goat_layer].output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b229d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_cache: 63it [00:46,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "llama_cache = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader), desc=\"llama_cache\"):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        with TraceDict(llama_model, llama_modules, retain_input=True) as cache:\n",
    "            _ = llama_model(inputs[\"input_ids\"])\n",
    "        \n",
    "        for llama_layer in llama_modules:\n",
    "            if \"o_proj\" in llama_layer:\n",
    "                if bi in llama_cache:\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].input.cpu()\n",
    "                else:\n",
    "                    llama_cache[bi] = {}\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].input.cpu()\n",
    "            else:\n",
    "                if bi in llama_cache:\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].output.cpu()\n",
    "                else:\n",
    "                    llama_cache[bi] = {}\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].output.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2b096",
   "metadata": {},
   "source": [
    "### CMAP (output patching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee702e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full circuit (Select group of heads for CMAP accordingly)\n",
    "pos_heads_dict = {}\n",
    "pos_heads_dict[0] = goat_value_fetcher\n",
    "pos_heads_dict[0] += goat_pos_transmitter\n",
    "pos_heads_dict[2] = goat_pos_detector\n",
    "pos_heads_dict[-1] = goat_circuit['struct_reader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c95c5393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:33,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_count, total_count = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        with TraceDict(llama_model,\n",
    "                       llama_modules,\n",
    "                       retain_input=True,\n",
    "                       edit_output=partial(\n",
    "                            cmap_out,\n",
    "                            model = llama_model,\n",
    "                            goat_cache = goat_cache,\n",
    "                            bi = bi,\n",
    "                            pos_heads_dict = pos_heads_dict,\n",
    "                            input_tokens = inputs)) as _:\n",
    "                outputs = llama_model(inputs[\"input_ids\"], output_attentions=True)\n",
    "\n",
    "        for bi in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][bi]\n",
    "            pred = torch.argmax(outputs.logits[bi][inputs[\"last_token_indices\"][bi]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            total_count += 1\n",
    "\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "current_acc = round(correct_count / total_count, 2)\n",
    "print(f\"Task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741276db",
   "metadata": {},
   "source": [
    "Output CMAP Results (Goat -> Llama):\n",
    "- Full Circuit: 0.82\n",
    "- Value Fetcher: 0.82\n",
    "- Position Transmitter: 0.78\n",
    "- Position Detector: 0.62\n",
    "- Structure Reader: 0.65\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb33f72",
   "metadata": {},
   "source": [
    "# CMAP (FLoat -> Llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82450caa",
   "metadata": {},
   "source": [
    "### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5de2617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/63 [00:00<00:32,  1.92it/s]\n",
      "  3%|▎         | 2/63 [00:00<00:26,  2.28it/s]\n",
      "  5%|▍         | 3/63 [00:01<00:24,  2.42it/s]\n",
      "  6%|▋         | 4/63 [00:01<00:23,  2.50it/s]\n",
      "  8%|▊         | 5/63 [00:02<00:22,  2.54it/s]\n",
      " 10%|▉         | 6/63 [00:02<00:22,  2.57it/s]\n",
      " 11%|█         | 7/63 [00:02<00:21,  2.59it/s]\n",
      " 13%|█▎        | 8/63 [00:03<00:21,  2.60it/s]\n",
      " 14%|█▍        | 9/63 [00:03<00:20,  2.61it/s]\n",
      " 16%|█▌        | 10/63 [00:03<00:20,  2.61it/s]\n",
      " 17%|█▋        | 11/63 [00:04<00:19,  2.61it/s]\n",
      " 19%|█▉        | 12/63 [00:04<00:19,  2.62it/s]\n",
      " 21%|██        | 13/63 [00:05<00:19,  2.62it/s]\n",
      " 22%|██▏       | 14/63 [00:05<00:18,  2.62it/s]\n",
      " 24%|██▍       | 15/63 [00:05<00:18,  2.62it/s]\n",
      " 25%|██▌       | 16/63 [00:06<00:17,  2.62it/s]\n",
      " 27%|██▋       | 17/63 [00:06<00:17,  2.62it/s]\n",
      " 29%|██▊       | 18/63 [00:07<00:17,  2.62it/s]\n",
      " 30%|███       | 19/63 [00:07<00:16,  2.62it/s]\n",
      " 32%|███▏      | 20/63 [00:07<00:16,  2.62it/s]\n",
      " 33%|███▎      | 21/63 [00:08<00:16,  2.62it/s]\n",
      " 35%|███▍      | 22/63 [00:08<00:15,  2.62it/s]\n",
      " 37%|███▋      | 23/63 [00:08<00:15,  2.62it/s]\n",
      " 38%|███▊      | 24/63 [00:09<00:14,  2.62it/s]\n",
      " 40%|███▉      | 25/63 [00:09<00:14,  2.62it/s]\n",
      " 41%|████▏     | 26/63 [00:10<00:14,  2.62it/s]\n",
      " 43%|████▎     | 27/63 [00:10<00:13,  2.62it/s]\n",
      " 44%|████▍     | 28/63 [00:10<00:13,  2.63it/s]\n",
      " 46%|████▌     | 29/63 [00:11<00:12,  2.63it/s]\n",
      " 48%|████▊     | 30/63 [00:11<00:12,  2.63it/s]\n",
      " 49%|████▉     | 31/63 [00:11<00:12,  2.63it/s]\n",
      " 51%|█████     | 32/63 [00:12<00:11,  2.63it/s]\n",
      " 52%|█████▏    | 33/63 [00:12<00:11,  2.63it/s]\n",
      " 54%|█████▍    | 34/63 [00:13<00:11,  2.63it/s]\n",
      " 56%|█████▌    | 35/63 [00:13<00:10,  2.63it/s]\n",
      " 57%|█████▋    | 36/63 [00:13<00:10,  2.62it/s]\n",
      " 59%|█████▊    | 37/63 [00:14<00:09,  2.62it/s]\n",
      " 60%|██████    | 38/63 [00:14<00:09,  2.62it/s]\n",
      " 62%|██████▏   | 39/63 [00:15<00:09,  2.62it/s]\n",
      " 63%|██████▎   | 40/63 [00:15<00:08,  2.62it/s]\n",
      " 65%|██████▌   | 41/63 [00:15<00:08,  2.62it/s]\n",
      " 67%|██████▋   | 42/63 [00:16<00:08,  2.62it/s]\n",
      " 68%|██████▊   | 43/63 [00:16<00:07,  2.62it/s]\n",
      " 70%|██████▉   | 44/63 [00:16<00:07,  2.63it/s]\n",
      " 71%|███████▏  | 45/63 [00:17<00:06,  2.63it/s]\n",
      " 73%|███████▎  | 46/63 [00:17<00:06,  2.63it/s]\n",
      " 75%|███████▍  | 47/63 [00:18<00:06,  2.63it/s]\n",
      " 76%|███████▌  | 48/63 [00:18<00:05,  2.62it/s]\n",
      " 78%|███████▊  | 49/63 [00:18<00:05,  2.62it/s]\n",
      " 79%|███████▉  | 50/63 [00:19<00:04,  2.62it/s]\n",
      " 81%|████████  | 51/63 [00:19<00:04,  2.62it/s]\n",
      " 83%|████████▎ | 52/63 [00:19<00:04,  2.62it/s]\n",
      " 84%|████████▍ | 53/63 [00:20<00:03,  2.62it/s]\n",
      " 86%|████████▌ | 54/63 [00:20<00:03,  2.62it/s]\n",
      " 87%|████████▋ | 55/63 [00:21<00:03,  2.62it/s]\n",
      " 89%|████████▉ | 56/63 [00:21<00:02,  2.62it/s]\n",
      " 90%|█████████ | 57/63 [00:21<00:02,  2.63it/s]\n",
      " 92%|█████████▏| 58/63 [00:22<00:01,  2.63it/s]\n",
      " 94%|█████████▎| 59/63 [00:22<00:01,  2.63it/s]\n",
      " 95%|█████████▌| 60/63 [00:23<00:01,  2.63it/s]\n",
      " 97%|█████████▋| 61/63 [00:23<00:00,  2.63it/s]\n",
      " 98%|█████████▊| 62/63 [00:23<00:00,  2.63it/s]\n",
      "100%|██████████| 63/63 [00:23<00:00,  2.63it/s]\n",
      "63it [00:23,  2.63it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/63 [00:00<00:23,  2.61it/s]\n",
      "  3%|▎         | 2/63 [00:00<00:23,  2.61it/s]\n",
      "  5%|▍         | 3/63 [00:01<00:22,  2.62it/s]\n",
      "  6%|▋         | 4/63 [00:01<00:22,  2.62it/s]\n",
      "  8%|▊         | 5/63 [00:01<00:22,  2.62it/s]\n",
      " 10%|▉         | 6/63 [00:02<00:21,  2.63it/s]\n",
      " 11%|█         | 7/63 [00:02<00:21,  2.63it/s]\n",
      " 13%|█▎        | 8/63 [00:03<00:20,  2.63it/s]\n",
      " 14%|█▍        | 9/63 [00:03<00:20,  2.63it/s]\n",
      " 16%|█▌        | 10/63 [00:03<00:20,  2.63it/s]\n",
      " 17%|█▋        | 11/63 [00:04<00:19,  2.63it/s]\n",
      " 19%|█▉        | 12/63 [00:04<00:19,  2.63it/s]\n",
      " 21%|██        | 13/63 [00:04<00:19,  2.63it/s]\n",
      " 22%|██▏       | 14/63 [00:05<00:18,  2.63it/s]\n",
      " 24%|██▍       | 15/63 [00:05<00:18,  2.62it/s]\n",
      " 25%|██▌       | 16/63 [00:06<00:17,  2.62it/s]\n",
      " 27%|██▋       | 17/63 [00:06<00:17,  2.62it/s]\n",
      " 29%|██▊       | 18/63 [00:06<00:17,  2.63it/s]\n",
      " 30%|███       | 19/63 [00:07<00:16,  2.62it/s]\n",
      " 32%|███▏      | 20/63 [00:07<00:16,  2.63it/s]\n",
      " 33%|███▎      | 21/63 [00:08<00:15,  2.63it/s]\n",
      " 35%|███▍      | 22/63 [00:08<00:15,  2.63it/s]\n",
      " 37%|███▋      | 23/63 [00:08<00:15,  2.63it/s]\n",
      " 38%|███▊      | 24/63 [00:09<00:14,  2.63it/s]\n",
      " 40%|███▉      | 25/63 [00:09<00:14,  2.62it/s]\n",
      " 41%|████▏     | 26/63 [00:09<00:14,  2.62it/s]\n",
      " 43%|████▎     | 27/63 [00:10<00:13,  2.62it/s]\n",
      " 44%|████▍     | 28/63 [00:10<00:13,  2.62it/s]\n",
      " 46%|████▌     | 29/63 [00:11<00:12,  2.62it/s]\n",
      " 48%|████▊     | 30/63 [00:11<00:12,  2.62it/s]\n",
      " 49%|████▉     | 31/63 [00:11<00:12,  2.62it/s]\n",
      " 51%|█████     | 32/63 [00:12<00:11,  2.62it/s]\n",
      " 52%|█████▏    | 33/63 [00:12<00:11,  2.62it/s]\n",
      " 54%|█████▍    | 34/63 [00:12<00:11,  2.62it/s]\n",
      " 56%|█████▌    | 35/63 [00:13<00:10,  2.63it/s]\n",
      " 57%|█████▋    | 36/63 [00:13<00:10,  2.63it/s]\n",
      " 59%|█████▊    | 37/63 [00:14<00:09,  2.63it/s]\n",
      " 60%|██████    | 38/63 [00:14<00:09,  2.63it/s]\n",
      " 62%|██████▏   | 39/63 [00:14<00:09,  2.63it/s]\n",
      " 63%|██████▎   | 40/63 [00:15<00:08,  2.63it/s]\n",
      " 65%|██████▌   | 41/63 [00:15<00:08,  2.63it/s]\n",
      " 67%|██████▋   | 42/63 [00:15<00:07,  2.63it/s]\n",
      " 68%|██████▊   | 43/63 [00:16<00:07,  2.63it/s]\n",
      " 70%|██████▉   | 44/63 [00:16<00:07,  2.63it/s]\n",
      " 71%|███████▏  | 45/63 [00:17<00:06,  2.63it/s]\n",
      " 73%|███████▎  | 46/63 [00:17<00:06,  2.63it/s]\n",
      " 75%|███████▍  | 47/63 [00:17<00:06,  2.63it/s]\n",
      " 76%|███████▌  | 48/63 [00:18<00:05,  2.63it/s]\n",
      " 78%|███████▊  | 49/63 [00:18<00:05,  2.63it/s]\n",
      " 79%|███████▉  | 50/63 [00:19<00:04,  2.63it/s]\n",
      " 81%|████████  | 51/63 [00:19<00:04,  2.63it/s]\n",
      " 83%|████████▎ | 52/63 [00:19<00:04,  2.62it/s]\n",
      " 84%|████████▍ | 53/63 [00:20<00:03,  2.62it/s]\n",
      " 86%|████████▌ | 54/63 [00:20<00:03,  2.62it/s]\n",
      " 87%|████████▋ | 55/63 [00:20<00:03,  2.62it/s]\n",
      " 89%|████████▉ | 56/63 [00:21<00:02,  2.62it/s]\n",
      " 90%|█████████ | 57/63 [00:21<00:02,  2.63it/s]\n",
      " 92%|█████████▏| 58/63 [00:22<00:01,  2.62it/s]\n",
      " 94%|█████████▎| 59/63 [00:22<00:01,  2.63it/s]\n",
      " 95%|█████████▌| 60/63 [00:22<00:01,  2.63it/s]\n",
      " 97%|█████████▋| 61/63 [00:23<00:00,  2.63it/s]\n",
      " 98%|█████████▊| 62/63 [00:23<00:00,  2.63it/s]\n",
      "100%|██████████| 63/63 [00:23<00:00,  2.64it/s]\n",
      "63it [00:23,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA accuracy: 0.66\n",
      "FLoat accuracy: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llama_acc = eval_model_performance(llama_model, dataloader, device)\n",
    "float_acc = eval_model_performance(float_model, dataloader, device)\n",
    "\n",
    "print(f\"LLAMA accuracy: {llama_acc}\")\n",
    "print(f\"FLoat accuracy: {float_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3dc7cc",
   "metadata": {},
   "source": [
    "### Loading Model Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71ba9b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_modules = [[f\"model.layers.{layer}.self_attn.k_proj\", \n",
    "                  f\"model.layers.{layer}.self_attn.q_proj\",\n",
    "                  f\"model.layers.{layer}.self_attn.v_proj\",\n",
    "                 f\"model.layers.{layer}.self_attn.o_proj\"] \n",
    "                 for layer in range(llama_model.config.num_hidden_layers)]\n",
    "\n",
    "llama_modules = [item for sublist in llama_modules for item in sublist]\n",
    "float_modules = llama_modules.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c91c180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "float_cache: 63it [00:42,  1.50it/s]\n"
     ]
    }
   ],
   "source": [
    "float_cache = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader), desc=\"float_cache\"):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(float_model.device)\n",
    "\n",
    "        with TraceDict(float_model, float_modules, retain_input=True) as cache:\n",
    "            _ = float_model(inputs[\"input_ids\"])\n",
    "        \n",
    "        for float_layer in float_modules:\n",
    "            if \"o_proj\" in float_layer:\n",
    "                if bi in float_cache:\n",
    "                    float_cache[bi][float_layer] = cache[float_layer].input.cpu()\n",
    "                else:\n",
    "                    float_cache[bi] = {}\n",
    "                    float_cache[bi][float_layer] = cache[float_layer].input.cpu()\n",
    "            else:\n",
    "                if bi in float_cache:\n",
    "                    float_cache[bi][float_layer] = cache[float_layer].output.cpu()\n",
    "                else:\n",
    "                    float_cache[bi] = {}\n",
    "                    float_cache[bi][float_layer] = cache[float_layer].output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838b51bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_cache: 63it [00:44,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "llama_cache = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader), desc=\"llama_cache\"):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        with TraceDict(llama_model, llama_modules, retain_input=True) as cache:\n",
    "            _ = llama_model(inputs[\"input_ids\"])\n",
    "        \n",
    "        for llama_layer in llama_modules:\n",
    "            if \"o_proj\" in llama_layer:\n",
    "                if bi in llama_cache:\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].input.cpu()\n",
    "                else:\n",
    "                    llama_cache[bi] = {}\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].input.cpu()\n",
    "            else:\n",
    "                if bi in llama_cache:\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].output.cpu()\n",
    "                else:\n",
    "                    llama_cache[bi] = {}\n",
    "                    llama_cache[bi][llama_layer] = cache[llama_layer].output.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4abb52",
   "metadata": {},
   "source": [
    "### CMAP (output patching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76d5cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full circuit (Select group of heads for CMAP accordingly)\n",
    "pos_heads_dict = {}\n",
    "pos_heads_dict[0] = float_value_fetcher\n",
    "pos_heads_dict[0] += float_pos_transmitter\n",
    "pos_heads_dict[2] = float_pos_detector\n",
    "pos_heads_dict[-1] = float_circuit['struct_reader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319de0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:14,  1.91it/s]"
     ]
    }
   ],
   "source": [
    "correct_count, total_count = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, inputs in tqdm(enumerate(dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(llama_model.device)\n",
    "\n",
    "        with TraceDict(llama_model,\n",
    "                       llama_modules,\n",
    "                       retain_input=True,\n",
    "                       edit_output=partial(\n",
    "                            cmap_out,\n",
    "                            model = llama_model,\n",
    "                            finetuned_cache = float_cache,\n",
    "                            bi = bi,\n",
    "                            pos_heads_dict = pos_heads_dict,\n",
    "                            input_tokens = inputs)) as _:\n",
    "                outputs = llama_model(inputs[\"input_ids\"], output_attentions=True)\n",
    "\n",
    "        for bi in range(inputs[\"labels\"].size(0)):\n",
    "            label = inputs[\"labels\"][bi]\n",
    "            pred = torch.argmax(outputs.logits[bi][inputs[\"last_token_indices\"][bi]])\n",
    "\n",
    "            if label == pred:\n",
    "                correct_count += 1\n",
    "            total_count += 1\n",
    "\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "current_acc = round(correct_count / total_count, 2)\n",
    "print(f\"Task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae8692a",
   "metadata": {},
   "source": [
    "Output CMAP Results (FLoat -> Llama):\n",
    "- Full Circuit: 0.82\n",
    "- Value Fetcher: 0.82\n",
    "- Position Transmitter: 0.74\n",
    "- Position Detector: 0.56\n",
    "- Structure Reader: 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be5419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
